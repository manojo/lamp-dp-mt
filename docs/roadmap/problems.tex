% LaTeX Template for a short article
\documentclass[11pt]{article}
\usepackage{amssymb,amsmath,amsthm,hyperref,verbatim,pict2e,graphicx,marvosym}
\hypersetup{colorlinks,citecolor=black,filecolor=black,linkcolor=black,urlcolor=black}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
 \pagestyle{empty} % remove page numbers

\oddsidemargin   0.0cm
\evensidemargin  0.0cm
\topmargin       0.0cm
\headheight      0.0cm
\headsep         1.0cm
\textheight     21.0cm
\textwidth      16.0cm
\parskip         0.1cm
\parindent       0.0cm
\footskip        1.0cm

\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{8pt}{0pt}
\titlespacing{\subsection}{0pt}{8pt}{0pt}
\titlespacing{\subsubsection}{0pt}{8pt}{0pt}
\usepackage{enumitem}
\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\setenumerate{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\def\ul{\begin{itemize}}
\def\ule{\end{itemize}}
\def\ol{\begin{enumerate}}
\def\ole{\end{enumerate}}


\title{DP problems of interest}
\author{Manohar Jonnalagedda, Thierry Coppey}
\date{}
\begin{document}
\maketitle
\pagestyle{headings}

% ----------------------------------------------------------------------------------------------------------------------------------------
\section{Introduction}
\subsection{Definitions}\ul
\item Block of computation: a block is simply a part of the DP matrix that we want to compute.
\item Wavefront: this is the place around which computation happen, typically. There should be some memory to store intermediate information between block of computations
\ule

\subsection{Problems classification}
In the literature dynamic programming problems (DP) are classified according to two criteria:

\textbf{Monadic / polyadic}\ul
\item Monadic: on the right hand-side of the recurrence formula, only one term appears. For instance, Smith-Waterman with constant penalty is monadic
	\[M_{(i,j)}=\max\left\{\begin{array}{l} 0 \\ M_{(i-1,j-1)}+{\rm cost}(S(i),T(j))\\ M_{(i-1,j)}-d\\ M_{(i,j-1)}-d \end{array}\right. \]
\item \textbf{Polyadic:} when multiple terms of the recurrence occur in the right and-side of the recurrence formula. For instance Fibonacci is polyadic: \[F(n) = F(n-1) + F(n-2)\]
\ule

\textbf{Serial/non-serial} \ul
\item \textbf{Serial:} when the solution depends only of a fixed number of immediately previous solutions (i.e. neighbor cells). For instance Fibonacci is serial.
\item \textbf{Non-serial:} when the solution of a level depends of an arbitrary number of previous solutions. Typically Smith-Waterman with arbitrary gap penalty and Nussinov are non-serial:
	\[M_{(i,j)}=\max\left\{\begin{array}{l} ... \\ M_{(i,j-1)}\\ \max\limits_{i<k<j} [ M_{(i,k)}+M_{(k+1,j)} ] \end{array}\right. \]
\ule

\subsection{Simplifications}
\subsubsection{Calculus}
In some special case, it is possible to transform a non-serial problem into a serial problem, if we can embed the non-serial term into an additional aggregation matrix. For example, we can transform
	\[M_{(i,j)}=\max\left\{\begin{array}{l} \max\limits_{k<i} M_{(k,j)}
	\\ \sum\limits_{k<i, l<j}M_{(k,l)} \end{array}\right.
	\implies M_{(i,j)}=\max\left\{\begin{array}{l} C_{(k,j)} \\ A_{(i-1,j-1)} \end{array}\right.\]
Where $C$ is a matrix that stores the maximum along the column and $A$ is a matrix that stores the sum of the array of the previous elements. They can be easily computed with the additional recurrence.
	\[\begin{array}{rcl} C_{(i,j)}&=&\max(C_{(i-1,j)}, M_{(i,j)}) \\
	A_{(i,j)}&=&A_{(i-1,j)}+A_{(i,j-1)}-A_{(i-1,j-1)}+M_{(i,j)}\end{array}\]

This simplification avoids the non-serial dependencies at the cost of an additional storage at the wavefront.

\subsubsection{Precomputations}
When a calculus computation is impossible, it might be worth to interleave a computation phase that will aggregate some of the results that are necessary to the computation block. For instance, for Nussinov's $\max_{i<k<j} [ M_{(i,k)}+M_{(k+1,j)} ]$ we can precompute it for all rows and columns of the block, and for all elements that are not part of the block, and pass these partial results together at the block launch.

On GPU, this could be done by interleaving a new kernel for this specific purpose, on FPGA, this could be done by preparing the maximums in another memory area whose pointer will later be passed to the co-processor.

% ----------------------------------------------------------------------------------------------------------------------------------------
\section{Problems of interest}
We describe problems structures: inputs, cost matrices, backtracking matrix. They all have an alphabet (that hopefully corresponds to a maximal bit-size) and matrixes dimensions are specified by number of indices and their number of elements is usually the same as the input. Unless otherwise specified, number are all unsigned integers. We describe the problem processing in terms of both initialization and recurrences.

% ----------------------------------------------
\subsection{Smith-Waterman (simple)}
Problem: matching two strings $S$, $T$.

Alphabets:\ul
\item Input: $\Sigma(S)=\Sigma(T)=\{a,c,g,t\}$.
\item Cost matrix: $\Sigma(M) = {\rm integers} \{0..n\}, n=\max({\rm cost}) \cdot \min({\rm length}(S),{\rm length}(T))$
\item Backtrack matrix: $\Sigma(B)=\{stop,W,N,NW\}$
\ule

Initialization:\ul
\item Cost matrix: $M_{(i,0)}=M_{(0,j)}=0$.
\item Backtrack matrix: $B_{(i,0)}=B_{(0,j)}=stop$.
\ule

Recurrence:
\[M_{(i,j)}=\max\left\{\begin{array}{l|l}
	0 & stop\\
	M_{(i-1,j-1)}+{\rm cost}(S(i),T(j)) & NW\\
	M_{(i-1,j)}-d & N\\
	M_{(i,j-1)}-d & W
\end{array}\right\}=B_{(i,j)} \]

% ----------------------------------------------
\subsection{Smith-Waterman (with gap extension at different cost)}
Problem: matching two strings $S$, $T$.

Alphabets:\ul
\item Input: $\Sigma(S)=\Sigma(T)=\{a,c,g,t\}$.
\item Cost matrix: $\Sigma(M) = {\rm integers} \{0..n\}, n=\max({\rm cost}) \cdot \min({\rm length}(S),{\rm length}(T))$
\item Backtrack matrix: $\Sigma(B)=\{stop,W,N,NW\}$
\ule

Initialization:\ul
\item Cost matrix: $M_{(i,0)}=M_{(0,j)}=0$.
\item Gap opening matrix: $E_{(i,0)}= 0, 0 \le i \le {\rm length}(S)$
\item Gap extending matrix: $E_{(0,j)}= 0, 0 \le j \le {\rm length}(T)$
\item Backtrack matrix: $B_{(i,0)}=B_{(0,j)}=stop$.
\ule

Recurrence for the cost matrix:
\[M_{(i,j)}=\max\left\{\begin{array}{l|l}
	0 & stop\\
	M_{(i-1,j-1)}+{\rm cost}(S(i),T(j)) & NW\\
	E_{(i,j)} & N\\
	F_{(i,j)} & W
\end{array}\right\}=B_{(i,j)} \]

Recurrence for the gap opening/extending matrices:
\[E_{(i,j)}=\max\left\{\begin{array}{l|l}
	M_{(i, j-1)} - \alpha & NW\\
	E_{(i,j-1)} - \beta & N\\
\end{array}\right\}=B_{(i,j)} \]

\[F_{(i,j)}=\max\left\{\begin{array}{l|l}
	M_{(i-1, j)} - \alpha & NW\\
	E_{(i-1,j)} - \beta & N\\
\end{array}\right\}=B_{(i,j)} \]

Otherwise written as:
\[M_{(i,j)}=\max\left\{\begin{array}{l|l}
	0 & stop\\
	M_{(i-1,j-1)}+{\rm cost}(S(i),T(j)) & NW\\
	\max_{1 \le k \le j-1} M_{(i,k)} - \alpha - (j-1-k) \cdot \beta & N\\
	\max_{1 \le k \le i-1} M_{(k,j)} - \alpha - (i-1-k) \cdot \beta & W\\
\end{array}\right\}=B_{(i,j)} \]

% ----------------------------------------------
\subsection{Convex polygon triangulation}
Problem: triangulating a polygon with least cost overall for added edges

Alphabets:\ul
\item Input: $\Sigma(S)= $ a matrix of costs for every possible edge in the polygon. The 
size of the input is $\frac{n^2}{2}$, where $n$ is the number of vertices in the polygon. 
\item Cost matrix: $\Sigma(M)=\mathbb{N}$. The sum of cost of edges is not "bounded".
\item Backtrack matrix: $\Sigma(B)=\{n\}$
\ule

Recurrence:
\[M_{(i,j)}= \max_{i<k<j}M_{(i,k)}+M_{(k+1,j)} + c(i,k) | k = B_{(i,j)} \]

It is interesting to note that even in the sequential world, this problem is best solved 
by filling the diagonals, ie. computing sub-solutions for all polygons on size $k$ before
those of size $k+1$.

% ----------------------------------------------


% ----------------------------------------------
\newpage
\subsection{Nussinov algorithm}
Problem: folding a RNA string $S$ over itself.

Alphabets:\ul
\item Input: $\Sigma(S)=\{A,C,G,U\}$.
\item Cost matrix: $\Sigma(M)=\{0..n\}, n={\rm length}(S)/2$
\item Backtrack matrix: $\Sigma(B)=\{stop,W,S,SW, 1..n\}$
\ule

Initialization: \ul
\item Cost matrix: $\left\{\begin{array}{l} M_{(i,i)}=0 \\ M_{(i,i-1)}=0 \end{array}\right. \forall i \in 1..{\rm length}(S)$
\item Backtrack matrix: $\left\{\begin{array}{l} B_{(i,i)}=stop \\ B_{(i,i-1)}=stop \end{array}\right.  \forall i \in 1..{\rm length}(S)$
\ule

Recurrence:
\[M_{(i,j)}=\max\left\{\begin{array}{l|l}
	M_{(i+1,j-1)}+\omega(i,j) & SW\\
	M_{(i+1,j)} & S\\
	M_{(i,j-1)} & W\\
	\max_{i<k<j}M_{(i,k)}+M_{(k+1,j)} & k
\end{array}\right\} = B_{(i,j)} \]
With $\omega(i,j)=1$ if $i,j$ are complementary. 0 otherwise.

% ------------------------------------------------------------------------------------------------
\newpage
\subsection{Zuker folding}
XXX: we let this aside for the moment as the recurrence has too many unknown parameters
Problem: folding a RNA string $S$ over itself.

Alphabets:\ul
\item Input: $\Sigma(S)=\{A,C,G,U\}$.
\item Cost matrix: $\Sigma(W)=\Sigma(V)=\Sigma(F)=\{0..n\},n=$
\item Backtrack matrix: $\Sigma(B)=\{\}$
\ule

Initialization:\ul
\item Cost matrices:\ul
	\item xxx
\ule
\item Backtrack matrix: $B$
\ule

Recurrence:
\[\begin{array}{rcl}
W_{(i,j)}&=&\min\left\{\begin{array}{l}
	W_{(i+1,j)}+b \\
	W_{(i,j-1)}+b \\
	V_{(i,j)}+\delta(S_i,S_j) \\
	\min_{i<k<j}W_{(i,k)}+W_{(k+1,j)}
\end{array}\right.\\\\

V_{(i,j)}&=&\min\left\{\begin{array}{ll}
	\infty \qquad\qquad\qquad\qquad {\rm if}(S_i,S_j) \text{ is not a base pair}\\
	eh(i,j)+b \qquad\qquad\, \text{otherwise} \\
	V_{(i+1,j-1)}+es(i,j) \\
	VBI(i,j) \\
	\min_{i<k<j-1}\{W_{(i+1,k)}+W_{(k+1,j-1)}\} +c
\end{array}\right.\\\\

VBI(i,j)&=&\min_{i<i'<j'<j}\{V_{(i',j')}+ebi(i,j,i',j')\} +c \\\\

F_{(j)}&=&\min\left\{\begin{array}{ll}
F_{(j-i)} \\
\min_{1\le i< j} (V_{(i,j)} + F_{(j-1)})
\end{array}\right. \text{(Free Energy)}\\\\

\end{array}\]

\subsection{Comments}

VBI stores scores for internal loops and bulges.
Notice in VBI, two moving indices $i', j'$. In general this gives a quadratic complexity for that particular recurrence, making the overall complexity of the algorithm $O(n^4)$. In practice (as read in Arpith's work), we have that $i' - i + j'- j < 30$, so the complexity is reduced to $O(n^3)$, albeit with a large constant.

%XXX: missing: eh(i,j), ebi(i,j,i',j'), es(i,j), ...



\end{document}

% LaTeX Template for a short article
\documentclass[11pt]{article}
\usepackage{amssymb,amsmath,amsthm,hyperref,verbatim,pict2e,graphicx,marvosym,array,booktabs}
\hypersetup{colorlinks,citecolor=black,filecolor=black,linkcolor=black,urlcolor=black}
\usepackage[usenames]{xcolor} % color names ,dvipsnames,svgnames,table
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\pagestyle{empty} % remove page numbers

\oddsidemargin   0.0cm
\evensidemargin  0.0cm
\topmargin       0.0cm
\headheight      0.0cm
\headsep         1.0cm
\textheight     21.0cm
\textwidth      16.0cm
\parskip         0.1cm
\parindent       0.0cm
\footskip        1.0cm

\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{8pt}{0pt}
\titlespacing{\subsection}{0pt}{8pt}{0pt}
\titlespacing{\subsubsection}{0pt}{8pt}{0pt}
\usepackage{enumitem}
\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\setenumerate{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\def\ul{\begin{itemize}}
\def\ule{\end{itemize}}
\def\ol{\begin{enumerate}}
\def\ole{\end{enumerate}}

% Recurrence visualisation
\newcommand\Cd[3][0,-1]{\put(#2){\put(.5,.5){\circle*{.3}}\put(.5,.5){\linethickness{1.5pt}\vector(#1){#3}}}} % dependency [dx,dy]{x,y}{len}
\def\Cg#1{\put(#1){\color{lightgray}\put(0,0){\polygon*(0,0)(0,1)(1,1)(1,0)}}} % grayed cell (not to store
\def\Cz#1{\put(#1){\put(0,.35){\parbox{1\unitlength}{\centering\bf 0}}}} % zero-init cell
\def\Cm{\put(6.5,1.5){\circle*{.4}}\multiput(0,0)(1,0){9}{\line(0,1){8}}\multiput(0,0)(0,1){9}{\line(1,0){8}}} % matrix base

\title{DP problems of interest}
\author{Manohar Jonnalagedda, Thierry Coppey}
\date{}
\begin{document}
\maketitle
\pagestyle{headings}

% ----------------------------------------------------------------------------------------------------------------------------------------
\section{Introduction}
\subsection{Definitions}\ul
\item Block of computation: a block is simply a part of the DP matrix that we want to compute.
\item Wavefront: this is the place around which computation happen, typically. There should be some memory to store intermediate information between block of computations
\ule

\subsection{Problems classification}
In the literature, dynamic programming problems (DP) are classified according to two criteria:

\textbf{Monadic/polyadic}\ul
\item \textbf{Monadic:} on the right hand-side of the recurrence formula, only one term appears. For instance, Smith-Waterman with constant penalty is monadic
	\[M_{(i,j)}=\max\left\{\begin{array}{l} 0 \\ M_{(i-1,j-1)}+{\rm cost}(S(i),T(j))\\ M_{(i-1,j)}-d\\ M_{(i,j-1)}-d \end{array}\right. \]
\item \textbf{Polyadic:} when multiple terms of the recurrence occur in the right and-side of the recurrence formula. For instance Fibonacci is polyadic: \[F(n) = F(n-1) + F(n-2)\]
\ule

\textbf{Serial/non-serial} \ul
\item \textbf{Serial:} when the solution depends only of a fixed number of immediately previous solutions (i.e. neighbor cells). For instance Fibonacci is serial (it accesses only 2 cells backward).
\item \textbf{Non-serial:} when the solution depends of an arbitrary number of previous solutions. Typically Smith-Waterman with arbitrary gap penalty and Nussinov are non-serial:
	\[M_{(i,j)}=\max\left\{\begin{array}{l} ... \\ M_{(i,j-1)}\\ \max\limits_{i<k<j} [ M_{(i,k)}+M_{(k+1,j)} ] \end{array}\right. \]
\ule

\subsection{Simplifications}
\subsubsection{Calculus}
In some special case, it is possible to transform a non-serial problem into a serial problem, if we can embed the non-serial term into an additional aggregation matrix. For example:
	\[M_{(i,j)}=\max\left\{\begin{array}{l} \max\limits_{k<i} M_{(k,j)}
	\\ \sum\limits_{k<i, l<j}M_{(k,l)} \end{array}\right.
	\implies M_{(i,j)}=\max\left\{\begin{array}{l} C_{(k,j)} \\ A_{(i-1,j-1)} \end{array}\right.\]
Where the matrix $C$ stores the maximum along the column and matrix $A$ stores the sum of the array of the previous elements. Both can be easily computed with an additional recurrence:
	\[\begin{array}{rcl} C_{(i,j)}&=&\max(C_{(i-1,j)}, M_{(i,j)}) \\
	A_{(i,j)}&=&A_{(i-1,j)}+A_{(i,j-1)}-A_{(i-1,j-1)}+M_{(i,j)}\end{array}\]

This simplification avoids the non-serial dependencies at the cost of extra storage in the wavefront, unfortunately, it might be applicable only for some special cases.

\subsubsection{Precomputations}
When a calculus transformation is impossible, it might be worth to interleave a computation phase that will aggregate some of the results that are necessary to the computation block. For instance, for Nussinov term $\max_{i<k<j} [ M_{(i,k)}+M_{(k+1,j)} ]$, we can precompute it over all rows and columns of the block, and for all elements that are not part of the block, then pass these partial results together at the block launch to finish the computation.

On GPU, this could be done by interleaving a new kernel for this specific purpose, on FPGA, this could be done by preparing the maximums in another memory area whose pointer will later be passed to the co-processor. We may notice that since this phase is at the same time necessary for both architecture, and can be run independently, we can both execute them concurrently and mix between architectures (use CUDA for pre-computation and FPGA for actual tile computation for instance).

% ----------------------------------------------------------------------------------------------------------------------------------------
\section{Problems of interest}
We describe problems structures: inputs, cost matrices and backtracking matrix. These all have an alphabet (that must be bounded in terms of bit-size). Unless otherwise specified, we adopt the following conventions:\ul
\item Matrices dimensions are implicitly specified by number of indices and their number of elements is usually the same as the input length.
\item Number are all unsigned integers
\item Problem dimension is $m,n$ (or $n$) indices $i,j$ ranges are respectively $0\le i<m$, $0\le j<n$.
\item Unless otherwise specified, the recurrence applies to all non-initialized matrix elements.
\ule
We describe the problem processing in terms of both initialization and recurrences.

% ----------------------------------------------
\newpage
\subsection{Smith-Waterman (simple)}
Problem: matching two strings $S$, $T$ with $|S|=m, |T|=n$.

Matrices: $M_{m \times n}, B_{m \times n}$

Alphabets:\ul
\item Input: $\Sigma(S)=\Sigma(T)=\{a,c,g,t\}$.
\item Cost matrix: $\Sigma(M) = [0..z], z=\max({\rm cost(\_)}) \cdot \min(m,n)$
\item Backtrack matrix: $\Sigma(B)=\{stop,W,N,NW\}$
\ule

Initialization:\ul
\item Cost matrix: $M_{(i,0)}=M_{(0,j)}=0$.
\item Backtrack matrix: $B_{(i,0)}=B_{(0,j)}=stop$.
\ule

Recurrence:
\[M_{(i,j)}=\max\left\{\begin{array}{l|l}
	0 & stop\\
	M_{(i-1,j-1)}+{\rm cost}(S(i),T(j)) & NW\\
	M_{(i-1,j)}-d & N\\
	M_{(i,j-1)}-d & W
\end{array}\right\}=B_{(i,j)} \]

Backtracking: starts from the cell $M_{(m,j)} \cup M_{(i,n)} $

Visualisation:
\begin{center}\setlength{\unitlength}{.6cm}\begin{picture}(8,9)
	\put(-.5,7.5){S}\put(-.35,7.4){\linethickness{1pt}\vector(0,-1){2}}
	\put(.2,8.2){T}\put(.8,8.4){\linethickness{1pt}\vector(1,0){2}}
	\Cz{0,0}\Cz{0,1}\Cz{0,2}\Cz{0,3}\Cz{0,4}\Cz{0,5}\Cz{0,6}\Cz{0,7}
	\Cz{1,7}\Cz{2,7}\Cz{3,7}\Cz{4,7}\Cz{5,7}\Cz{6,7}\Cz{7,7}
	\Cd{6,2}{0.8}
	\Cd[1,0]{5,1}{0.8}
	\Cd[1,-1]{5,2}{0.8}
\Cm\end{picture}\end{center}

Optimizations\ul
\item In serial (monadic) problems we can avoid building the matrix $M$ by only maintaining the 3 last diagonals in memory (one for the diagonal element, one for horizontal/vertical, and one being built). This construction extends easily to polyadic problems where we need to maintain $k+2$ diagonals in memory where $k$ is the maximum backward lookup.
\ule

% ----------------------------------------------
\newpage
\subsection{Smith-Waterman (with gap extension at different cost)}
Problem: matching two strings $S$, $T$ with $|S|=m, |T|=n$.

Matrices: $M_{m \times n}, E_{m \times n}, F_{m \times n}, B_{m \times n}$

Alphabets:\ul
\item Input: $\Sigma(S)=\Sigma(T)=\{a,c,g,t\}$.
\item Cost matrices: $\Sigma(M) = \Sigma(E) = \Sigma(F) = [0..z], z=\max({\rm cost(\_)}) \cdot \min(m,n)$
\item Backtrack matrix: $\Sigma(B)=\{stop,W,N,NW\}$
\ule

Initialization:\ul
\item No gap cost matrix: $M_{(i,0)}=M_{(0,j)}=0$.
\item T-gap extension cost matrix: $E_{(i,0)}= 0$ \textit{<<eat S chars only>>}
\item S-gap extension cost matrix: $F_{(0,j)}= 0$
\item Backtrack matrix: $B_{(i,0)}=B_{(0,j)}=stop$.
\ule

Recurrence for the cost matrices:
\[\begin{array}{rcl}
M_{(i,j)}&=&\max\left\{\begin{array}{l|l}
	0 & stop\\
	M_{(i-1,j-1)}+{\rm cost}(S(i),T(j)) & NW\\
	E_{(i,j)} & N\\
	F_{(i,j)} & W
\end{array}\right\}=B_{(i,j)}\\
\\
E_{(i,j)}&=&\max\left\{\begin{array}{l|l}
	M_{(i, j-1)} - \alpha & NW\\
	E_{(i,j-1)} - \beta & N\\
\end{array}\right\}=B_{(i,j)}\\
\\
F_{(i,j)}&=&\max\left\{\begin{array}{l|l}
	M_{(i-1,j)} - \alpha & NW\\
	F_{(i-1,j)} - \beta & W\\
\end{array}\right\}=B_{(i,j)}
\end{array}\]

{\color{red} XXX: Are we sure we do not need to maintain separate backtrack tables for the gap extensions?}


Otherwise written as:
\[M_{(i,j)}=\max\left\{\begin{array}{l|l}
	0 & stop\\
	M_{(i-1,j-1)}+{\rm cost}(S(i),T(j)) & NW\\
	\max_{1 \le k \le j-1} M_{(i,k)} - \alpha - (j-1-k) \cdot \beta & N\\
	\max_{1 \le k \le i-1} M_{(k,j)} - \alpha - (i-1-k) \cdot \beta & W\\
\end{array}\right\}=B_{(i,j)} \]

{\color{red} XXX: how to simplify this relation, can we move the max in the wavefront?}

Recurrence visualisation:

{\color{red} XXX: todo}

Optimizations:

{\color{red} XXX: todo}

% ----------------------------------------------
\newpage
\subsection{Convex polygon triangulation}
Problem: triangulating a polygon with least cost overall for added edges

Alphabets:\ul
\item Input: $\Sigma(S)= $ a matrix of costs for every possible edge in the polygon. The 
size of the input is $\frac{n^2}{2}$, where $n$ is the number of vertices in the polygon. 
\item Cost matrix: $\Sigma(M)=\mathbb{N}$. The sum of cost of edges is not "bounded".
\item Backtrack matrix: $\Sigma(B)=\{n\}$
\ule

Recurrence:
\[M_{(i,j)}= \max_{i<k<j}M_{(i,k)}+M_{(k+1,j)} + c(i,k) | k = B_{(i,j)} \]

It is interesting to note that even in the sequential world, this problem is best solved 
by filling the diagonals, ie. computing sub-solutions for all polygons on size $k$ before
those of size $k+1$.

% ----------------------------------------------


% ----------------------------------------------
\newpage
\subsection{Matrix chain multiplication}
Problem: find an optimal parenthesizing of the multiplication of $n$ matrices $A_i$ of dimension $r_i \times c_i$ \textit{(r=rows, c=columns)}. 

Alphabets:\ul
\item Input: matrix size is defined as pairs of integers $(r_i,c_i)$.
\item Cost matrix: the cost is an integer (might blow up, use a float or a double?).
\item Backtrack matrix: $\Sigma(B)=\{stop\} \cup \{0..n\}$ with $n$ the input length.
\ule

Initialization (line=length of sequence, column=start):\ul
\item Cost matrix: $M_{(0,j)}=0$.
\item Backtrack matrix: $B_{(0,j)}=stop$.
\ule

Recurrence for the cost matrix:
\[M_{(i,j)}=\min_{i\le k<j}\left\{\begin{array}{l|l}
	M_{(i,k)}+M_{(k,j)}+r_i \cdot r_k \cdot col_j & k
\end{array}\right\}=B_{(i,j)} \]

% ----------------------------------------------
\subsection{Nussinov algorithm}
Problem: folding a RNA string $S$ over itself.

Alphabets:\ul
\item Input: $\Sigma(S)=\{A,C,G,U\}$.
\item Cost matrix: $\Sigma(M)=\{0..n\}, n={\rm length}(S)/2$
\item Backtrack matrix: $\Sigma(B)=\{stop,W,S,SW, 1..n\}$
\ule

Initialization: \ul
\item Cost matrix: $\left\{\begin{array}{l} M_{(i,i)}=0 \\ M_{(i,i-1)}=0 \end{array}\right. \forall i \in 1..{\rm length}(S)$
\item Backtrack matrix: $\left\{\begin{array}{l} B_{(i,i)}=stop \\ B_{(i,i-1)}=stop \end{array}\right.  \forall i \in 1..{\rm length}(S)$
\ule

Recurrence:
\[M_{(i,j)}=\max\left\{\begin{array}{l|l}
	M_{(i+1,j-1)}+\omega(i,j) & SW\\
	M_{(i+1,j)} & S\\
	M_{(i,j-1)} & W\\
	\max_{i<k<j}M_{(i,k)}+M_{(k+1,j)} & k
\end{array}\right\} = B_{(i,j)} \]
With $\omega(i,j)=1$ if $i,j$ are complementary. 0 otherwise.

% ------------------------------------------------------------------------------------------------
\newpage
\subsection{Zuker folding}
Problem: folding a RNA string $S$ over itself.

Alphabets:\ul
\item Input: $\Sigma(S)=\{A,C,G,U\}$.
\item Cost matrix: $\Sigma(W)=\Sigma(V)=\Sigma(F)=\{0..n\},n=$
\item Backtrack matrix: $\Sigma(B)=\{\}$
\ule

Initialization:\ul
\item Cost matrices:\ul
	\item XXX: $W$,$V$
	\item $F_{(0)}=0$
\ule
\item Backtrack matrix: $B$
\ule

Recurrence:
\[\begin{array}{rcl}
W_{(i,j)}&=&\min\left\{\begin{array}{l|l}
	W_{(i+1,j)}+b & S\\
	W_{(i,j-1)}+b & W\\
	V_{(i,j)}+\delta(S_i,S_j) & ?V? \\
	\min_{i<k<j}W_{(i,k)}+W_{(k+1,j)} & ?k?
\end{array}\right.\\
\\
V_{(i,j)}&=&\min\left\{\begin{array}{ll}
	\infty \qquad\qquad\qquad\qquad {\rm if}(S_i,S_j) \text{ is not a base pair}\\
	eh(i,j)+b \qquad\qquad\, \text{otherwise} \\
	V_{(i+1,j-1)}+es(i,j) \\
	VBI(i,j) \\
	\min_{i<k<j-1}\{W_{(i+,k)}+W_{(k+1,j-1)}\} +c
\end{array}\right.\\
\\
 F_{(j)}&=&\min\left\{\begin{array}{ll} F_{(j-1)} \\ \min_{1\le i< j} (V_{(i,j)} + F_{(j-1)})
%
\end{array}\right. \text{(Free Energy)}\\\\
VBI(i,j)&=&\min_{i<i'<j'<j}\{V_{(i',j')}+ebi(i,j,i',j')\} +c
\end{array}\]
%







%%\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash}m{#1}}
%\newcolumntype{C}{@{\hspace{7pt}}c@{\hspace{7pt}}}
%\def\mnl{\rule{0pt}{2.6ex}\rule[-1.2ex]{0pt}{0pt} \\ \hline}
%$\begin{array}{|C|C|C|C|C|C|} \hline
%0 & 0 & 0 & 0 & 0 & 0 \mnl
%0 &  &  &  &  & \mnl
%0 & M_{23}  &  &  &  & \mnl
%0 &  & \sum  &  &  & \mnl
%0 &  &  &  &  & \mnl
%0 &  &  &  &  & \mnl
%\end{array}$


\end{document}

\input{style.sty}

\title{Draft}
\begin{document}
\maketitle
\pagestyle{headings}
\setcounter{tocdepth}{2} \tableofcontents

% ------------------------------------------------------------------------------------------------
\newpage
\section{Planning/draft}
\subsubsection*{Contributions}\ul
\item Identify DP classes and give a generalization of their shape/dependency graph
\item Provide state of the art/on par parallel implementations for all these classes
\item Generalize/simplify expression by using a DSL to implement these problems
\item Provide multi-platform support (CPU/GPU/FPGA)
\ule

\subsubsection*{Todo @TCK}
\begin{verbatim}
1. check the vanilla implementation
2. read ADP legacy parsers
3. presentation for Thursday
4. Propose an IR and code generators related to
Note. We only need to generate code directly for CUDA, no interaction. Look at string templating .
"""xxx $var xxx"""
- Fork LMS on GitHub and cleanup all the CUDA generator

% class problem into categories
% think of hashmap implementation. is it useful ?
- CPU+GPU implementation for serial (NS?) problems larger than device memory
- optimizations for in-memory computations

1. restriction vs ADP: we give only the best answer, do we care about semi-optimal ones?
2. separate initialization (base cases, atoms) and processing (rules, recurrences) => less cases to handle (no if)
3. use sub-cell parallelism (seen in 2 papers already)

Plan:
We need to extend the language to support all 3 cases
- Parallelogram is triangular + cyclic => add a cyclic keyword that applies to the whole problem
- Square: what is the transform being done in ADP extension?

1. optimize: terminals of bounded yield + binary decomposition if possible
2. transform into: axioms (init/fixed) + rules (iterations)
3. transform to code
     axiom(i := k(i)) -> M[i,i]=k(i)
     rule(x~y := h(x,y)) -> M[i,j]=forall_i<k<j h(M[i,k],M[k,j]) for both unbounded yield
     rule(x~y_l := h(x,y)) -> M[i,j]=forall_j-l<k<j h(M[i,k],M[k,j]) for y bounded by l
     rule(x~y_l := h(x,y)) -> M[i,j]=h(M[i,j-l],M[j-l,j]) for y of exactly l

XXX: how to encode multi-dimensional matrices
1. assume they have the same type put one after another => different dimensions ok
2. assume of same size => put into a struct
=> but using different pointers seems more reliable => completely different matrices => fixed list of matrices by dimensionality (O(1), O(n), O(n^2), ...) of structs (determined by number of indices to access object)
\end{verbatim}

\subsubsection*{Todo @Manohar}
\begin{verbatim}
- generate chain matrix recurrences to generate implementation
- more theory behind what we want to support, how we encompass all cases, ...
\end{verbatim}

\subsection*{Plan} \ol
\item {\color{gray}
\textbf{Problems description:} parallel tree-raking, does not share much with other algorithms (sparse version of computations, might not scale efficiently). Most common patterns are already enclosed by the above problems. Real input size is around 300K. We might want to also look at an $O(n^3)$-space-complex problem (like matching 3 strings $S,T,U$).}

All the problems we consider use 2D storage matrix, their dependencies are an union of: \ul
\item Serial dependencies
\item Non-serial horizontal or vertical dependencies (1D non-serial)
\item Non-serial horizontal+vertical dependencies in the form $M_{(i,j)} = {\rm op}_k f ( M_{(i,k)}, M_{(k,j)} ) $
\item We have not found other type of dependencies in the literature
\ule

\item \textbf{User facing language:} goals are flexibility and compactness.\ul
	\item User-facing language should be similar to related paper \cite{adp_gpu} or \href{http://hackage.haskell.org/package/ADPfusion}{\it ADP fusion}. We want to reuse the transformation mapping (problem description) $\mapsto$ (kernel implementation) for a single element.
	\item We also may want to try to make implicit transformation for code like \\
		{\tt @DP def Fib(n:Int) = if (n<=2) return 1 else Fib(n-1)+Fib(n-2)}.
	\item Windowing: the user should be able to force a windowing (i.e. force a non-serial problem to be a $k$-polyadic serial problem).	
	\item 3 different cases: we care about backtrack cost or both.
	\item Backtracking: create an operator that produces the whole backtrack sequence indices.
	\ule

$\implies$ \emph{end of October}.

\item \textbf{Prototyping:} get a prototype to understand difficulties and share common base. \\ Implement a working prototype of \nameref{aswat} on CPU (for correctness), and specific platform (CUDA/FPGA). This will give us an idea of how to implement the general case. We also need to benchmark and compare both implementations to see how we compare to existing implementations and see the direction to take (which decide is faster and by how much). Here we aim to do as good an implementation for the specific platform (CPU/GPA) as possible.\\
$\implies$ \emph{end of October}.

\item \textbf{Baseline:} Also use benchmarks provided by existing implementations as baselines.\\
$\implies$ \emph{end of October}.

\item \textbf{Formalize IR:} describe the intermediate representation, formalize the framework provided to the code generators (i.e. memory management, ...).
\item \textbf{Full compiler stack:} enrich the compiler stack from both top-down (translate best user-facing language parsers) and bottom-up (parametric code generators), core of the work.
\item \textbf{Benchmark:} make sure implementations are correct, compare them other papers.
\item \textbf{Optimizations:} improve as much as possible / as long as time permits
\ole

% --------------------------------------------------------------------------------
\input{problems}
\input{benchmarks}

% --------------------------------------------------------------------------------
\bibliographystyle{plain}
\bibliography{bibliography.bib}
\end{document}
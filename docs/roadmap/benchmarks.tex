\input{style.sty}

\title{Benchmarks}
\begin{document}
\maketitle
\pagestyle{headings}

The goal of this document is to see how we are comparing with other papers in terms of performance and to note performance progress of the improvements.

% --------------------------------------------------------------------------------
\subsection*{Graphic cards\footnote{Source: \url{http://en.wikipedia.org/wiki/Comparison_of_Nvidia_graphics_processing_units}}}
\def\unt#1{& \footnotesize #1}
\begin{center}\begin{tabular}{lrrrr} \toprule
\bf Paper		&				& \bf -- 		& \bf  ATLP\cite{gpu_atlp} & \bf SWMB\cite{swat_mega} \\ \midrule
Serie		&				& GeForce	& Tesla	& GeForce  \\
Model		&				& GT 650M	& C1060	& GTX 560 \\
Architecture	&				& Kepler		& GT200	& GF114 \\
Capability		&				& 3.0		& 1.3	& 2.1 \\
Memory \unt{Mb}				& 1024		& 4096	& 1024 \\
CUDA cores &					& 384		& 240	& 384 \\
Core clock \unt{MHz}			& 756		& 1300	& 822 \\
Memory clock \unt{MHz}			& 1953		& 1600	& 4008 \\
Memory bus \unt{bit}				& 128		& 512	& 256 \\
Memory bandwidth \unt{GB/s}		& 28.8		& 102.4	& 128.26 \\
Processing power \unt{GFLOPS}	&564.5		& 622.08	& 1263.4 \\ \midrule
\bf Processing speedup & 		& 1			& 1.07	& \\
\bf Memory speedup & 			& 1			& 3.55	& \\ \bottomrule
\end{tabular}\end{center}

% --------------------------------------------------------------------------------
\subsection*{Results}
\subsubsection*{ATLP\cite{gpu_atlp}}
\begin{center}\begin{tabular}{lrrrrrrrrrr} \toprule
\bf Matrix size & 128 & 256 & 512 & 1024 & 1536 & 2048 & 2560 & 3072 & 3584 & 4096 \\ \midrule
\bf No split & 0.07 & 0.09 & 0.19 & 0.59 & 1.27 & 2.25 & 3.51 & 5.07 & 6.92 & 9.06 \\
\bf Split at 1 & 0.06 & 0.07 & 0.08 & 0.14 & 0.26 & 0.47 & 0.77 & 1.21 & 1.80 & 2.57 \\ \bottomrule
\end{tabular} \\[4pt] Matrix multiplication timing in seconds \end{center}

\subsubsection*{SWMB\cite{swat_mega}}
\begin{center}\begin{tabular}{rlrr} \toprule
\bf Matrix size & \bf Sequences & \bf No pruning & \bf Pruning \\ \midrule
162K $\times$ 172K		& NC\_000898.1, NC\_007605.1	& 1.2 & 1.2 \\
543K $\times$ 536K		& NC\_003064.2, NC\_000914.1	& 10.8 & 10.8 \\
1044K $\times$ 1073K	& CP000051.1, AE002160.2		& 40.3 & 36.2 \\
3147K $\times$ 3283K	& BA000035.2, BX927147.1		& 363.6 & 363.2 \\ 
5227K $\times$ 5229K	& AE016879.1, AE017225.1		& 962.4 & 469.5 \\
7146K $\times$ 5227K	& NC\_005027.1, NC\_003997.3	& 1309 & 1309 \\
23012K $\times$ 24544K	& NT\_033779.4, NT\_037436.3	& 19701 & 19694 \\
59374K $\times$ 23953K	& NC\_000024.9, NC\_006492.2	& 49634 & 46869 \\
32799K $\times$ 46944K	& BA000046.3, NC\_000021.7		& 53869 & 29133 \\ \bottomrule
\end{tabular} \\[4pt] Smith-Waterman, timing in seconds \end{center}


% --------------------------------------------------------------------------------
\subsubsection*{Intermediate results}
\begin{center}\begin{tabular}{cclrrr} \toprule
\bf Matrix & \bf Block & \bf Comment & \bf R(ms) & \bf T(ms) & \bf P(ms) \\ \midrule
1024 & 32 & CPU baseline & 1965.63 & 1191.13 & 6069.66 \\
1024 & 32 & GPU baseline & 838.67 & 499.87 & 516.82 \\
1024 & 32 & GPU sync improved & 642.37 & 316.40 & 343.29 \\
2048 & 32 & CPU baseline & 27229.28 & 15296.09 & 57323.04 \\
2048 & 32 & GPU P 32 blocks & 2864.27 & 1427.21 & 2096.34 \\
\bottomrule \end{tabular} \\[4pt]
Best timings for R=rectangle, T=triangle, P=parallelogram\footnote{$\le 32$ blocks on my GPU to prevent deadlock}
\end{center}

% --------------------------------------------------------------------------------
\subsection*{Micro-benchmarks \footnote{Micro-benchmark do not provide extensive results as they stand only to validate implementation strategies.}}
Problem are described by the triplet <matrix size/block size/shape>.
\ul
\item It has been put in evidence in a previous work\footnote{Performance Evaluation 2012 miniproject, \it Performance Evaluation of Mersenne arithmetic on GPUs} that with Mac OS operating system, using the GPU exclusively for CUDA or combining with UI display may affect the performance (GeForce 330M, architecture 1.2). With the new architecture (3.0, GeForce 650M), this difference has been reduced to less than 3.5\% with the decoupling of UI and CUDA performing best. So in micro-benchmarks, we can safely ignore the graphic card usage.

\item Synchronization  between blocks\ul
	\item Removing {\tt \_\_threadfence()} before the synchronization is not syntactically correct but still remains correct, this validates the observation made by \cite{gpu_barrier}. Speedup for <1024/32/*> are 67ms (parallelogram) 100ms (triangle) 180ms (rectangle).
	\item In the parallelogram case, using all threads to monitor other blocks status instead of 1 thread results in a 6.4x speedup (22.72$\to$3.52ms) on <1024/32/para>.
	\ule
\ule

% --------------------------------------------------------------------------------
\bibliographystyle{plain}
\bibliography{bibliography.bib}
\end{document}

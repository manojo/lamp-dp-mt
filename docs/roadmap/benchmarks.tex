%\input{style.sty}

%\title{Benchmarks}
%\begin{document}
%\maketitle
%\pagestyle{headings}

\newpage
\section{Benchmarks}

The goal of this document is to see how we are comparing with other papers in terms of performance and to note performance progress of the improvements.

% --------------------------------------------------------------------------------
\subsection*{Graphic cards\footnote{Source: \url{http://en.wikipedia.org/wiki/Comparison_of_Nvidia_graphics_processing_units}}}
\def\unt#1{& \footnotesize #1}
\begin{center}\begin{tabular}{lrrrr} \toprule
\bf Paper		&				& \bf -- 		& \bf  ATLP\cite{gpu_atlp} & \bf SWMB\cite{swat_mega} \\ \midrule
Serie		&				& GeForce	& Tesla	& GeForce  \\
Model		&				& GT 650M	& C1060	& GTX 560 \\
Architecture	&				& Kepler		& GT200	& GF114 \\
Capability		&				& 3.0		& 1.3	& 2.1 \\
Memory \unt{Mb}				& 1024		& 4096	& 1024 \\
CUDA cores &					& 384		& 240	& 384 \\
Core clock \unt{MHz}			& 756		& 1300	& 822 \\
Memory clock \unt{MHz}			& 1953		& 1600	& 4008 \\
Memory bus \unt{bit}				& 128		& 512	& 256 \\
Memory bandwidth \unt{GB/s}		& 28.8		& 102.4	& 128.26 \\
Processing power \unt{GFLOPS}	&564.5		& 622.08	& 1263.4 \\ \midrule
\bf Processing speedup & 		& 1			& 1.07	& \\
\bf Memory speedup & 			& 1			& 3.55	& \\ \bottomrule
\end{tabular}\end{center}

% --------------------------------------------------------------------------------
\subsection*{Results}
\subsubsection*{ATLP\cite{gpu_atlp}}
\begin{center}\begin{tabular}{lrrrrrrrrrr} \toprule
\bf Matrix size & 128 & 256 & 512 & 1024 & 1536 & 2048 & 2560 & 3072 & 3584 & 4096 \\ \midrule
\bf No split & 0.07 & 0.09 & 0.19 & 0.59 & 1.27 & 2.25 & 3.51 & 5.07 & 6.92 & 9.06 \\
\bf Split at 1 & 0.06 & 0.07 & 0.08 & 0.14 & 0.26 & 0.47 & 0.77 & 1.21 & 1.80 & 2.57 \\ \bottomrule
\end{tabular} \\[4pt] Matrix multiplication timing in seconds \end{center}

\subsubsection*{SWMB\cite{swat_mega}}
\begin{center}\begin{tabular}{rlrr} \toprule
\bf Matrix size & \bf Sequences & \bf No pruning & \bf Pruning \\ \midrule
162K $\times$ 172K		& NC\_000898.1, NC\_007605.1	& 1.2 & 1.2 \\
543K $\times$ 536K		& NC\_003064.2, NC\_000914.1	& 10.8 & 10.8 \\
1044K $\times$ 1073K	& CP000051.1, AE002160.2		& 40.3 & 36.2 \\
3147K $\times$ 3283K	& BA000035.2, BX927147.1		& 363.6 & 363.2 \\ 
5227K $\times$ 5229K	& AE016879.1, AE017225.1		& 962.4 & 469.5 \\
7146K $\times$ 5227K	& NC\_005027.1, NC\_003997.3	& 1309 & 1309 \\
23012K $\times$ 24544K	& NT\_033779.4, NT\_037436.3	& 19701 & 19694 \\
59374K $\times$ 23953K	& NC\_000024.9, NC\_006492.2	& 49634 & 46869 \\
32799K $\times$ 46944K	& BA000046.3, NC\_000021.7		& 53869 & 29133 \\ \bottomrule
\end{tabular} \\[4pt] Smith-Waterman, timing in seconds \end{center}

% --------------------------------------------------------------------------------
\subsubsection*{Intermediate results}
\begin{center}\begin{tabular}{cclrrr} \toprule
\bf Matrix & \bf Block & \bf Comment & \bf R(s) & \bf T(s) & \bf P(s) \\ \midrule
1024 & 1 & CPU					& 1.965		& 1.191		& 6.069 \\
2048 & 1 & CPU					& 27.229		& 15.296		& 57.323 \\
4096 & 1 & CPU					& 			& 177.608	&  \\
1024 & 32 & GPU baseline			& 0.838		& 0.500		& 0.516 \\
1024 & 32 & GPU sync improved		& 0.642		& 0.316		& 0.343 \\
2048 & 32 & GPU P $\le32$ blocks		& 2.864		& 1.427		& 2.096 \\
4096 & 32 & GPU 8 splits				& 21.902		& 8.841		& 16.767 \\
8192 & 32 & GPU 64 splits			& 159.058	& 62.064		& 135.793 \\
12288 & 32 & GPU 256 splits			& 419.030	& 196.971	& 460.912 \\
\bottomrule \end{tabular} \\[4pt]
Best timings for R=rectangle, T=triangle, P=parallelogram\footnote{$\le 32$ blocks on my GPU to prevent deadlock}
\end{center}

% --------------------------------------------------------------------------------
\subsection*{Micro-benchmarks \footnote{Micro-benchmark do not provide extensive results as they stand only to validate implementation strategies.}}
Problem are described by the triplet <matrix size/block size/shape>.
\ul
\item It has been put in evidence in a previous work\footnote{Performance Evaluation 2012 miniproject, \it Performance Evaluation of Mersenne arithmetic on GPUs} that with Mac OS operating system, using the GPU exclusively for CUDA or combining with UI display may affect the performance (GeForce 330M, architecture 1.2). With the new architecture (3.0, GeForce 650M), this difference has been reduced to less than 3.5\% with the decoupling of UI and CUDA performing best. So in micro-benchmarks, we can safely ignore the graphic card usage.

\item Synchronization  between blocks\ul
	\item Removing {\tt \_\_threadfence()} before the synchronization is not syntactically correct but results still remains valid, this confirms the observation made by \cite{gpu_barrier}. Speedup for <1024/32/*> are 67ms (parallelogram) 100ms (triangle) 180ms (rectangle).
	\item In the parallelogram case, using all threads to monitor other blocks status instead of 1 thread results in a 6.4x speedup (22.72$\to$3.52ms) on <1024/32/para>.
	\ule
\item Using multiple threads per matrix cell:
In the case of a triangular matrix, at each step, the number of cells to be computed (on the diagonal) decrease while the computation complexity increase (there is one more dependency). According to \cite{gpu_atlp}, the solution lies in adaptive thread mapping, using more than one thread to compute one matrix cell, depending on the complexity. However, in our setup (memory layout+algorithm+hardware), we did not found any improvement by doing so. We want to explore the reason for that: we pose as hypothesis that the bandwidth is the bottleneck of our setup and test it.\ul
\item

First we need to prove that we use almost all the available memory bandwidth: for matrix multiplication, in a triangular matrix, we have
\[\text{Total transfer}=\frac{n(n+1)}{2} \text{ writes} + \sum_{i=0}^{n-1} 2 i \cdot (n-i) \text{ reads}\]
where each write is 10 bytes (long+short), and each read is 8 bytes (long). For $n=4096$ we transfer
% n*(n + 1)/2*12 + Sum[2*i*(n - i), {i, 0, n - 1}]*8
183'352'614'912 bytes which corresponds to 183.35GB. In 8.841 seconds, we can transfer theoretically at most $8.841\cdot 28.8 = 254 \rm GB$. Hence we can say that 72\% of the algorithm running time is spent into memory accesses.

\item On a 4096 matrix, if we assume that ATLP card would have the same bandwidth as our card, their running time would be
\[2.57 \cdot (1-.72) + 2.57 \cdot 0.72 \cdot \tfrac{102.4_{GB/s}}{28.8_{GB/s}} = 9.43\rm s_{\text{ ATLP}} > 8.84\rm s_{\text{ our}}\]
Which shows that our algorithm is comparable to theirs. However, we must avoid a close comparison because the fundamental hardware differences would make a tight computation almost intractable (additionally, we do not have ATLP source code).
\ule
As a conclusion, (1) we must remain away to invalidate their result as previous hardware generations might be subject to more constraint to our hardware and (2) we are on par, if not better with one of the best current implementation.

\item Reducing the number of threads launched at different splits of the algorithm (especially in latest splits in rectangular and triangular shapes) does not bring any speedup. Even worse, it slows down slightly the computation. We might attribute this to a better constant transformation by the compiler. Hence, having many idle threads does not impede performance.
\ule

%\end{document}

CONTRIBUTIONS:
-------------------------------
- identify DP classes and give a generalization of their shape/dependency graph
- provide state of the art/on par parallel implementations for all these classes
- generalize/simplify expression by using a DSL to implement these problems
- provide multi-platform support (CPU/GPU/FPGA)

LIST OF DP PROBLEMS:
-------------------------------
Detailed problems:

We might need to introduce new parameters for serial
- delay (access 1 or up to k elements backwards)
- number of cost matrices to maintain

- SWat simple
- SWat affine gap extension
- SWat arbitrary gap cost
- Convex polygon triangulation
- Matrix chain multiplication
- Nussinov
- Zuker folding

I guess we need between 20-50 DP problems to show that we identified all types of problems

Dijkstra shortest path, Fibonacci, Balanced 0-1 matrix, Checkerboard, Tower of Hanoi
Recurrent solutions to lattice models for protein-DNA binding
Backward induction as a solution method for finite-horizon discrete-time dynamic optimization problems
Method of undetermined coefficients can be used to solve the Bellman equation in infinite-horizon, discrete-time, discounted, time-invariant dynamic optimization problems
Many string algorithms including longest common subsequence, longest increasing subsequence, longest common substring, Levenshtein distance (edit distance).
Many algorithmic problems on graphs can be solved efficiently for graphs of bounded treewidth or bounded clique-width by using dynamic programming on a tree decomposition of the graph.
The Cocke–Younger–Kasami (CYK) algorithm which determines whether and how a given string can be generated by a given context-free grammar
Knuth's word wrapping algorithm that minimizes raggedness when word wrapping text
The use of transposition tables and refutation tables in computer chess
The Viterbi algorithm (used for hidden Markov models)
The Earley algorithm (a type of chart parser)
The Needleman–Wunsch and other algorithms used in bioinformatics, including sequence alignment, structural alignment, RNA structure prediction.
Floyd's all-pairs shortest path algorithm
Pseudo-polynomial time algorithms for the subset sum and knapsack and partition problems
The dynamic time warping algorithm for computing the global distance between two time series
The Selinger (a.k.a. System R) algorithm for relational database query optimization
De Boor algorithm for evaluating B-spline curves
Duckworth–Lewis method for resolving the problem when games of cricket are interrupted
The Value Iteration method for solving Markov decision processes
Some graphic image edge following selection methods such as the "magnet" selection tool in Photoshop
Some methods for solving interval scheduling problems
Some methods for solving word wrap problems
Some methods for solving the travelling salesman problem, either exactly (in exponential time) or approximately (e.g. via the bitonic tour)
Recursive least squares method
Beat tracking in music information retrieval.
Adaptive-critic training strategy for artificial neural networks
Stereo algorithms for solving the correspondence problem used in stereo vision.
Seam carving (content aware image resizing)
The Bellman–Ford algorithm for finding the shortest distance in a graph.
Some approximate solution methods for the linear search problem.
Kadane's algorithm for the maximum subarray problem.
	http://en.wikipedia.org/wiki/Dynamic_programming#A_type_of_balanced_0.E2.80.931_matrix

Shortest path in DAGs
Longest increasing subsequences
Knapsack
Shortest path
All pair shortest paths
Independent sets in trees
=> also see exercises for more problems
	http://www.cs.berkeley.edu/~vazirani/algorithms/chap6.pdf

Longest Increasing Subsequence
Longest Common Subsequence
Subset Sum
Coin Change
Family Graph
	http://www.algorithmist.com/index.php/Dynamic_Programming

Longest Increasing Subsequence
Edit/Levenshtein distance
Optimal Binary Search Trees
	http://www.cs.uiuc.edu/~jeffe/teaching/algorithms/notes/05-dynprog.pdf

Independent set on a tree
0-1 Knapsack

Floyd-Warshall
	http://www.cs.ucsb.edu/~suri/cs130b/NewDynProg.pdf












--------------------------------------------------------

Plan:
1. Define the main problem in focus and its class
   - Dimension
   - Serial/non-serial
   - Monadic/polyadic
2. Define some other problems that belong to this class or simpler classes
3. See the feasibility for FPGA (possibly helped by CPU)
4. Implement this algorithm in both CUDA and FPGA
   -> specify all parameters so that we can compare both implementations
5. Find a common IR for the problems
6. Write a code generator for both architectures from IR
   -> make sure all the test-DP are compiled well from IR
7. Find an expressive and compact language for the user-facing representation

Translation into C++:
http://bibiserv.cebitec.uni-bielefeld.de/macports/resources/download/

----------------------------------

Ideas for the language:
- we want maximum flexibility
- backtrack can be custom, we may want just to give back indices
- two problems: we care about cost and we care about whole backtrack.
	@DP def Fib(n:Int) = if (n<=2) return 1 else Fib(n-1)+Fib(n-2)
  should be translated into the more complete syntax that allows more flexibility
  => get inspiration from papers to write an internal language

- we may want to keep track of indices but we do not want to give access to the backtrack matrix / cost matrix

Language side:
- define a simple way to represent some DP programs
- define a comprehensive way to define them => micro-syntax
- define the mapping into IR (actually the IR might BE the micro-syntax)
- define optimizations to IR
  - making serializable by using aggregation functions/transformations
  - avoiding the cost matrix by moving it into the wavefront
- scheduling (with memory loads for non-serializable)
- code generation for both platforms
  - possibly choice to use which platform for what part (load/compute @ cpu/gpu/fpga)

----------------------------------

Core function F:
- in: s,t strings
- in: neighbor costs: top, left, top+left
- in: neighbor stats: top, left, top+left
- out: backtrack information
- out: cost(i,j)
- out: stats

To be most efficient, we need to get exact bit-sizes of
- strings' char
- costs: min/max values
- stats: min/max values
- backtrack

=> we might want to parallelize processing within a thread that operate on 32 or 64 bit inputs (to reduce memory accesses)



Some ideas:
- define a way to pack the characters => less memory transfer (i.e. GATC=>4 letters in 1 char)
- operate on some larger word (ex 64 bits) to increase thread locality and reduce memory accesses

- write a kernel that takes stats from left and r

		stats (x)
		 ||
		 vv
stats -> KK -> new_stats (y')
(y)		 || \
		 vv  --+ backtrack info (Bxy)
		 new_stats (x')

and compute the backtracking for its cell (optional as template boolean)

- multi-grain
  + within the 64b-words : multiple letters at once (thread level)
  + group threads into blocks that operate on (block level)
  + kernel that operates one after another (GPU/CPU level)
    => can we catch a stream's event at CPU level to alloc memory to get back results ?
- keep track of
  + stats O(m+n)
  + backtrack O(m*n)

- are we sure we need squares ?
  - rectangles can increase the full usage of all threads during (x-y) runs
  - assuming we put longest word vertically we play hot potato for stats
    left->right => block.x exchanges within block, last thread writes to
    global exchange memory (if short, so that it does not penalize running time)
  - we may go up to running in stripes

- if we put in diagonal-major data in the memory, why not stream all chunks like
  that using a cyclic buffer (?) => no empty slot
  ==> this might be useful for the backtrack information


- use extensively profiling : CUDA profiler

- Robust DP paper provides 2 insights: coalesced access + GPU synchronization
  => can we do a producer-consumer scenario at the block border so that we can
     execute a large diagonal horizontal/vertical swipe made of multiple blocks?
  => multiple swipes at different delays (so that we avoid syncing at every step

          s-->
  	  +----------------/------------+
	t | B1            /             |
	| +------/-------/              | KERNEL1 (keep benefit of t in shared mem)
	| | B2  /                       | delay between b1 and b2 can be 1% of line length
	v +----/------------------------+
	  |                             |
	  |                             | KERNEL2
	Memory organized as
	b1 -  -
	b1 b1 b1
	b1 b1 b1
	.. .. ..
	b1 b1 b1
	b2 b1 b1
	b2 b2 b1
	b2 b2 b2
	.. .. ..

For stats (aggregation of the table): we need to maintain both an horizontal and vertical front
For backtracking we need to maintain the whole table
     .. | . |
	..A | B | min/max: Min(D)=Min(Min(C),Min(B))
	----+---+ sum    : Sum(D)=Sum(C)+Sum(B)-Sum(A)
	..C | D | avg    : sum both #cells and values then divide at appropriate cell
	----+---+

we many not need to store what's the previous cell, however, the previous
cell information (usually limited range) is much more compact than the score(32-64 bits)

Memory usage of CPU:

we can potentially extend the GPU's memory by storing some information in the CPU RAM, which in general is larger.
For large sequences, this technique may apply to problems where one needs to access elements "all the way to the top" and aggregate them in a non trivial (max,min,sum,avg) way.

Problem parameters:
- bit size (alphabet,stats,cost,backtrack)
- dimensionality (2D,3D for the matrix)
  - 3D problem example is Smith Waterman with arbitrary gap weight function.
- Summarizability of the set: for computing the stats of an element, we can limit ourselves to computing values up to a certain value
  -in SW with gap extending penalty, we can summarize easily.
  -in Nussinov, we _need_ to access all the previous row-column elements for the pairwise sum (without a window limit).
Plan:
1. Define the main problem in focus and its class
   - Dimension
   - Serial/non-serial
   - Monadic/polyadic
2. Define some other problems that belong to this class or simpler classes
3. See the feasibility for FPGA (possibly helped by CPU)
4. Implement this algorithm in both CUDA and FPGA
   -> specify all parameters so that we can compare both implementations
5. Find a common IR for the problems
6. Write a code generator for both architectures from IR
   -> make sure all the test-DP are compiled well from IR
7. Find an expressive and compact language for the user-facing representation

----------------------------------

Ideas for the language:
- we want maximum flexibility
- backtrack can be custom, we may want just to give back indices
- two problems: we care about cost and we care about whole backtrack.
	@DP def Fib(n:Int) = if (n<=2) return 1 else Fib(n-1)+Fib(n-2)
  should be translated into the more complete syntax that allows more flexibility
  => get inspiration from papers to write an internal language

- we may want to keep track of indices but we do not want to give access to the backtrack matrix / cost matrix

Language side:
- define a simple way to represent some DP programs
- define a comprehensive way to define them => micro-syntax
- define the mapping into IR (actually the IR might BE the micro-syntax)
- define optimizations to IR
  - making serializable by using aggregation functions/transformations
  - avoiding the cost matrix by moving it into the wavefront
- scheduling (with memory loads for non-serializable)
- code generation for both platforms
  - possibly choice to use which platform for what part (load/compute @ cpu/gpu/fpga)

----------------------------------

Core function F:
- in: s,t strings
- in: neighbor costs: top, left, top+left
- in: neighbor stats: top, left, top+left
- out: backtrack information
- out: cost(i,j)
- out: stats

To be most efficient, we need to get exact bit-sizes of
- strings' char
- costs: min/max values
- stats: min/max values
- backtrack

=> we might want to parallelize processing within a thread that operate on 32 or 64 bit inputs (to reduce memory accesses)



Some ideas:
- define a way to pack the characters => less memory transfer (i.e. GATC=>4 letters in 1 char)
- operate on some larger word (ex 64 bits) to increase thread locality and reduce memory accesses

- write a kernel that takes stats from left and r

		stats (x)
		 ||
		 vv
stats -> KK -> new_stats (y')
(y)		 || \
		 vv  --+ backtrack info (Bxy)
		 new_stats (x')

and compute the backtracking for its cell (optional as template boolean)

- multi-grain
  + within the 64b-words : multiple letters at once (thread level)
  + group threads into blocks that operate on (block level)
  + kernel that operates one after another (GPU/CPU level)
    => can we catch a stream's event at CPU level to alloc memory to get back results ?
- keep track of
  + stats O(m+n)
  + backtrack O(m*n)

- are we sure we need squares ?
  - rectangles can increase the full usage of all threads during (x-y) runs
  - assuming we put longest word vertically we play hot potato for stats
    left->right => block.x exchanges within block, last thread writes to
    global exchange memory (if short, so that it does not penalize running time)
  - we may go up to running in stripes

- if we put in diagonal-major data in the memory, why not stream all chunks like
  that using a cyclic buffer (?) => no empty slot
  ==> this might be useful for the backtrack information


- use extensively profiling : CUDA profiler

- Robust DP paper provides 2 insights: coalesced access + GPU synchronization
  => can we do a producer-consumer scenario at the block border so that we can
     execute a large diagonal horizontal/vertical swipe made of multiple blocks?
  => multiple swipes at different delays (so that we avoid syncing at every step

          s-->
  	  +----------------/------------+
	t | B1            /             |
	| +------/-------/              | KERNEL1 (keep benefit of t in shared mem)
	| | B2  /                       | delay between b1 and b2 can be 1% of line length
	v +----/------------------------+
	  |                             |
	  |                             | KERNEL2
	Memory organized as
	b1 -  -
	b1 b1 b1
	b1 b1 b1
	.. .. ..
	b1 b1 b1
	b2 b1 b1
	b2 b2 b1
	b2 b2 b2
	.. .. ..

For stats (aggregation of the table): we need to maintain both an horizontal and vertical front
For backtracking we need to maintain the whole table
     .. | . |
	..A | B | min/max: Min(D)=Min(Min(C),Min(B))
	----+---+ sum    : Sum(D)=Sum(C)+Sum(B)-Sum(A)
	..C | D | avg    : sum both #cells and values then divide at appropriate cell
	----+---+

we many not need to store what's the previous cell, however, the previous
cell information (usually limited range) is much more compact than the score(32-64 bits)

Memory usage of CPU:

we can potentially extend the GPU's memory by storing some information in the CPU RAM, which in general is larger.
For large sequences, this technique may apply to problems where one needs to access elements "all the way to the top" and aggregate them in a non trivial (max,min,sum,avg) way.

Problem parameters:
- bit size (alphabet,stats,cost,backtrack)
- dimensionality (2D,3D for the matrix)
  - 3D problem example is Smith Waterman with arbitrary gap weight function.
- Summarizability of the set: for computing the stats of an element, we can limit ourselves to computing values up to a certain value
  -in SW with gap extending penalty, we can summarize easily.
  -in Nussinov, we _need_ to access all the previous row-column elements for the pairwise sum (without a window limit).
\subsection*{Abstract}
Dynamic programming is an algorithmic technique to solve problems that follow the Bellman's principle\cite{bellman_principle}: optimal solutions depends on optimal sub-problem solutions. The core idea behind dynamic programming is to memoize intermediate results into matrices to avoid multiple computations. Solving a dynamic programming problem consists of two phases: filling one or more matrices with intermediate solutions for sub-problems and recomposing how the final result was constructed (backtracking). In the textbooks, problems are usually described in terms of recurrence relations between matrices elements. Expressing dynamic programming problems in terms of recursive formulae involving matrix indices might be difficult, if often error prone, and the notation does not capture the essence of the underlying problem (for example aligning two sequences). Moreover, writing correct and efficient parallel implementation require different competencies and potentially a significant amount of time.

In this project, we present \textit{DynaProg}, a language embedded in Scala (DSL) to address dynamic programming problems on heterogeneous platforms. DynaProg allows the programmer to write concise programs based on ADP \cite{adp}, using a pair of parsing grammar and algebra; these program can then be executed either on CPU or on GPU. We evaluate the performance of our implementation against existing work and own baseline implementations for both the CPU and GPU versions. Experimental results show an average speedup of {\color{red} XXX} for large problems when they are run on the GPU (compared to CPU). Compared to ad-hoc GPU implementations, the generated parsers have an average slowdown of {\color{red} XXX}.
The CPU implementation has a slowdown of {\color{red} XXX} for {\color{red} PROBLEM} against {\color{red} REF,GAPC?}.
The GPU implementation has a slowdown of {\color{red} XXX} for {\color{red} PROBLEM} against {\color{red} REF,CudAlign, RNAFold}.

% Paper introduction:
% Problem to solve, what exists (related work) and how do we compare to other
% Contributions (3): 3 tensed sentences
% Benchmarks => evaluation metrics, prove introduction statements by evaluation

\vfill
This project has been achieved in collaboration with Manohar Jonnalagedda. I also would like to thank the LAMP team, including Eugene Burmako, Sandro Stucki, Vojin Jovanovic and Tiark Rompf who provided insightful advice and suggestions. I hope you will enjoy reading this report. \vspace{.3cm}\\
\textit{Thierry Coppey}

% ------------------------------------------------------------------------------------------------
\newpage
\setcounter{tocdepth}{2} \tableofcontents
\newpage
\section{Introduction} \label{intro}
Dynamic programming (DP) is a technique to solve combinatorial optimization problems that verify the Bellman's principle\cite{bellman_principle}: optimal solutions depends on optimal solutions of sub-problems. This property allows to find solutions in an exponential search space in polynomial running time by memoizing intermediate optimal solutions and reusing them multiple times. Dynamic programming problems are usually described in the textbooks by matrices recurrence relations involving indices and optimality is defined in terms of an objective function (maximum/minimum cost, ...). The computation is generally split in two phases: first intermediate scores are memoized in a matrix (tabulation) and reused to construct scores for larger problems; then a backtrack stage retrieves the solution associated with the optimal score for the problem. This solution describes how to obtain the optimal score and is called trace or backtrack trace, and heavily depends on the matrix design.

Dynamic programming problems arise in several disciplines of applied Computer Science such as biosequence analysis, natural language processing and operational research: sequence alignment, RNA sequence folding or expression parenthesisation are examples of such problems. Unfortunately, these often appear in multiple variations and with a considerable degree of sophistication such that there is a mismatch between the textbook solution and its concrete implementation. The user is often interested in one optimal solution, but he might also request \textit{all} co-optimal solutions, a fixed number of near-optimal solutions, or some synthetic properties of the search space (size, sum of scores, ...). The backtracking is usually ad-hoc because it needs both to be kept consistent with matrix filling and present the information in a format suitable for the user (human readable or ready to drive further computations). Additionally, debugging matrix indices is tedious and requires a lot of time, and little changes in the formulae might imply large rewrites of the matrices and recurrences \cite{gapc_yield}. Finally, once the implementation is correct, it is possible to turn it into an efficient implementation for specific architectures such as multi-CPU, GPU or programmable hardware (FPGA). However, a domain specialist who writes the recurrences might not be very familiar with these platforms, whereas parallelization and hardware experts might not deeply understand the domain of the dynamic programming recurrences.

To overcome these difficulties, Algebraic Dynamic Programming (ADP) \cite{adp} proposes a language-independent declarative approach that separate the concerns of dynamic programming on sequences into four distinct components that are tightly connected:\ol
\item The search space is described by a context-free \textbf{parsing grammar} that produces intermediate solution candidates whose score might be inserted in the matrix.
\item Constructed candidates are then evaluated by a \textbf{scoring function} (where all these functions form an \textbf{algebra}), so that they can be compared appropriately.
\item The \textbf{objective function} (or aggregation function) operates on the scores previously obtained to retain valid candidates.
\item Finally, results are \textbf{tabulated} (memoized in an array) in corresponding matrices. Tabulation process regulates the trade-off between running time and space efficiency by memoizing appropriate results that are reused multiple times.
\ole

A \textbf{signature} serves as interface between the grammar, the scoring algebra and the aggregation function, making possible that the same grammar share different algebrae or vice versa. Because recurrence relations are expressed by a parsing grammar, ADP makes the candidate structure explicit and hides tabulation indices, thereby preventing potential errors. Finally, since the expression of the dynamic program is formalized and abstracted into a grammar and an algebra, it becomes possible to systematically convert dynamic programming descriptions into efficient recurrences for many-core platforms such as GPUs \cite{adp_gpu}.

% -----------------------------
DynaProg, the DSL we present in this report, implements the concepts of ADP in Scala as an embedded DSL (domain-specific language) with a syntax similar to the combinators parsers of Scala library\footnote{See \url{http://www.scala-lang.org/api/current/index.html\#scala.util.parsing.combinator.Parsers}}. It extends ADP by allowing grammars for pairing two sequences (multi-track grammars) similarly as GAPC\cite{gapc_thesis}, simplifies the process of writing programs by inferring additional information (\S\ref{yield_analysis}) and can translate them into efficient CUDA\footnote{Compute Unified Device Architecture: a parallel computing platform and programming model created by NVIDIA, supported by their graphics processing units (GPUs).} program that are competitive to their handwritten counterpart (\S\ref{benchmarks}). Since the program structure is formalized in ADP framework, it can be analyzed to remove unused grammar rules (\S\ref{dead_rules}) and avoid some non-termination issues; since it is generated, correct scheduling is guaranteed and indices errors are avoided, thereby producing an arguably more reliable program.

DynaProg provides a generic way of backtracking the results such that the same trace can be used with different algebras sharing the same grammar. This allows to construct a two step pattern for solving problems: first the DP problem is solved using the appropriate cost functions; then from the backtrack of its optimal, the desired result is computed. As example, consider multiplying a chain\footnote{Assuming matrices are of appropriated dimension to be multiplied with each other} of matrices efficiently: first, optimal execution scheduling (or parenthesization) trace is found using dynamic programming and cost algebra (\S\ref{mat_mult_plain}). The backtrack trace is then used (with a multiplication algebra) to multiply the actual matrices.

Finally, offloading dynamic programming computations to CUDA devices has been made effortless for the programmer: it suffices to enable code generation to schedule dynamic compilation and execution of the GPU-optimized program, as if it was executed in plain Scala.

This project is currently available online\footnote{\url{https://github.com/manojo/lamp-dp-mt}}; it implements dynamic programming parsers in Scala (CPU) and CUDA (GPU). Its contribution is an novel approach to systematically encode and process backtracking information such that the reconstruction complexity is reduced compared to \cite{gapc_thesis}, and backtrack trace can be exchanged among different algebras sharing the same grammar.

The rest of the document consists of:\ul
\item A brief background on dynamic programming, followed by an introduction to some of the key features of the Scala programming language and LMS framework (\S\ref{background}).
\item A classification of DP problems in terms of matrix shape and dependencies, followed by a detailed analysis of some specific problems (\S\ref{problems}). Related work addressing dynamic programming challenges is presented in (\S\ref{related_work}).
\item A description of the parser stack (\S\ref{architecture}), going from the user facing language (\S\ref{user_lang}, \S\ref{adp_grammar}) to optimizations (\S\ref{recurrences}, \S\ref{backtracking}) and implementation constraints (\S\ref{normalization}, \S\ref{memory_constr}), describing all the architectural decisions we made.
\item The concrete implementation of these ideas (\S\ref{implementation}) in the form of a DSL in Scala (\S\ref{scala_parsers}) and in efficient CUDA code generation (\S\ref{codegen}).
\item A brief usage explanation detailing the available features for the DSL user (\S\ref{usage}).
\item An evaluation of the performance of our work by providing appropriate benchmarks against existing implementations (\S\ref{benchmarks}).
\ule

%\item Propose a systematic approach to encode backtracking information such that the backtracking process can be made linear to the size of the problem
%\item Provide an concrete implementation in the form of a language embedded DSL in Scala, leveraging the grammar and algebra concepts of ADP
%\item Describe two implementations: Scala for CPU (focusing on multiple backtracking) and an CUDA for GPU (focusing on efficiency)

%The contributions of this project are: \ul
%\item A classification of dynamic programming problems characteristics in terms of matrix shape and recurrence formulae dependencies.
%\item A systematic approach to convert a top-down recurrence description (grammar) into efficient bottom-up
%\item A systematic approach to process backtracking information (focus on running time and memory efficiency)
%\item A language embedded in Scala (DSL) to express DP problems concisely (based on ADP)
%\item Two implementations: Scala for CPU (features) and an CUDA for GPU (efficiency)
%%\item Reuse of existing compiler technology (fusion) for a specific purpose
%%\item State of the art parallel implementation of these classes on GPUs
%%\item Normalization of the grammar into efficient productions
%%\item Code generator to transform a grammar into efficient code for CPU, GPU (and FPGA)
%\ule

% ------------------------------------------------------------------------------------------------
\newpage
% Is Fibonacci a good example wrt what we describe in the rest of the report? After all we only deal with problems on sequences ?
% - A typical characteristic => As one can see, a Fibonacci number is described __recursively__ in terms of "smaller" Fibonacci numbers. The naive algorithm for computing F(n) would require recomputing subsolutions many times (here F(3) helps ... ). Reusing ... F(n) - 1 additions!
% - any links to tree-related algorithms?
% - "input are usually few thousands" => input is usually few thousands of K long ?
% - Scala
%  - However, we would list some ... => ; this is of course not the only reason; Scala offers some interesting features which make it a nice language to develop a DSL in ?
%  - large adoption in the industry is not really a feature is it?
%  - Implicit functions definition => implicit functions and parameters offer a way to neatly integrate primitive/pre-existing constructs into a DSL, by converting their types in order to correspond to the DSL semantics, while at the same time guarantee-ing type safety.
%  - Macros[6] and LMS are mechanisms to modify semantics of a program; in particular, these tools can be used to implement domain-specific optimizations of programs. LMS also contains a code generation platform which we use to generate some functions in C (link to section).
%  - Mixin composition (traits) allow for multiple inheritance and composition, which makes it easy to represent, in our DSL, multiple algebrae, and also interface with different runtimes (link to section).

\section{Background} \label{background}
\subsection{Dynamic programming} \label{bg_dp}
A classic example of dynamic programming is the Fibonacci series that is defined by the following recurrence:
\[F(n+1) = F(n)+F(n-1) \qquad \text{ with } F(0)=F(1)=1 \]
which expands to (first 21 numbers)
\[1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, ...\]

A typical characteristic is that an intermediate solution is reused multiple times to construct larger solutions (here $F(3)$ helps constructing $F(4)$ and $F(5)$). Reusing an existing solution avoids redoing expensive computations: with memoization (memorizing intermediate results), the solution for $F(n)$ would be obtained after $n$ additions whereas without memoization it requires $F(n)-1$ additions !

Formally, dynamic programming problems respect the Bellman's principle of optimality\cite{bellman_principle}: \textit{<<An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision>>}\footnote{\url{http://en.wikipedia.org/wiki/Bellman\_equation\#Bellman.27s\_Principle\_of\_Optimality}}. This means that every intermediate result needs to be computed only once, although it might be reused as a basis for multiple larger problems, hence our first observation.

There exist various categories of dynamic programming:\ul
\item Series that operate usually on a single dimension (like Fibonacci)
\item Sequences alignment (matching two sequences at best), top-down grammar analysis (parenthesizing), sequence folding, ... (see \S\ref{problems} for more examples and detailed classification)
\item Tree-related algorithms: phylogenetic, trees raking, maximum tree independent set, ... (can be viewed as a sparse version of the second category)
\ule

Since the first category operates on a single dimension, to benefit of the smaller solutions to compute larger ones, elements must be computed sequentially (one at a time), hence computations cannot be made parallel (unless duplicated, thereby hindering benefits of memoization). The third category suffers from limited parallelism \cite{philogeny} and its implementation does not share much with the previous category, hence we focus on the second type of problems.

Taking real-world examples in biology, the average input size for sequence alignment (\S\ref{swat_affine}) is around 300K whereas for problems like RNA folding (\S\ref{zuker}), input are usually around few thousands. Problems operating on multiple input sequences also require more memory: for instance matching 3 sequences is $O(n^3)$-space complex (as intermediate results needs to be stored in a position representing the progress in each of the involved sequence). Since we target a single computer with one or more attached devices (GPUs, FPGAs), and since we plan to maintain data in memory (due to the multiple reuse of intermediate solutions) the storage complexity must be relatively limited, compared to other problem that could leverage the disk storage. Hence in general, we focus on problems that have $O(n^2)$-space complexity whereas time complexity is usually $O(n^3)$ or larger. We encourage you to refer to \S\ref{problems} for further classification and examples.

% ------------------------------------------------------------------------------------------------
\newpage
\subsection{Scala} \label{bg_scala}
\textit{<<Scala is a general purpose programming language designed to express common programming patterns in a concise, elegant, and type-safe way. It smoothly integrates features of object-oriented and functional languages, enabling programmers to be more productive. Many companies depending on Java for business critical applications are turning to Scala to boost their development productivity, applications scalability and overall reliability.>>}\footnote{\url{http://www.scala-lang.org/node/25}}

As the Scala \cite{scala} programming language is developed by our laboratory (LAMP, EPFL), it seems natural to use it as host language for our project, however, we would list some of its features \cite{scala_api} that makes it an interesting development language for this project:\ul
\item The functional programming style and syntactic sugar offered by Scala allow concise writing of implementation, analysis and transformations of our DSL, allowing us to focus on \textit{what} we want to achieve instead of \textit{how}.
\item Scala is largely adopted in the industry\footnote{\url{http://www.scala-lang.org/node/1658}}, which makes both the adoption of related project easier and reduces the learning time of its potential users.
\item Since Scala programs execute in the Java Virtual Machine (JVM), they can benefit of the native interface (JNI) that offers the possibility to dynamically load libraries (usually written in C) and possibly interact with CUDA to leverage the GPU.
\item Scala is equipped with a strong typing and type inference system that reduces the syntactical constraints while putting strong guarantees on type correctness at compilation.
\item Implicit functions definition can help interfacing functions by automatically changing the type of passed arguments
\item Manifests (or TypeTags and ClassTags) allow type extraction at runtime (we use this to convert a Scala type into a C/CUDA type)
\item Macros\cite{scala_macros} and LMS (\S\ref{bg_lms}) could be used to modify the semantics of a specific part of the user program (we currently use LMS to produce the corresponding C function).
\item One Scala concept that we heavily use is \textit{traits} that can be viewed as abstract classes and mixed together, thereby allowing multiple inheritance.
\ule

% ------------------------------------------------------------------------------------------------
\subsection{Lightweight Modular Staging} \label{bg_lms}
Lightweight Modular Staging (LMS) \cite{lms}, \cite{lms_thesis} is a runtime code generation built on top of Scala virtualized \cite{scala_virtualized} that uses types to distinguish between binding time (compilation and runtime) for code compilation. This makes possible to annotate parts of the code with special types, such that their compilation is delayed until the program is executed. At run time, these parts are represented as a \textit{sea of nodes} that serve as the basis for another compilation phase where all the code executed until this point provides additional information to produce a more efficient compilation. The process of delaying the compilation is known as \textit{lifting} whereas \textit{lowering} corresponds to transforming this intermediate representation into executable code. LMS code generation is not limited to Scala, it can also target other languages like C. In short, LMS is an optimizing compiler framework that allows integration of domain-specific abstractions and optimizations into the generation process.

A discussion of the integration of LMS in our project can be found in \S\ref{lms_use}.

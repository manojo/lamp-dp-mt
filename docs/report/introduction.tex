

% abstract = resume the paper
% introduction
% 


% Paper introduction:
% Problem to solve, what exists (related work) and how do we compare to other
% Contributions (3): 3 tensed sentences
% Benchmarks => evaluation metrics, prove introduction statements by evaluation
\subsection*{Abstract}
Dynamic programming is a common pattern of Computer Science; yet underlying matrix recurrences might be difficult to express, error prone and efficient parallel implementation might be difficult. In this project, we present \textit{DynaProg}, a Scala DSL for dynamic programming on heterogeneous platforms which allow to write concise programs based on a pair of parsing grammar and algebra and execute them either on CPU or on GPU.

We then evaluate the performance of both CPU and GPU implementations against existing work and ad-hoc implementations of our own. Experimental results show a average speedup of {\color{red} XXX} if run on GPU (compared to CPU), {\color{red} is comparable to GAPC?} on CPU and has a slowdown of {\color{red} XXX} versus ad-hoc problem implementations on GPU {\color{red} (ours, cudalign, rnafold?)}.

\vfill
This project has been achieved in collaboration with Manohar Jonnalagedda. I also would like to thank the LAMP team, including Eugene Burmako, Sandro Stucki, Vojin Jovanovic and Tiark Rompf who provided insightful advices and suggestions. I hope you will enjoy your reading. \vspace{.3cm}\\
\textit{Thierry Coppey}

% ------------------------------------------------------------------------------------------------
\newpage
\setcounter{tocdepth}{2} \tableofcontents
\newpage
\section{Introduction}
\textbf{Difficulties with dynamic programming:} Dynamic programming (DP) is a technique used to solve combinatorial optimization problems. Often, it allows to evaluate a search space of exponential size in polynomial time. Variations of the basic algorithm not only return an optimal solution, but may also report co- or near-optimal solutions, or compute synthetic properties of the search space such as its size or the sum of all scores.

In several areas of applied Computer Science, such as operational research, natural language processing, or biosequence analysis, dynamic programming problems arise in many variations and with a considerable degree of sophistication. Hence there is a mismatch between the simple textbook algorithms and the real-world implementation of their variants. Writing correct recurrences is difficult and error prone because it needs a lot of attention to get correct indices, and implementation debugging is tedious. Additionally, little changes in the computational process might imply large rewrites of the matrices and recurrences.

For efficiency reasons, the computation is split in two: a matrix elements computation followed by a backtracking stage to retrieve the solution associated with the optimal score. The backtrack information gathering depends on the matrix design and the final result needs also to be presented in a form that makes sense for the user. This might add an additional source of errors and tedious change, would the initial recurrence be changed.

Finally, once the implementation is correct, it is possible to turn it into efficient code for many core architectures such as multi-CPU, GPU or implementation on programmable hardware (FPGA). However, the domain specialist who write the recurrences might not be very familiar with these platforms, whereas parallelization experts might not deeply understand the domain of the dynamic programming recurrences.

\textbf{Separating concerns:} Algebraic Dynamic Programming (ADP) \cite{adp} is a language-independent declarative approach that separate the concerns of dynamic programming algorithms over data sequences:\ul
\item Search space definition (using a tree parsing grammar)
\item Candidates scoring and optimization objective (with mapping and aggregation functions)
\item Tabulation of the result in matrices (determines running time and space efficiency)
\ule

By the use of a parsing grammar, ADP makes the invisible candidate structure explicit. An additional signature serves as interface between the grammar and the scoring algebra and aggregation function, which makes possible that different grammars share different algebra or vice versa. Tabulation issues are hidden from the programmer, thus there are no subscripts any more hereby removing this source of errors.

Finally, since the expression of the dynamic program is formalized, it makes it possible to convert it to efficient recurrences to target many-core platforms such as GPUs.

\textbf{DynaProg framework:} DynaProg implements ADP in Scala as an embedded DSL (domain-specific language) with a syntax matching that of the existing parsers. It extends over the original ADP parsers by providing multi-track grammars, simplifies the combinators by providing yield analysis, and can translates programs into CUDA\footnote{Compute Unified Device Architecture: a parallel computing platform and programming model created by NVIDIA, supported by their graphics processing units (GPUs).} parsers, that are competitive to handwritten code, and arguably more reliable. It performs additional optimization for correct parallel scheduling and removal of unused tabulations (dead code elimination).

DynaProg provides a generic way of backtracking the parsers, such that the same backtrack can be used with multiple algebras, provided that they use the same grammar. This allows to construct a generic pattern <<solve-compute>> of solving problem: for example, assume that the problem consists of multiplying a chain of matrices efficiently. In the first step, the used would want to find the optimal execution schedule of the multiplication using dynamic programming. In the second step, the computed optimal solution will be used to execute the actual computation on actual data, hereby producing effortlessly the desired result: it suffice to provide an additional computation algebra.

Offloading dynamic programming computations to CUDA devices is achieved effortlessly, providing that the used has correctly configured few variables to match its system configuration: it suffices to add two lines of code (mixing a trait and providing two data types) to schedule the dynamic compilation and execution of the CUDA optimized program and obtain the result, as if it was executed in plain Scala.

In the rest of the document we\ul
\item Briefly describe dynamic programming background and some interesting features of the Scala language
\item Provide a classification of DP problems characteristics (matrix shape, dependency graph) and provide a detailed analysis of some related problems that we want to implement.
\item Propose a systematic approach to encode backtracking information such that the backtracking process can be made linear to the size of the problem
\item Describe how to convert parsers efficiently in parallel code from a top-down description to a bottom-up implementation (focusing on running time and memory efficiency)
\item Provide an concrete implementation in the form of a language embedded DSL in Scala, leveraging the grammar and algebra concepts of ADP
\item Describe two implementations: Scala for CPU (focusing on multiple backtracking) and an CUDA for GPU (focusing on efficiency)
\item Evaluate the performance of our work by providing appropriate benchmarks against existing implementations
\ule

%Our contributions are: \ul
%\item A classification of DP problems characteristics (matrix shape, dependency graph, ...)
%\item A systematic approach to process data (top-down/bottom-up) and backtracking information (focus on running time and memory efficiency)
%\item A language embedded in Scala (DSL) to express DP problems concisely (based on ADP)
%\item Two implementations: Scala for CPU (features) and an CUDA for GPU (efficiency)
%%\item Reuse of existing compiler technology (fusion) for a specific purpose
%%\item State of the art parallel implementation of these classes on GPUs
%%\item Normalization of the grammar into efficient productions
%%\item Code generator to transform a grammar into efficient code for CPU, GPU (and FPGA)
%\ule

% ------------------------------------------------------------------------------------------------
\newpage
%\section{Background}
\subsection{Dynamic programming}
Dynamic programming consists of solving a problem by reusing subproblems solutions. A famous example of dynamic programming is the Fibonacci series that is defined by the recurrence
\[F(n+1) = F(n)+F(n-1) \qquad \text{ with } F(0)=F(1)=1 \]
which expands to (first 21 numbers)
\[1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, ...\]

A typical characteristic is that an intermediate solution is reused multiple times to construct larger solutions (here $F(3)$ helps constructing $F(4)$ and $F(5)$). Reusing an existing solution avoid redoing expensive computations: with memoization (memorizing intermediate results), the solution of $F(n)$ would be obtained after $n$ additions whereas without memoization it requires $F(n)-1$ additions !

Formally, dynamic programming problems respect the Bellman's principle of optimality: \textit{<<An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision>>}. This means that every intermediate result is computed only once, although it might be reused as basis for multiple larger problems, hence our first observation.

There exist various categories of dynamic programming:\ul
\item Series that operates usually on a single dimension (like Fibonacci)
\item Sequences alignment (matching two sequences at best), top-down grammar analysis (parenthesizing), sequence folding, ...
\item Tree-related algorithms: phylogenetic, trees raking, maximum tree independent set, ...
\ule

Since the first category is inherently sequential (progress cannot be faster than one element at a time) and the third category is both hard to parallelize efficiently (similar to a sparse version of the second category) and does not share much with the previous category, we focus on the second type of problems, which is also the most common.

Taking real-world examples, the average input size for sequence alignment is around 300K whereas for problems like RNA folding, input are usually around few thousands. Multiple input problems also require more memory: for instance matching 3 sequences is $O(n^3)$-space complex. Since we target a single computer with one or more attached devices (GPUs, FPGAs), and since we plan to maintain data in memory (due to the multiple reuse of intermediate solutions) the storage complexity must be relatively limited, compared to other problem that could leverage the disk storage. Hence in general, we focus on problems that have $O(n^2)$-space complexity whereas time complexity is usually $O(n^3)$ or larger. We encourage you to refer to the section~\ref{problems} for further classification and examples.

% ------------------------------------------------------------------------------------------------
\newpage
\subsection{Scala}
\textit{<<Scala is a general purpose programming language designed to express common programming patterns in a concise, elegant, and type-safe way. It smoothly integrates features of object-oriented and functional languages, enabling programmers to be more productive. Many companies depending on Java for business critical applications are turning to Scala to boost their development productivity, applications scalability and overall reliability.>>}\footnote{\url{http://www.scala-lang.org}}

As the Scala \cite{scala} programming language is developed by our laboratory (LAMP, EPFL), it seems natural to use it as host language for our project, however, we would list some of its features \cite{scala_api} that makes it an interesting development language for this project:\ul
\item The functional programming style and syntactic sugar offered by Scala allows concise writing of implementation, analysis and transformations of our DSL, which would have been much more complex and tiresome in an imperative language like C.
\item Scala is largely adopted in the industry, which makes both the adoption of related project easier and offer a steeper learning curve to their potential users.
\item Finally, through the Java VM and JNI interface, Scala offers the possibility to load dynamically external libraries, to leverage best underlying hardware by mixing with CUDA kernels to obtain optimal performance.
\ule

% ------------------------------------------------------------------------------------------------
\subsection{Lightweight Modular Staging}
Lightweight Modular Staging (LMS) \cite{lms}, \cite{lms_thesis} is a runtime code generation built on top of Scala virtualized \cite{scala_virtualized} that uses types to distinguish between binding time (compilation and runtime) for code compilation. This makes possible to annotate parts of the code with special types, such that their compilation is delayed until the program is executed. At run time, these parts are represented as multiple nodes that serve as the basis for another compilation phase where all the code executed until this point can provide additional information to produce a more efficient compilation. The process of delaying the compilation is known as \textit{lifting} whereas \textit{lowering} corresponds to transforming this intermediate representation into executable code. Through extensive use of component technology, lightweight modular staging makes an optimizing compiler framework available at the library level, allowing programmers to tightly integrate domain-specific abstractions and optimizations into the generation process.

Delite \cite{lms2}, \cite{lms3}, \cite{delite} leverages the benefits of delayed compilation provided by LMS to generate from the same source code efficient implementation for heterogeneous platforms at runtime. For instance, this can be used to transform embarrassingly parallel operations on collections into their parallel counterpart that can be executed on GPU.

At first glance, LMS seems the ideal candidate to transform Scala code into its C-like equivalent. However, the concern in this project is that the GPU code sensibly differers from the original CPU code because the two implementations serve different purposes: CPU version (Scala) is more general whereas the GPU version trades some functionalities for performance and suffer additional restrictions, in particular for memory management and alignment. Ad-hoc C translation seems more appropriate than writing Scala code and convert it with LMS because LMS only understand a subset of both languages.

LMS would still be helpful for generating user-specific functions, as they are independent of the rest of the program and user wants to write his functions only once for both Scala and CUDA.

\subsection*{Abstract}
Dynamic programming is an algorithmic technique to solve problems that follow the Bellman's principle\cite{bellman_principle}: optimal solutions depends on optimal sub-problem solutions. The core idea behind dynamic programming is to memoize intermediate results into matrices to avoid multiple computations. Solving a dynamic programming problem consists of two phases: filling one or more matrices with intermediate solutions for sub-problems and recomposing how the final result was constructed (backtracking). In the textbooks, problems are usually described in terms of recurrence relations between matrices elements. Expressing dynamic programming problems in terms of recursive formulae involving matrix indices might be difficult, if often error prone, and the notation does not capture the essence of the underlying problem (for example aligning two sequences). Moreover, writing correct and efficient parallel implementation require different competencies and potentially a significant amount of time.

In this project, we present \textit{DynaProg}, a language embedded in Scala (DSL) to address dynamic programming problems on heterogeneous platforms. DynaProg allows the programmer to write concise programs based on ADP \cite{adp}, using a pair of parsing grammar and algebra; these program can then be executed either on CPU or on GPU. We evaluate the performance of our implementation against existing work and own baseline implementations for both the CPU and GPU versions. Experimental results show an average speedup of {\color{red} XXX} for large problems when they are run on the GPU (compared to CPU). Compared to ad-hoc GPU implementations, the generated parsers have an average slowdown of {\color{red} XXX}.
The CPU implementation has a slowdown of {\color{red} XXX} for {\color{red} PROBLEM} against {\color{red} REF,GAPC?}.
The GPU implementation has a slowdown of {\color{red} XXX} for {\color{red} PROBLEM} against {\color{red} REF,CudAlign, RNAFold}.

% Paper introduction:
% Problem to solve, what exists (related work) and how do we compare to other
% Contributions (3): 3 tensed sentences
% Benchmarks => evaluation metrics, prove introduction statements by evaluation

\vfill
This project has been achieved in collaboration with Manohar Jonnalagedda. I also would like to thank the LAMP team, including Eugene Burmako, Sandro Stucki, Vojin Jovanovic and Tiark Rompf who provided insightful advice and suggestions. I hope you will enjoy reading this report. \vspace{.3cm}\\
\textit{Thierry Coppey}

% ------------------------------------------------------------------------------------------------
\newpage
\setcounter{tocdepth}{2} \tableofcontents
\newpage
\section{Introduction} \label{intro}
Dynamic programming (DP) is an algorithmic technique to solve optimization problems. For example, we might want to multiply a chain of matrices\footnote{Such that matrices have appropriate dimensions to be multiplied with each other.} efficiently. The order in which matrices are combined changes the number of required scalar multiplications, we would want to find an optimal order that minimizes this number. Notice that if we knew how to split the chain into two subparts, we could recursively find an optimal order in these two parts and recombine them. Such combinatorial problems verify the Bellman's principle\cite{bellman_principle}: \textit{<<optimal solutions depends on optimal solutions of sub-problems>>}\footnote{\url{http://en.wikipedia.org/wiki/Bellman\_equation\#Bellman.27s\_Principle\_of\_Optimality}}. In order to find the optimal way of splitting the chain, we would need to explore an exponential number of possibilities. Using the Bellman's principle, we can memorize intermediate optimal solutions to save redundant computations, thereby reducing the problem to a polynomial complexity.

Dynamic programming problems are usually expressed in terms of recurrences on intermediate solutions that are stored in matrices, whereas optimality is defined in terms of an objective function (min/max cost, ...). In the case of a chain of $n$ matrices, the recurrence is:
	\[M_{(i,i)}=0 \quad\land\quad M_{(i,j)}=\min_{1\le i\le k<j\le n}\{M_{(i,k)}+M_{(k+1,j)}+r_i \cdot c_k \cdot c_j\} \qquad \forall 1\le i,j\le n\]
where $r_i$ and $c_i$ denotes respectively the row and column of the $i^{\rm th}$ matrix in the chain, $M$ is an $n \times n$ matrix and $M_{(i,j)}$ stores the number of multiplications to obtain the product of matrices in the chain $i,...,j$. The total number of required scalar multiplications is given by $M_{(1,n)}$ (refer to \S\ref{mat_mult_plain} for details). Once the optimal result is found, a second backtrack phase retrieves the construction trace associated with the optimal score for the problem. This trace (or backtrack trace) describes how to obtain the optimal score, and heavily depends on the matrix design.

Dynamic programming problems arise in several disciplines of applied Computer Science such as biosequence analysis, natural language processing and operational research: sequence alignment, RNA sequence folding or expression parenthesisation are examples of such problems. Unfortunately, these often appear in multiple variations and with a considerable degree of sophistication such that there is a mismatch between the textbook solution and its concrete implementation. The user is often interested in one optimal solution, but he might also request \textit{all} co-optimal solutions, a fixed number of near-optimal solutions, or some synthetic properties of the search space (size, sum of scores, ...).

The backtracking is usually ad-hoc because it needs both to be kept consistent with matrix filling and present the information in a format suitable for the user (human readable or ready to drive further computations). Additionally, debugging matrix indices is tedious and requires a lot of time, and little changes in the formulae might imply large rewrites of the matrices and recurrences \cite{gapc_yield}.

Finally, once the implementation is correct, it is possible to turn it into an efficient implementation for specific architectures such as multi-CPU, GPU or programmable hardware (FPGA). However, a domain specialist who writes the recurrences might not be very familiar with these platforms, whereas parallelization and hardware experts might not deeply understand the domain of the dynamic programming recurrences.

\newpage
To simplify the expression of dynamic problems, Algebraic Dynamic Programming (ADP) \cite{adp} proposes a language-independent declarative approach that separate the concerns of dynamic programming on sequences into four distinct components that are tightly connected:\ol
\item The search space is described by a context-free \textbf{parsing grammar} that produces intermediate solution candidates whose score might be inserted in the matrix.
\item Constructed candidates are then evaluated by a \textbf{scoring function} (where all these functions form an \textbf{algebra}), so that they can be compared appropriately.
\item The \textbf{objective function} (or aggregation function) operates on the scores previously obtained to retain valid candidates.
\item Finally, results are \textbf{tabulated} (memoized in an array) in corresponding matrices. Tabulation process regulates the trade-off between running time and space efficiency by memoizing appropriate results that are reused multiple times.
\ole

A \textbf{signature} serves as interface between the grammar, the scoring algebra and the aggregation function, making possible that the same grammar share different algebrae or vice versa. Because recurrence relations are expressed by a parsing grammar, ADP makes the candidate structure explicit and hides tabulation indices, thereby preventing potential errors. Finally, since the expression of the dynamic program is formalized and abstracted into a grammar and an algebra, it becomes possible to systematically convert dynamic programming descriptions into efficient recurrences for many-core platforms such as GPUs \cite{adp_gpu}.

% -----------------------------
DynaProg, the DSL we present in this report, implements the concepts of ADP in Scala as an embedded DSL (domain-specific language) with a syntax similar to the combinators parsers of Scala library\footnote{See \url{http://www.scala-lang.org/api/current/index.html\#scala.util.parsing.combinator.Parsers}}. It extends ADP by allowing grammars for pairing two sequences (multi-track grammars) similarly as GAPC\cite{gapc_thesis}, simplifies the process of writing programs by inferring additional information (\S\ref{yield_analysis}) and can translate them into efficient CUDA\footnote{Compute Unified Device Architecture: a parallel computing platform and programming model created by NVIDIA, supported by their graphics processing units (GPUs).} program that are competitive to their handwritten counterpart (\S\ref{benchmarks}). Since the program structure is formalized in ADP framework, it can be analyzed to remove unused grammar rules (\S\ref{dead_rules}) and avoid some non-termination issues; since it is generated, correct scheduling is guaranteed and indices errors are avoided, thereby producing an arguably more reliable program.

DynaProg provides a generic way of backtracking the results such that the same trace can be used with different algebras sharing the same grammar. This allows to construct a two step pattern for solving problems: first the DP problem is solved using the appropriate cost functions; then from the backtrack of its optimal, the desired result is computed. As example, consider multiplying a chain\footnote{Assuming matrices are of appropriated dimension to be multiplied with each other} of matrices efficiently: first, optimal execution scheduling (or parenthesisation) trace is found using dynamic programming and cost algebra (\S\ref{mat_mult_plain}). The backtrack trace is then used (with a multiplication algebra) to multiply the actual matrices.

Finally, offloading dynamic programming computations to CUDA devices has been made effortless for the programmer: it suffices to enable code generation to schedule dynamic compilation and execution of the GPU-optimized program, as if it was executed in plain Scala.

\newpage
This project is currently available online\footnote{\url{https://github.com/manojo/lamp-dp-mt}}; it implements dynamic programming parsers in Scala (CPU) and CUDA (GPU). Its contribution is an novel approach to systematically encode and process backtracking information such that the reconstruction complexity is reduced compared to \cite{gapc_thesis}, and backtrack trace can be exchanged among different algebras sharing the same grammar.

The rest of the document consists of:\ul
\item A brief background on dynamic programming, followed by an introduction to some of the key features of the Scala programming language and LMS framework (\S\ref{background}).
\item A classification of DP problems in terms of matrix shape and dependencies, followed by a detailed analysis of some specific problems (\S\ref{problems}). Related work addressing dynamic programming challenges is presented in (\S\ref{related_work}).
\item A description of the parser stack (\S\ref{architecture}), going from the user facing language (\S\ref{user_lang}, \S\ref{adp_grammar}) to optimizations (\S\ref{recurrences}, \S\ref{backtracking}) and implementation constraints (\S\ref{normalization}, \S\ref{memory_constr}), describing all the architectural decisions we made.
\item The concrete implementation of these ideas (\S\ref{implementation}) in the form of a DSL in Scala (\S\ref{scala_parsers}) and in efficient CUDA code generation (\S\ref{codegen}).
\item A brief usage explanation detailing the available features for the DSL user (\S\ref{usage}).
\item An evaluation of the performance of our work by providing appropriate benchmarks against existing implementations (\S\ref{benchmarks}).
\ule

%\item Propose a systematic approach to encode backtracking information such that the backtracking process can be made linear to the size of the problem
%\item Provide an concrete implementation in the form of a language embedded DSL in Scala, leveraging the grammar and algebra concepts of ADP
%\item Describe two implementations: Scala for CPU (focusing on multiple backtracking) and an CUDA for GPU (focusing on efficiency)

%The contributions of this project are: \ul
%\item A classification of dynamic programming problems characteristics in terms of matrix shape and recurrence formulae dependencies.
%\item A systematic approach to convert a top-down recurrence description (grammar) into efficient bottom-up
%\item A systematic approach to process backtracking information (focus on running time and memory efficiency)
%\item A language embedded in Scala (DSL) to express DP problems concisely (based on ADP)
%\item Two implementations: Scala for CPU (features) and an CUDA for GPU (efficiency)
%%\item Reuse of existing compiler technology (fusion) for a specific purpose
%%\item State of the art parallel implementation of these classes on GPUs
%%\item Normalization of the grammar into efficient productions
%%\item Code generator to transform a grammar into efficient code for CPU, GPU (and FPGA)
%\ule

% XXX: grammar for all the examples of problems

% ------------------------------------------------------------------------------------------------
\section{Background} \label{background}
\subsection{Graphic cards}
Modern graphic cards\footnote{We cover here interesting features of the CUDA devices and programming paradigm; however, the same concept should be applicable to graphic cards from other vendors.} are powered by massively parallel processors: they can typically run hundreds or thousands of cores, each able to schedule multiple threads. The threads are usually grouped in warps that are scheduled synchronously. This means that if there is a divergence in the execution path, both alternatives are executed sequentially, thereby stalling other warp's threads. Threads are logically grouped in blocks by the programmer whereas warps correspond to a physical constraint. In a deliberate design decision to simplify the hardware, there exist no global synchronization.

On graphic cards, there exist two levels of memory that are visible for the programmer: the global memory, which can be accessed by any thread, and the shared memory, that corresponds to an explicitly addressable cache memory, whose access is faster but restricted to threads in the same block. A small amount of (global) memory can be marked as constant, so that its caching and reading strategy can be adapted consequently \cite{cuda_constant}. Finally, access to the main memory of the computer is possible on recent cards but suffers an additional penalty, which makes it not desirable.

Since in such architecture the major bottleneck is often the access to the global memory, threads should access contiguous memory at the same time. This is called coalesced memory access and improving the memory layout in this direction can lead to significant speedup\footnote{\url{http://mc.stanford.edu/cgi-bin/images/5/5f/Darve_cme343_cuda_2.pdf}}.

\newpage
\subsection{ADP and parsing grammars} \label{adp_grammar}
\subsubsection{ADP formal specifications}
\textit{This subsection is an excerpt of "Algebraic Dynamic Programming" \cite{adp}, section 3. Would the reader be interested in more details, we encourage him to read the corresponding paper.} \\[6pt]
\textbf{Terminology}\\
An \textbf{alphabet} $\mathcal{A}$ is a finite set of symbols. Sequence of symbols are called strings. $\epsilon$ denotes the empty string, $\mathcal{A}^1=\mathcal{A}$, $\mathcal{A}^{n+1}=\{ax|a\in\mathcal{A},x\in\mathcal{A}^n\}$, $\mathcal{A}^+=\bigcup_{n\ge 1}\mathcal{A}^n,\mathcal{A}^*=\mathcal{A}^+\cup\{\epsilon\}$.

A \textbf{signature} $\Sigma$ over some alphabet $\mathcal{A}$ consists of a sort symbol $S$ with a family of operators. Each operator $\circ$ has fixed arity: $\circ:s_1,...,s_k\to S$ where each $s_i$ is either $S$ or $\mathcal{A}$.

A $\Sigma$-\textbf{algebra} $\mathcal{I}$ over $\mathcal{A}$, also called an interpretation, is a set $S_\mathcal{I}$ of values together with a function $\circ_\mathcal{I}$ for each operator $\circ$. Each $\circ_\mathcal{I}$ has type $\circ_\mathcal{I}: (s_1)_\mathcal{I} ...(s_k)_\mathcal{I} \to S_\mathcal{I}$ where $\mathcal{A}_\mathcal{I} = \mathcal{A}$.

A \textbf{term algebra} $T_\Sigma$ arises by interpreting the operators in $\Sigma$ as constructors, building bigger terms from smaller ones.
When variables from a set $V$ can take the place of arguments to constructors, we speak of a term algebra with variables $T_\Sigma(V)$ with $V\subset T_\Sigma(V)$.

Terms will be viewed as rooted, ordered, node-labeled trees in the obvious way. According to the special role of $\mathcal{A}$, only leaf nodes can carry symbols from $\mathcal{A}$. A term/tree with variables is called a tree pattern. A tree containing a designated occurrence of a subtree $t$ is denoted $C[...t...]$.
A tree language over $\Sigma$ is a subset of $T_\Sigma$. Tree languages are described by tree grammars, which can be defined in analogy to the Chomsky hierarchy of string grammars.

\textbf{Definition 1:} (Tree grammar $\mathcal{G}$ over $\Sigma$ and $\mathcal{A}$) \\
A (regular) tree grammar $\mathcal{G}$ over $\Sigma$ and $\mathcal{A}$ is given by\ul
\item A set $V$ of nonterminal symbols
\item A designated nonterminal symbol $Ax$ called the axiom
\item A set $P$ of productions of the form $v\to t$ where $v\in V$ and $t\in T_\Sigma(V)$
\ule
The derivation relation for tree grammars is $\to^*$, with $C[...v...] \to C[...t...]$ if $v\to t\in P$. The language of $v\in V$ is $\mathcal{L}(v)=\{t\in T_\Sigma | v\to^* t\}$. The language of $\mathcal{G}$ is $\mathcal{L}(\mathcal{G})=\mathcal{L}(Ax)$.

\textbf{Definition 2:} (Evaluation algebra)\\ Let $\Sigma$ be a signature over $\mathcal{A}$ with sort symbol $Ans$. A $\Sigma$-evaluation algebra is a $\Sigma$-algebra augmented with an objective function $h: [Ans] \to [Ans]$, where $[Ans]$ denotes lists over $Ans$.

\textbf{Definition 3:} (Yield grammars and yield languages)\\
Let $\mathcal{G}$ be a tree grammar over $\Sigma$ and $\mathcal{A}$, and $y$ the yield function. The pair $(\mathcal{G}, y)$ is called a yield grammar. It defines the yield language $\mathcal{L}(\mathcal{G}, y) = y(\mathcal{L}(\mathcal{G}))$.

\textbf{Definition 4:} (Yield parsing)\\
Given a yield grammar $(\mathcal{G}, y)$ over $\mathcal{A}$ and $w\in\mathcal{A}^*$, the yield parsing problem is to find $P_\mathcal{G}(w) := \{t\in \mathcal{L}(\mathcal{G})|y(t) = w\}$..

\textbf{Definition 5:} (Algebraic dynamic programming)\ul
\item An ADP problem is specified by a signature $\Sigma$ over $\mathcal{A}$, a yield grammar $(\mathcal{G}, y)$ over $\Sigma$, and a $\Sigma$-evaluation algebra $I$ with objective function $h_I$ .
\item An ADP problem instance is posed by a string $w\in\mathcal{A}^*$. The search space it spawns is the set of all its parses, $P_\mathcal{G}(w)$.
\item Solving an ADP problem is computing $h_I\{t_I | t\in P_\mathcal{G}(w)\}$
in polynomial time and space.
\ule

\textbf{Definition 6:} (Algebraic version of Bellman’s principle)\\
For each $k$-ary operator $f$ in $\Sigma$, and all answer lists $z_1,\ldots, z_k$, the objective function $h$ satisfies
\[\begin{array}{rl}
& h( [ f(x_1,\ldots,x_k) | x_1\leftarrow z_1,\ldots ,x_k\leftarrow z_k ] ) \\[4pt]
= & h( [ f(x_1,\ldots,x_k) | x_1\leftarrow h(z_1),\ldots,x_k\leftarrow h(z_k) ] )
\end{array}\]
Furthermore, the same property holds for the concatenation of answer lists: \[h(z_1 ::: z_2) = h(h(z_1) ::: h(z_2))\]

% -----------------------------
\subsubsection{ADP in practice} \label{adp_practice}
ADP is a formalization of parsers that introduces a distinction between the \textbf{parsing grammar} (recognition phase) and an associated \textbf{algebra} (evaluation phase). Such separation makes it possible to define multiple algebra for the same grammar. This has two main applications:\ol
\item Experiment variants with the same grammar:  for example, Needleman-Wunsch and Smith-Waterman share the same grammar but have a different evaluation algebra
\item Use an evaluation and execution algebra: a dynamic programming problem is solved in two steps: computing one optimal solution and applying it over actual data. For example in matrix chain multiplication, the first step solves the underlying dynamic program by evaluating the number of necessary multiplications, the second step \textit{effectively} multiplies matrices according to the order previously defined.
\ole

Practically, an ADP program is made of 3 components: a \textbf{signature} that define a set of function signatures, one or more \textbf{algebrae} implementing these functions and a \textbf{grammar} containing parsers that make use of the functions defined in the signature. The concrete program instance combines the algebra with the grammar. The grammar parsers' intermediate results are memoized in a matrix (tabulation parser). A parser usually consist of a tree of:\ul
\item \textbf{Terminal:} operates on a subsequence of input elements and returns either its content or position (or a failure if the sequence does not fit the terminal).
\item \textbf{Filter:} accepts only subsequences matching a certain predicate. The condition is evaluated ahead of its actual content evaluation.
\item \textbf{Or:} expresses alternative between two different parsers and returns their result union.
\item \textbf{Concatenation:} constructs all possible combinations from two subsequences. The subsequences can be of fixed or varying size and concatenation operators might impose restrictions on the subsequences length to be considered.
\item \textbf{Map:} this parser transform its input using a user-defined function. It is typically used to transform a subword into a score that can later be aggregated.
\item \textbf{Aggregation:} the aggregation applies a functions that reduces the list of results, typically minimum or maximum, but the function can be arbitrarily defined.
\item \textbf{Tabulation:} the tabulation's primary function is to store intermediate results and possibly serve as connection point between different parsers.
\ule

Additionally, the signature must define an input alphabet ({\tt Alphabet}), and an output alphabet ({\tt Answer}) can be defined either in the signature or in the algebra. Finally, the grammar needs to have a starting point, denoted as axiom. Finally, the default aggregation function $h$ must be defined\footnote{Although aggregation usage is not mandatory in the framework, we force the existence of an aggregation function over the ouput type so that we can use it to aggregate windowed results.}. To make it more clear, we propose an example of the matrix chain multiplication problem\footnote{The original ADP framework is an embedded DSL of Haskell, however, we assume that the reader is more familiar with Scala notation and immediately present the syntax of our implementation.}.

\begin{lstlisting}[language=Scala,caption={Matrix chain mulitiplication DSL implementation}]
trait MatrixSig extends Signature {
  type Alphabet = (Int,Int) // Matrix(rows, columns)
  val single:Alphabet=>Answer
  val mult:(Answer,Answer)=>Answer
}

trait MatrixAlgebra extends MatrixSig {
  type Answer = (Int,(Int,Int)) // Answer(cost, Matrix(rows, columns))
  override val h = minBy((a:Answer) => a._1)
  val single = (a: Alphabet) => (0, a)
  val mult = (a:Answer,b:Answer) =>
    { val ((m1,(r1,c1)),(m2,(r2,c2)))=(a,b); (m1+m2+r1*c1*c2, (r1,c2)) }
}

trait MatrixGrammar extends ADPParsers with MatrixSig {
  val axiom:Tabulate = tabulate("M",
    (el ^^ single | axiom ~ axiom ^^ mult) aggregate h)
}

object MatrixMult extends MatrixGrammar with MatrixAlgebra with App {
  println(parse(Array((10,100),(100,5),(5,50)))) // List((7500,(10,50)))
}
\end{lstlisting}
\begin{center}\vspace{-18pt} with or: $|\,\,$ map: $\land\land\,\,$ concatenation: $\sim$ \end{center}

%\vspace{3cm}
This program grammar can also be expressed in BNF\footnote{\url{http://en.wikipedia.org/wiki/Backus-Naur\_Form}}:
\[\begin{array}{lrl}
axiom &::=& \text{matrix} \\
 &|& axiom \,\,\, axiom
\end{array}\]

and it encodes the following recurrence (cost only):
	\[M_{(i,j)}=\left\{\begin{array}{ll}
		0 & \text{if } i+1= j\\
		\min_{i<k<j} M_{(i,k)}+M_{(k,j)}+r_i \cdot c_k \cdot c_j & \text{otherwise}
	\end{array}\right. \]

Notice that the semantics of indices differ slightly from the problem presented in \S\ref{mat_mult_plain}; this is because empty chain are made expressible (denoted $M_{(i,i)}$, single matrices are denoted $M_{(i,i+1)}$).

% ------------------------------------------------------------------------------------------------
\newpage
\subsection{Scala} \label{bg_scala}
\textit{<<Scala is a general purpose programming language designed to express common programming patterns in a concise, elegant, and type-safe way. It smoothly integrates features of object-oriented and functional languages, enabling programmers to be more productive. Many companies depending on Java for business critical applications are turning to Scala to boost their development productivity, applications scalability and overall reliability.>>}\footnote{\url{http://www.scala-lang.org/node/25}}

As the Scala \cite{scala} programming language is developed by our laboratory (LAMP, EPFL), it seems natural host language for our project. Its large adoption\footnote{\url{http://www.scala-lang.org/node/1658}}, would make the adoption of our DSL easier while reducing the learning time of its potential users. Additionally, some features \cite{scala_api} of Scala makes it an interesting development language for this project:\ul
\item The functional programming style and syntactic sugar offered by Scala allow concise writing of implementation, analysis and transformations of our DSL, allowing us to focus on \textit{what} we want to achieve instead of \textit{how}.
\item Since Scala programs execute in the Java Virtual Machine (JVM), they can benefit of the native interface (JNI) that offers the possibility to dynamically load libraries (usually written in C) and possibly interact with CUDA to leverage the GPU.
\item Scala is equipped with a strong typing and type inference system that reduces the syntactical constraints while putting strong guarantees on type correctness at compilation.
\item Implicit functions and parameters allow to simplify the syntactic usage of the DSL by implementing automatic conversions, while at the same time preserving type safety.
\item Manifests (or TypeTags and ClassTags) allow type extraction at runtime (we use this to convert a Scala type into a C/CUDA type)
\item Macros\cite{scala_macros} and LMS (\S\ref{bg_lms}) could be used to modify the semantics of specific parts, or implement domain-specific optimizations of the user program. LMS also contains a multi-language code generator that we leverage to produce C functions (see \S\ref{user_fun_gen}).
\item One Scala concept that we heavily use is \textit{traits} that can be viewed as abstract classes and combined (mixin composition), thereby allowing multiple inheritance. We use this feature in particular to smoothly combine algebra, grammar and possibly code generation (\S\ref{codegen}) into a concrete program.
\ule

% ------------------------------------------------------------------------------------------------
\subsection{Lightweight Modular Staging} \label{bg_lms}
Lightweight Modular Staging (LMS) \cite{lms}, \cite{lms_thesis} is a runtime code generation built on top of Scala virtualized \cite{scala_virtualized} that uses types to distinguish between binding time (compilation and runtime) for code compilation. This makes possible to annotate parts of the code with special types, such that their compilation is delayed until the program is executed. At run time, these parts are represented as a \textit{sea of nodes} that serve as the basis for another compilation phase where all the code executed until this point provides additional information to produce a more efficient compilation. The process of delaying the compilation is known as \textit{lifting} whereas \textit{lowering} corresponds to transforming this intermediate representation into executable code. LMS code generation is not limited to Scala, it can also target other languages like C. In short, LMS is an optimizing compiler framework that allows integration of domain-specific abstractions and optimizations into the generation process.

A discussion on the integration of LMS in our project can be found in \S\ref{lms_use}.

% ------------------------------------------------------------------------------------------------
\newpage
\subsection{Related work} \label{related_work}
Work related to dynamic programming can be separated in two categories: ad-hoc implementations and grammar-based implementations. The former focus on the performance for a specific problem whereas the latter generalize and formalize the dynamic programming problem description into a parsing grammar paired with a costing algebra.

Grammar-based dynamic programming was inseminated by ADP \cite{adp_discipline} and first implemented as a Haskell DSL \cite{adp}. To overcome performance issues, multiple solutions were devised:\ul
\item Converting Haskell parsers in their C or CUDA equivalent \cite{adp_gpu}
\item Modifying Haskell execution environment to provide loop fusion to improve ADP parsers performance \cite{adp_fusion}, \cite{adp_fusion_pkg}.
\item Ultimately, the dynamic programming algebra and grammar were formalized into a specific language \cite{gapl} provided with an ad-hoc compiler \cite{gapc_thesis}, thereby allowing more advanced analysis of the grammar \cite{gapc_yield}.
\ule

The research on ad-hoc implementation has focused on three kind of problems:\ul
\item Genral problems, attempting to provide the most efficient implementation for a particular problem \cite{gpu_atlp}, \cite{robust_mapping}, \cite{gpu_from_dsl}.
\item RNA sequence folding (variants of the Zuker folding): \cite{nussinov_gpu}, \cite{gpu_rnafold}.
\item Biological sequence alignment (Smith-Waterman) for huge sequences: \cite{swat_gpu}, \cite{swat_linear} \cite{swat_mega}.
\ule

Since this project involves various domains, we also investigated in the memory management on graphic cards and existing code generation frameworks.

In an attempt to support a varying number of results per matrix cell, we considered dynamic memory allocation \cite{nvidia} (available on recent graphic cards), ad-hoc memory allocation \cite{scatter_alloc} and hash tables \cite{parallel_hashing}. However the costs associated with dynamic memory allocation makes it unattractive for this particular kind of problem, and the use of cuckoo hash tables adds a constant factor penalty to every memory access. Finally both solution introduce undesirable possibility of failure (respectively out of memory or unrecoverable collision) in the middle of the algorithm computation process.

Automated code generation and execution flow is addressed by Delite \cite{lms2}, \cite{lms3}, \cite{delite}, that leverages LMS\cite{lms_thesis} to generate from the same source code efficient implementation for heterogeneous platforms (including CUDA) at runtime. Although this shares many patterns with our project, we can not reuse this framework because the scheduling and computation is tightly interleaved in dynamic programming (see \ref{bg_lms}) whereas Delite focuses on parallelizing operations on collections (array, lists, maps, ...) of independent elements.


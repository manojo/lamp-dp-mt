\subsection*{Abstract}
Dynamic programming is an algorithmic technique to solve problems that follow the Bellman's principle\cite{bellman_principle}: optimal solutions depends on optimal sub-problem solutions. The core idea behind dynamic programming is to memoize intermediate results into matrices to avoid multiple computations. Solving a dynamic programming problem consists of two phases: filling one or more matrices with intermediate solutions for sub-problems and recomposing how the final result was constructed (backtracking). In the textbooks, problems are usually described in terms of recurrence relations between matrices elements. However, matrices computation formulae and recurrences might be difficult to express, indexing element is often error prone and writing an efficient parallel implementation can take a significant amount of time.

In this project, we present \textit{DynaProg}, a language embedded in Scala (DSL) to address dynamic programming problems on heterogeneous platforms. DynaProg allows the programmer to write concise programs based on ADP \cite{adp}, using a pair of parsing grammar and algebra; these program can then be executed either on CPU or on GPU. We evaluate the performance of our implementation against existing work and own ad-hoc implementations for both the CPU and GPU versions. Experimental results show an average speedup of {\color{red} XXX} for large problems when they are run on the GPU (compared to CPU). Compared to ad-hoc GPU implementations, the generated parsers have an average slowdown of {\color{red} XXX}.
The CPU implementation has a slowdown of {\color{red} XXX} for {\color{red} PROBLEM} against {\color{red} REF,GAPC?}.
The GPU implementation has a slowdown of {\color{red} XXX} for {\color{red} PROBLEM} against {\color{red} REF,CudAlign, RNAFold}.

% Paper introduction:
% Problem to solve, what exists (related work) and how do we compare to other
% Contributions (3): 3 tensed sentences
% Benchmarks => evaluation metrics, prove introduction statements by evaluation
 
\vfill
This project has been achieved in collaboration with Manohar Jonnalagedda. I also would like to thank the LAMP team, including Eugene Burmako, Sandro Stucki, Vojin Jovanovic and Tiark Rompf who provided insightful advice and suggestions. I hope you will enjoy your reading. \vspace{.3cm}\\
\textit{Thierry Coppey}

% ------------------------------------------------------------------------------------------------
\newpage
\setcounter{tocdepth}{2} \tableofcontents
\newpage
\section{Introduction}
Dynamic programming (DP) is a technique used to solve combinatorial optimization problems that verify the Bellman's principle\cite{bellman_principle}: optimal solutions depends on optimal solutions of sub-problems. Dynamic programming often allows to find solutions in an exponential search space (number of solutions) in polynomial running time. The user is usually interested in one optimal solution of the problem, but he might also desire to have \textit{all} optimal solutions (co-optimal), a fixed number of near-optimal solutions, or some synthetic properties of the search space such as its size or the sum of all scores.

Dynamic programming problems arise in several disciplines of applied Computer Science such as biosequence analysis, natural language processing and operational research: sequence alignment, RNA sequence folding or expression parenthesisation are example of such problems which are usually described by matrix recurrence relations. Unfortunately, these problems often appear in multiple variations and with a considerable degree of sophistication such that there is a mismatch between the textbook solution and its concrete implementation. Additionally, debugging is tedious and requires a lot of time, and little changes in the formulae might imply large rewrites of the matrices and recurrences\cite{gapc_yield}.

Dynamic programming computation is usually split in two phases: first intermediate scores are memoized in a matrix (tabulation), and reused to construct scores for larger problems; then a backtrack stage retrieves the solution associated with the optimal score for the problem. This solution describes how to obtain the optimal score and is called trace or backtrack trace, and heavily depends on the matrix design. The obtained information then needs to be presented in a form that makes sense for the user (or  might drive further computations). These two phases need to be kept consistent with each other, thereby introducing a new potential source of errors.

Finally, once the implementation is correct, it is possible to turn it into an efficient implementation for specific architectures such as multi-CPU, GPU or programmable hardware (FPGA). However, a domain specialist who writes the recurrences might not be very familiar with these platforms, whereas parallelization and hardware experts might not deeply understand the domain of the dynamic programming recurrences.

% http://en.wikipedia.org/wiki/Regular_tree_grammar: language class between regular languages and the deterministic context-free languages.
To solve these concerns, Algebraic Dynamic Programming (ADP) \cite{adp} proposes a language-independent declarative approach that separate the concerns of dynamic programming algorithms into four distinct components that are tightly connected:\ol
\item The search space is described using a tree \textbf{parsing grammar} that describes how to construct intermediate candidates whose score might be inserted in the matrix.
\item Constructed candidates are then evaluated by a \textbf{scoring function} (where all these functions form an \textbf{algebra}), so that they can be compared appropriately.
\item The \textbf{objective function} (or aggregation function) operates on the scores previously obtained to retain valid candidates.
\item Finally, results are \textbf{tabulated} (memoized in an array) in corresponding matrices. Tabulation process regulates the trade-off between running time and space efficiency by memoizing appropriate results that are reused multiple times.
\ole

By using a parsing grammar, ADP makes the candidate structure explicit. An additional signature serves as interface between the grammar, the scoring algebra and the aggregation function which makes possible that different grammars share different algebra or vice versa. Tabulation indices issues are hidden from the programmer, thereby removing one source of errors. Finally, since the expression of the dynamic program is formalized and abstracted into a grammar and algebra, it becomes possible to convert it to efficient recurrences for many-core platforms such as GPUs. \cite{adp_gpu}

% -----------------------------
DynaProg implements the concepts of ADP in Scala as an embedded DSL (domain-specific language) with a syntax similar to the combinators parsers of Scala library\footnote{See \url{http://www.scala-lang.org/api/current/index.html\#scala.util.parsing.combinator.Parsers}}. It extends ADP by allowing grammars for pairing two sequences (multi-track grammars) similarly as GAPC\cite{gapc_thesis}, simplifies the process of writing programs by inferring additional information (\S\ref{yield_analysis}) and can translate them into efficient CUDA\footnote{Compute Unified Device Architecture: a parallel computing platform and programming model created by NVIDIA, supported by their graphics processing units (GPUs).} program that are competitive to their handwritten counterpart (\S\ref{benchmarks}). Since the program structure is formalized in ADP framework, it can be analyzed to remove unused grammar productions (dead code elimination) and avoid some non-termination issues; since it is generated, correct scheduling is guaranteed and indices errors are avoided, thereby producing an arguably more reliable program.

DynaProg provides a generic way of backtracking the results, such that the same backtrack trace can be used with multiple algebras if they share the same grammar. This allows to construct a two step pattern for solving problems: first the DP problem is solved using the appropriate cost function; then from the backtrack of its optimal, the desired result is computed. As example, consider multiplying a chain\footnote{Assuming matrices are of appropriated dimension to be multiplied with each other} of matrices efficiently. In the first step, optimal execution scheduling (or parenthesization) is found using dynamic programming (\S\ref{mat_mult_plain}). In the second step, the backtrack trace of optimal solution is used to multiply the actual matrices (given the corresponding algebra).

Finally, offloading dynamic programming computations to CUDA devices has been made effortless for the programmer: it suffices to enable code generation to schedule dynamic compilation and execution of the GPU-optimized program, as if it was executed in plain Scala.

This project resulted is an open-source\footnote{\url{https://github.com/manojo/lamp-dp-mt}} implementation of dynamic programming parsers for Scala optimized for both on CPU and GPU and featuring several analyses (\S\ref{architecture}) to ease the writing of dynamic programs. Its contribution is an automated approach to encode and process backtracking information such that the reconstruction complexity is reduced compared to \cite{gapc_thesis} and backtrack trace can be exchanged among different algebras sharing the same grammar. 

The rest of the document consists of:\ul
\item A brief background on dynamic programming (\S\ref{intro_dp}), followed by an explanation of some of the Scala programming language (\S\ref{intro_scala}) and LMS framework (\S\ref{intro_lms}) features.
\item A classification of DP problems in terms of matrix shape and dependencies in section~\ref{problems}, followed by a detailed analysis of some specific problems is provided. Related work addressing the dynamic programming challenges is presented in \ref{related_work}.
\item A description of the whole parser stack (\S\ref{architecture}), going from the user facing language (\S\ref{user_lang}, \S\ref{adp_grammar}) to optimizations (\S\ref{recurrences}, \S\ref{backtracking}) and implementation constraints (\S\ref{normalization}, \S\ref{memory_constr}), describing all the architectural decisions we made.
\item The concrete implementation of these ideas (\S\ref{implementation}) in the form of a DSL for Scala (\S\ref{scala_parsers}) and in efficient CUDA code generation (\S\ref{codegen}).
\item An evaluation of the performance of our work (\S\ref{benchmarks}) by providing appropriate benchmarks against existing implementations.
\ule

%\item Propose a systematic approach to encode backtracking information such that the backtracking process can be made linear to the size of the problem
%\item Provide an concrete implementation in the form of a language embedded DSL in Scala, leveraging the grammar and algebra concepts of ADP
%\item Describe two implementations: Scala for CPU (focusing on multiple backtracking) and an CUDA for GPU (focusing on efficiency)

%The contributions of this project are: \ul
%\item A classification of dynamic programming problems characteristics in terms of matrix shape and recurrence formulae dependencies.
%\item A systematic approach to convert a top-down recurrence description (grammar) into efficient bottom-up 
%\item A systematic approach to process backtracking information (focus on running time and memory efficiency)
%\item A language embedded in Scala (DSL) to express DP problems concisely (based on ADP)
%\item Two implementations: Scala for CPU (features) and an CUDA for GPU (efficiency)
%%\item Reuse of existing compiler technology (fusion) for a specific purpose
%%\item State of the art parallel implementation of these classes on GPUs
%%\item Normalization of the grammar into efficient productions
%%\item Code generator to transform a grammar into efficient code for CPU, GPU (and FPGA)
%\ule

% ------------------------------------------------------------------------------------------------
\newpage
%\section{Background}
\subsection{Dynamic programming} \label{intro_dp}
Dynamic programming consists of solving a problem by reusing subproblems solutions. A famous example of dynamic programming is the Fibonacci series that is defined by the recurrence
\[F(n+1) = F(n)+F(n-1) \qquad \text{ with } F(0)=F(1)=1 \]
which expands to (first 21 numbers)
\[1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, ...\]

A typical characteristic is that an intermediate solution is reused multiple times to construct larger solutions (here $F(3)$ helps constructing $F(4)$ and $F(5)$). Reusing an existing solution avoid redoing expensive computations: with memoization (memorizing intermediate results), the solution of $F(n)$ would be obtained after $n$ additions whereas without memoization it requires $F(n)-1$ additions !

Formally, dynamic programming problems respect the Bellman's principle of optimality: \textit{<<An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision>>}. This means that every intermediate result is computed only once, although it might be reused as basis for multiple larger problems, hence our first observation.

There exist various categories of dynamic programming:\ul
\item Series that operates usually on a single dimension (like Fibonacci)
\item Sequences alignment (matching two sequences at best), top-down grammar analysis (parenthesizing), sequence folding, ...
\item Tree-related algorithms: phylogenetic, trees raking, maximum tree independent set, ...
\ule

Since the first category is inherently sequential (progress cannot be faster than one element at a time) and the third category is both hard to parallelize efficiently (similar to a sparse version of the second category) and does not share much with the previous category, we focus on the second type of problems, which is also the most common.

Taking real-world examples, the average input size for sequence alignment is around 300K whereas for problems like RNA folding, input are usually around few thousands. Multiple input problems also require more memory: for instance matching 3 sequences is $O(n^3)$-space complex. Since we target a single computer with one or more attached devices (GPUs, FPGAs), and since we plan to maintain data in memory (due to the multiple reuse of intermediate solutions) the storage complexity must be relatively limited, compared to other problem that could leverage the disk storage. Hence in general, we focus on problems that have $O(n^2)$-space complexity whereas time complexity is usually $O(n^3)$ or larger. We encourage you to refer to the section~\ref{problems} for further classification and examples.

% ------------------------------------------------------------------------------------------------
\newpage
\subsection{Scala} \label{intro_scala}
\textit{<<Scala is a general purpose programming language designed to express common programming patterns in a concise, elegant, and type-safe way. It smoothly integrates features of object-oriented and functional languages, enabling programmers to be more productive. Many companies depending on Java for business critical applications are turning to Scala to boost their development productivity, applications scalability and overall reliability.>>}\footnote{\url{http://www.scala-lang.org}}

As the Scala \cite{scala} programming language is developed by our laboratory (LAMP, EPFL), it seems natural to use it as host language for our project, however, we would list some of its features \cite{scala_api} that makes it an interesting development language for this project:\ul
\item The functional programming style and syntactic sugar offered by Scala allows concise writing of implementation, analysis and transformations of our DSL, which would have been much more complex and tiresome in an imperative language like C.
\item Scala is largely adopted in the industry, which makes both the adoption of related project easier and offer a steeper learning curve to their potential users.
\item Since Scala programs execute in the JVM, they can benefit of the JNI, that is the possibility to dynamically load C libraries and interact with them, possibly leveraging  underlying hardware by launching CUDA kernels.
\item One Scala concept that we heavily use is \textit{traits} that can be viewed as abstract classes and mixed together, thereby allowing multiple inheritance.
\ule

% ------------------------------------------------------------------------------------------------
\subsection{Lightweight Modular Staging} \label{intro_lms}
Lightweight Modular Staging (LMS) \cite{lms}, \cite{lms_thesis} is a runtime code generation built on top of Scala virtualized \cite{scala_virtualized} that uses types to distinguish between binding time (compilation and runtime) for code compilation. This makes possible to annotate parts of the code with special types, such that their compilation is delayed until the program is executed. At run time, these parts are represented as a \textit{sea of nodes} that serve as the basis for another compilation phase where all the code executed until this point provides additional information to produce a more efficient compilation. The process of delaying the compilation is known as \textit{lifting} whereas \textit{lowering} corresponds to transforming this intermediate representation into executable code. LMS code generation is not limited to Scala, it can also target other languages like C. In short, LMS is an optimizing compiler framework that allows integration of domain-specific abstractions and optimizations into the generation process.

At first glance, LMS seems the ideal candidate to transform Scala code into its C-like equivalent. However, the concern in this project is that the GPU code sensibly differers from the original CPU code because the two implementations serve different purposes: CPU version (Scala) is more general whereas the GPU version trades some functionalities for performance and suffer additional restrictions, in particular for memory management and alignment. Ad-hoc C translation seems more appropriate than writing Scala code and convert it with LMS because LMS only understand a subset of both languages.

LMS would still be helpful for generating user-specific functions, as they are independent of the rest of the program and user wants to write his functions only once for both Scala and CUDA.

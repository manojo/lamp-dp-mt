\newpage
\section{Implementation} \label{implementation}
% ------------------------------------------------------------------------------------------------
\subsection{Ad-hoc CUDA}
In the project planning, an ad-hoc implementation phase immediately followed the problem analysis (we also present the parallelogram matrix case). The goal of this phase is threefold:\ol
\item Better understand the challenges in CUDA implementation of dynamic programming problems and get on par with state-of-art implementations.
\item Have a baseline implementation that is independent of the hardware and that could be benchmarked. We also tried to contact the authors of \cite{swat_mega} and \cite{gpu_atlp} to obtain their implementation. The former provided us with their implementation, which turned out to address large serial problems whereas our focus was on smaller non-serial problems, the latter did not respond to our solicitations.
\item Have an optimal implementation that can serve as target to be imitated and generalized by the code generation.
\ole

Leveraging the insights provided by \cite{gpu_atlp} and \cite{gpu_barrier}, we started with a basic implementation (where each CUDA thread processes one matrix line) with three additional optimizations:\ul
\item Memory accesses must be coalesced (memory accesses account for a significant part of the total running time, according to both manufacturer documentation and experiments \cite{perfeval})
\item Synchronization between threads can be done according to \cite{gpu_barrier}, additionally, we can slightly loosen the synchronization restrictions, as the paper describes a thread barrier whereas we only require a condition on previous thread progress (except for the parallelogram case, where we still require a barrier).
\item Computation progresses element-wise along the diagonal (maximizes the parallelism level)
\item Thread block size = warp size (32) to benefit from implicit synchronization within warps
\ule

% ----------------------------------------------
\subsubsection{Related work}
Since \cite{swat_mega} focuses on a different class of problem, we compare our implementation against \cite{gpu_atlp}, which provides an efficient matrix multiplication implementation. However, since we have neither the source code (or binary) nor the same evaluation hardware, we need to normalize the results. To do that, we present hardware differences and their result:

\def\unt#1{& \footnotesize #1}
\begin{table}[H]\begin{center}\begin{tabular}{lrrr} \toprule
\bf Graphic card &				& \bf Our 		& \bf  ATLP\cite{gpu_atlp} \\ \midrule
Model &						& GeForce GT 650M	& Tesla C1060 \\
Architecture, capability  &			& Kepler (3.0)	& GT200 (1.3) \\
Memory \unt{Mb}				& 1024		& 4096 \\
CUDA cores &					& 384		& 240 \\
Clock (core, memory) \unt{MHz}	& 756, 1953	& 1300, 1600 \\
Memory bus \unt{bit}				& 128		& 512 \\
Memory bandwidth \unt{GB/s}		& 28.8		& 102.4 \\
Processing power \unt{GFLOPS}	&564.5		& 622.08 \\ \midrule
\bf Processing speedup & 		& 1			& 1.07 \\
\bf Memory speedup & 			& 1			& 3.55 \\ \bottomrule
\end{tabular}\end{center}\caption{Graphic cards technical specifications (source: \href{http://en.wikipedia.org/wiki/Comparison_of_Nvidia_graphics_processing_units}{Wikipedia})}\end{table}

\begin{table}[H]\begin{center}\begin{tabular}{lrrrrrrrrrr} \toprule
\bf Matrix size & 128 & 256 & 512 & 1024 & 1536 & 2048 & 2560 & 3072 & 3584 & 4096 \\ \midrule
\bf No split & 0.07 & 0.09 & 0.19 & 0.59 & 1.27 & 2.25 & 3.51 & 5.07 & 6.92 & 9.06 \\
\bf Split at 1 & 0.06 & 0.07 & 0.08 & 0.14 & 0.26 & 0.47 & 0.77 & 1.21 & 1.80 & 2.57 \\ \bottomrule
\end{tabular}\end{center}\caption{ATLP\cite{gpu_atlp} results: matrix chain multiplication, execution time (in seconds)}\end{table}

% ----------------------------------------------
\subsubsection{Results}
We present here the timings of our implementation. For correctness, we first implemented a CPU version that we used to compare CUDA results against. Input data is made of random numbers. The implemented dynamic programming problems are:\ul
\item Rectangle: Smith-Waterman with arbitrary cost
\item Triangle: matrix chain multiplication
\item Parallelogram: polygon triangulation using a matrix larger than necessary. Note that this implementation uses at most 32 blocks to prevent dead locks on our hardware (restriction due to the number of concurrent threads on the device).
\ule

\begin{table}[H]
\begin{center}\begin{tabular}{rlrrr} \toprule
\bf Matrix size & \bf Comment & \bf R & \bf T & \bf P \\ \midrule
1024 & CPU					& 1.965		& 1.191		& 6.069 \\
2048 & CPU					& 27.229		& 15.296		& 57.323 \\
4096 & CPU					& 			& 177.608	&  \\
1024 & GPU baseline			& 0.838		& 0.500		& 0.516 \\
1024 & GPU sync improved		& 0.642		& 0.316		& 0.343 \\
2048 & GPU P $\le32$ blocks		& 2.864		& 1.427		& 2.096 \\
4096 & GPU 8 splits				& 21.902		& 8.841		& 16.767 \\
8192 & GPU 64 splits			& 159.058	& 62.064		& 135.793 \\
12288 & GPU 256 splits			& 419.030	& 196.971	& 460.912 \\
\bottomrule \end{tabular}\end{center}
\caption{Execution time (in seconds) for R=rectangle, T=triangle, P=parallelogram}
\end{table}

% ----------------------------------------------
\subsubsection{Results discussion}\ul
\item \textbf{User interface:} It has been put in evidence in \cite{perfeval} that using the GPU exclusively for CUDA or in combination with UI display (Mac OS) affects the performance (GeForce 330M). With the newer architecture, this difference has been reduced to less than 3.5\%, decoupled UI and CUDA   performing best. So we can safely ignore this issue.
\item \textbf{Blocks synchronization:}\ul
	\item Removing {\tt \_\_threadfence()} before the synchronization is not syntactically correct but results still remains valid, this confirms the observation made by \cite{gpu_barrier}. Speedup for matrix size of 1024 are 67ms (parallelogram) 100ms (triangle) 180ms (rectangle).
	\item In the parallelogram case, using all threads to monitor other blocks status instead of the first one only results in a 6.4x speedup (22.72$\to$3.52ms) for the parallelogram.
	\ule
\item \textbf{Multiple threads per matrix cell:} in the case of a triangular matrix, at each step, the number of cells to be computed (on the diagonal) decrease while the computation complexity increase (there is one more dependency). According to \cite{gpu_atlp}, the solution lies in adaptive thread mapping, using more than one thread to compute one matrix cell, depending on the complexity. However, in our setup (memory layout+algorithm+hardware), we did not found any improvement by doing so. We want to explore the reason for that: we pose as hypothesis that the bandwidth is the bottleneck of our setup and test it.\ul
\item

First we need to prove that we use almost all the available memory bandwidth: for matrix multiplication, in a triangular matrix, we have
\[\text{Total transfer}=\frac{n(n+1)}{2} \text{ writes} + \sum_{i=0}^{n-1} 2 i \cdot (n-i) \text{ reads}\]
where each write is 10 bytes (long+short), and each read is 8 bytes (long). For $n=4096$ we transfer
% n*(n + 1)/2*12 + Sum[2*i*(n - i), {i, 0, n - 1}]*8
183'352'614'912 bytes which corresponds to 183.35GB. In 8.841 seconds, we can transfer theoretically at most $8.841\cdot 28.8 = 254 \rm GB$. Hence  72\% of the algorithm running time is spent into memory accesses.

\item On a 4096 matrix, if we assume that ATLP card would have the same bandwidth as our card, their running time would be
\[2.57 \cdot (1-.72) + 2.57 \cdot 0.72 \cdot \tfrac{102.4_{GB/s}}{28.8_{GB/s}} = 9.43\rm s_{\text{ ATLP}} > 8.84\rm s_{\text{ our}}\]
Which shows that our algorithm is comparable to theirs. However, we must avoid a close comparison because the fundamental hardware differences would make a tight computation almost intractable (additionally, we do not have ATLP source code).
\ule
As a conclusion, (1) we must remain away to invalidate their result as previous hardware generations might be subject to more constraint to our hardware and (2) we are on par, if not better with one of the best current implementation.

\item \textbf{Threads number:} reducing the number of threads launched at different splits of the algorithm (especially in latest splits in rectangular and triangular shapes) does not bring any speedup. Even worse, it slows down slightly the computation. We might attribute this to a better constant transformation by the compiler. Hence, having many idle threads does not impede performance.

\item \textbf{Unrolling:} unrolling the inner loops (non-serial dependencies) a small number of time provide some speedup, for a 2048-matrix respectively 10.9\% (rectangle, $2.765\to 2.464$), 14.1\% (triangle, $1.427\to 1.225$) and 9.7\% (parallelogram $1.539\to 1.389$). The best experimental number of unrolling is 5.
\ule

% ------------------------------------------------------------------------------------------------
\subsection{Scala parsers} \label{scala_parsers}
The Scala parsers consist in 4 traits that are used to construct a DSL program:\ul
\item \textbf{Signature:} abstraction to define input ({\tt Alphabet}) and output ({\tt Answer}) types, and the aggregation function. The signature is implemented by all other traits (in particular algebras and grammars).
\item \textbf{BaseParsers:} serves as basis for the two other traits and defines common features. It implements the {\tt Parser} abstraction and all its inheriting classes: {\tt Tabulate}, (abstract) {\tt Terminal}, {\tt Aggregate}, {\tt Filter}, {\tt Map}, {\tt Or}, {\tt Concat}. Terminals are further specialized in the two other traits (ADPParsers and TTParsers). The parser abstraction specifies 3 methods:\ul
	\item {\tt apply(subword)} computes the parser result; it is used to obtain the corresponding results.
	\item {\tt unapply(subword,backtrack)} computes the previous step of the backtrack by returning subsequences at the origin of the result; it is invoked recursively to obtain the full backtrack trace.
	\item {\tt reapply(subword,backtrack)} is very similar to apply, except that it  computes only the results matching the backtrack. It is used to construct the result corresponding to a backtrack trace (possibly in a different domain, pretty printing, ...).
	\ule
	To support analysis, the parsers carry additional values:\ul
	\item Minimum and maximum yield size: functions evaluated recursively except for tabulations where value is attributed in the yield analysis phase.
	\item Number of inner alternatives: helps counting alternatives, hereby guaranteeing an unique number for each (provided that parsers obtain non-overlapping ranges).
	\item Number of inner moving concatenations: helps determining required storage for the backtrack as well as retrieving the appropriate index in the backtrack phase
	\ule
	Additionally, the BaseParser implements the analysis that are shared by both the Scala and the CUDA version: dead rules elimination, yield analysis and dependencies ordering. Finally, it provides some implicit functions to flatten nested tuples (that are constructed by multiple concatenations).
\item \textbf{ADPParsers:} used as basis for a single track DP grammar (using one input sequence). It defines the concatenation operator $\sim$ ({\tt Concat} wrapper), and the terminals (empty, element and sequence). Additionally, it defines the interface functions {\tt parse(input)}, {\tt backtrack(input)} and {\tt build(in,backtrack)} that respectively compute the result, the backtrack and the result corresponding to a trace.
\item \textbf{TTParsers:} used to define two-track DP grammar (using a pair of sequences as input). Similarly, this class defines concatenations $-\!\!\sim$ and $\sim\!\!-$, terminals (for each track) and the {\tt parse(in1,in2)}, {\tt backtrack(in1,in2)} and {\tt build(in1,in2,backtrack)} functions.
\ule

\begin{figure}[H]\begin{center}\setlength{\unitlength}{.6cm}\begin{picture}(14,11)
\put(3,8){\tbox{8}{2.5}{{\bf Signature} \footnotesize\\ Types: Alphabet, Answer \\ $h$ (aggregation function)}}
\put(0,4){\tbox{14}{2.5}{{\bf BaseParsers} \footnotesize\\ Tabulate, Terminal, Aggregate, Filter, Map, Or, Concat \\ Analysis: dead rules, yield analysis, dependencies}}
\put(0,0){\tbox{6}{2.5}{{\bf ADPParsers} \footnotesize\\ $\sim$, $\sim(a,b,c,d)\sim$ \\ Single track terminals}}
\put(8,0){\tbox{6}{2.5}{{\bf TTParsers} \footnotesize\\ $-\!\!\sim$, $\sim\!\!-$ \\ Two-tracks terminals}}
{\linethickness{1.5pt}\put(3,2.5){\vector(1,1){1.5}}\put(11,2.5){\vector(-1,1){1.5}}\put(7,6.5){\vector(0,1){1.5}}}
\end{picture}\end{center}\caption{Scala parsers class diagram (simplified)}\end{figure}

% ------------------------------------------------------------------------------------------------
\newpage
\subsection{Code generation} \label{codegen}
The code generation step produces multiple outputs that are tightly bound to each other. Besides the Scala wrapper (a simple JNI interface), in the C/CUDA code generated we distinguish:\ol
\item JNI input and output conversion functions
\item Host helpers for memory management and scheduling of CUDA kernels
\item CUDA matrix computation, which can be further decomposed into matrix scheduling (loops) and (matrix cell) computation. 
\item CUDA backtrack collection kernel
\ole

\begin{figure}[H]\begin{center}\setlength{\unitlength}{.6cm}\begin{picture}(16,8)
\put(0,-.5){
{\color{lightgray}\put(0,7){\tbox{4}{1}{\footnotesize\bf BaseParsers}}
	\put(0,2){\rotatebox{90}{\tbox{4}{1}{\footnotesize\bf ADPParsers}}}
	\put(1.5,2){\rotatebox{90}{\tbox{4}{1}{\footnotesize\bf TTParsers}}}
	{\linethickness{1.5pt}\put(.5,6){\vector(1,1){1}}\put(2,6){\vector(0,1){1}}}}
{\linethickness{1.5pt}\put(6,6){\vector(-7,2){3.5}}\put(8,1.33){\vector(3,1){2}}\put(8,1.33){\vector(3,-1){2}}}
\put(3,0){\tbox{6}{6}{{\bf CodeGen} \footnotesize\\ code generation: parsers, backtrack, helpers, JNI \\[12pt] CodeCompiler}}
\put(10,3.5){\tbox{6}{2.5}{{\bf CodeHeader} \footnotesize\\ Types conversion, headers management}}
\put(10,1.5){\tbox{6}{1}{\bf ScalaCompiler}}\put(10,0){\tbox{6}{1}{\bf CCompiler}}
\moveto(2,2)\lineto(2,1.5)\lineto(2.5,1.5)\strokepath\put(2.5,1){\vector(1,0){.5}}
\moveto(.5,2)\lineto(.5,1)\lineto(2.5,1)\strokepath\put(2.5,1.5){\vector(1,0){.5}}
\put(-1.25,-.1){\tbox[0l]{4.5}{1}{\footnotesize automatic call \tiny\\ if CodeGen mixed-in}}
}
\end{picture}\end{center}\caption{Code generation and runtime engine class diagram (simplified)}\end{figure}

% ----------------------------------------------
\subsubsection{Scala structures conversion (JNI)}
Since Scala general types can be extremely complex and might heavily depend of the JVM, we want to restrict the supported types; additionally types should be of fixed size for more efficient processing and easier memory allocation. We support the following types:\ul
\item \textbf{Primitive types:} natively supported in both Java and C. Since there is some little semantics difference between these two languages types, we used C (signed) types as reference. Supported types are: boolean, byte (unsigned char), char, short, int (32bit), long (64bit), float and double.
\item \textbf{Empty case classes:} user-defined types might be more complex, so we allow users to define case classes that serve as data container and would be translated into C {\tt struct}s.
\item \textbf{Tuples:} if the user-defined type is fairly simple, a named case class might be cumbersome. Tuples are a syntactical lightweight alternative to case classes, although they translate very similarly. Since Tuple classes are generic and can carry different member types; need to name tuple types uniquely, according to their arity and inner types.
\ule

Currently we use {\tt Manifest}s and reflection to extract types, and convert their string representation into our restricted subset. Manifests expands tuple inner types and reflection can be used to find class member's types. This imposes the additional restriction that we can not nest tuples into case classes, because generic types are then erased. However, the same effect could be achieved with Scala 2.10 {\tt TypeTag}s, converting immediately to concrete type tree representation using macros expansion\footnote{Hint provided by Eugene Burmako, \url{https://gist.github.com/4407488}}.

The JNI functions are involved at input to decode sequences arrays and at output, to encode the result and possibly its corresponding trace. Input method is constructed in two steps:\ul
\item Recursively obtain the classes and accessor methods of the composite input type. A subtle variation is that case classes primitive types are immediately converted into native types whereas tuple members are boxed in their respective class (i.e. {\tt java.lang.Integer}, ...).
\item For each element of the input array, retrieve the objects recursively and write their primitive values in the corresponding {\tt struct} array.
\ule
The output method consist of two different steps:\ul
\item Converting the result into its JVM counterpart by using the opposite rule as for decoding input (but with JNI types specified in the constructor lookup instead of accessors).
\item Optionally encoding the backtrack: this is pretty straightforward as the structure is more regular (and make uses of Lists); additional care should be taken to avoid bloating concatenation indices lists with unnecessary elements (as C uses fixed memory whereas Scala lists length might vary).
\ule

% ----------------------------------------------
\subsubsection{Host wrappers}
The host wrappers are the functions bridging between JNI and CUDA; their duties are:\ul
\item Exposing JNI parsing and backtracking functions
\item Calling appropriate conversion methods
\item Allocating host and CUDA memory (and managing transfers between them)
\item Launching CUDA kernels: matrix computation, backtrack, and possibly aggregation within window (additional aggregation among window results, would this option be set)
\ule

One peculiarity of our execution environment, is that the kernel execution duration is bound to approximately 10 seconds\footnote{Hard limit imposed by the operating system. Although workarounds exist for Linux and Windows (requiring a second graphic card to display the UI), none of them is compatible with Mac OS. Eventually, a hack has been devised to force the UI on CPU while keeping the dedicated CUDA card powered; unfortunately this does not alleviate the kernel execution timeout.}. To solve this issue, we estimate the overall complexity of matrix computation, which allows us to estimate running time, then break computation into multiple kernels sufficiently small to fit in the time limit.

Since computations are made diagonal-by-diagonal (see \ref{matrix_scheduling}), we can easily decompose the matrix computation by adapting the number of diagonals computed per kernel. The global complexity being the product of the number of elements and the complexity per element, the latter being equal to the number of unbounded concatenations (where maximal size is infinite).

\textbf{Bonus: problems larger than device memory}\\
Problems larger than the device memory can actually be processed on recent CUDA devices (with CUDA architecture $\ge 2.0$) as these are able to address the main memory from the device. However, since the distance between CUDA processors and memory is increased, there is an approximate $5\times$ slowdown penalty to be paid in this configuration (experimentally, on a $1024\times 1024$ triangular matrix). Nevertheless, this workaround implementation has 2 benefits:\ul
\item It allows larger problem to be solved, with very little implementation effort, would the user be patient enough for the computation to terminate
\item It provides a good estimation of the main memory usage penalty, and hereby a strong argument in favor of the implementation described in \ref{ns_mem_transfer} (with lest than 1\% overhead due to transfers). However, since we have not found concrete applications with such matrix size, the benefit of supporting large matrices is unclear, hence we leave the optimal implementation for future work.
\ule

% ----------------------------------------------
\subsubsection{Matrix computation scheduling} \label{matrix_scheduling}
Similarly as in the ad-hoc implementation, progress is made along the diagonal (see \ref{mem_layout}) and each thread is responsible of one line. That is, the matrix is swept horizontally by a <<diagonal of threads>>, that are enabled only if they are within a valid matrix cell.

\begin{figure}[H]\begin{center}\setlength{\unitlength}{.6cm}\begin{picture}(6,6)
	\def\Cfl2#1{#1{0,4}#1{0,3}#1{1,3}#1{0,2}#1{1,2}#1{2,2}#1{0,1}#1{1,1}#1{2,1}#1{3,1}#1{0,0}#1{1,0}#1{2,0}#1{3,0}#1{4,0}}
	\Cfl2{\Cg}
	{\color{cyan}\Cd[0,1]{4,1}{2.8}\Cd[0,1]{4,2}{1.8}\Cd[1,0]{1,4}{2.8}\Cd[1,0]{2,4}{1.8}\Cd[1,1]{3,3}{0.8}\Cd[2,1]{2,3}{1.8}\Cd[1,2]{3,2}{.9}}
	\Cd[0,1]{4,3}{0.8}\Cd[1,0]{3,4}{0.8}
	\multiput(3.5,5.5)(1,-1){6}{\circle{.4}}
	\multiput(0,0)(1,0){7}{\line(0,1){6}}\multiput(0,0)(0,1){7}{\line(1,0){6}} % matrix
	\put(3.5,5.5){\color{lightgray}\line(1,-1){5}}
	\multiput(3.7,5.5)(1,-1){6}{\color{red}\linethickness{1.5pt}\vector(1,0){2}}
	\put(8.6,0.05){\tiny thread 0}
	\put(3.6,5.75){\tiny thread 5}
\end{picture}\end{center}\caption{<<Diagonal of threads>> and maximal dependencies}\label{fig:diag_deps}\end{figure}

Special care must be taken to handle computation dependencies: within a warp, all threads are executed at the same time, hence no synchronization is necessary. To benefit from this implicit synchronization, we set block size being equal to wrap size. It remains to provide inter-block synchronization: dependencies are along line, column and possibly intermediate elements. By induction on rows and columns, it suffice to have the last column and row element valid. Since line is computed by the current thread (hereby valid), it only remains to guarantee that the column element of the previous line is valid (in figure \ref{fig:diag_deps}, previous refers to the line immediately below). To do that, each block writes last valid diagonal in a <<lock>> array, and next block need only to wait (polling) until desired element is marked valid. Notice that {\tt \_\_threadfence} is not mandatory (hereby slightly improving performance), verifying the observation of \cite{gpu_barrier}.

\begin{lstlisting}[language=C,caption=Synchronization with previous thread block (active waiting)]
__global__ void gpu_solve(/*...*/ volatile unsigned* lock, // = {0}
		unsigned d_start, unsigned d_stop) {
	const unsigned tB = blockIdx.x;
	unsigned tP=d_start; // block progress

	for (unsigned diag=d_start; diag<d_stop; ++diag) {
		/* ... compute diagonal values ... */

		// __threadfence();
		if (threadIdx.x == 0) {
			lock[tB] = ++tP;
			if (tB > 0) while(lock[tB-1]<tP) {}
		}
		__syncthreads();
	}
}
\end{lstlisting}

% ----------------------------------------------
\subsubsection{Parsers code generation}
Parsers generation is independent of user-defined function generation (see \ref{user_fun_gen}). Tabulation inner parsers are first wrapped in additional aggregation (by $h$, hereby ensuring they produce at most one result) and normalized (according to \ref{normalization}); code generation then occurs recursively, producing a list of loops and conditions, and body (possibly with a hoisted part). Additionally, position variables are maintained and subrule index and concatenation indices are propagated. We give an overview of each parser transformation:\ul
\item \textbf{Terminal:} provides its own C code, which correspond usually to the input element value, its position or the position of the matching range.
\item \textbf{Tabulate:} is a simple value load, possibly wrapped into a validity check. Useless validity verification can be removed by marking the tabulation as <<always valid>>.
\item \textbf{Aggregate:} corresponds to an intermediate (value,backtrack) pair where inner parsers write their result; outermost aggregation is written back to corresponding (cost, backtrack) matrices. Validity information, and concatenation indices are propagated within backtrack. To preserve a correct semantic, inner aggregations body is hoisted outside loops and condition checks of the enclosing parser.
\item \textbf{Or:} since parsers are normalized and operate on a single aggregation result, it suffice to emit sequentially code of alternatives.
\item \textbf{Map:} wraps its argument into a the user-defined function call
\item \textbf{Filter:} wraps its body into user-defined condition check
\item \textbf{Concat:} fixed size concatenation are wrapped in simple conditions; moving concatenations are wrapped in a {\tt for} loop. The loops and conditions are further simplified to reduce range and remove useless conditions before actual code is emitted.
\ule

Intermediate types must be correctly declared. To do that each user-defined function provides its input and output types. Aggregation temporary values declaration is ensured by a \textit{exists-or-declare} header policy that is called for every type declaration.

% ----------------------------------------------
\subsubsection{Backtracking on the GPU}
The backtracking is processed similarly as with the Scala parser, the major difference being that since we are generating the C code, we can provide an immediate mapping from the subrule index to the backtrack elements to add to the trace. The backtrack is done in 3 steps:\ul
\item If window is set, the windowing aggregation kernel is run to determine the position of the best result within the matrix. Otherwise the best position can be found at the last computed element of the matrix.
\item For a $m\times n$ matrix, allocate a $m+n$ vector with two heads (reading, writing, initialized at the same position). Write the best element in the vector.
\item While there is a vector element that has been written but not read\ul
	\item From the parser id and its position retrieve the corresponding (subrule, concatenation indices) pair by reading in the corresponding matrix cell
	\item Using this, write new backtrack items that are at the origin of the current element.
	\ule
\ule

Since code is generated, it is possible to write the last step using a switch case, hereby flattening the writes in the vector (compared to recursive calls in Scala). Finally, since the trace has to be reversed, we can obtain this transformation for free by constructing the trace list from the end in the JNI conversion. Reversing the list present the advantage that trace is immediately usable to construct the desired element. It might be possible that Scala and CUDA parser provide different traces to construct the same result, because the trace verifies the dependency order, which is only a partial order.

% ----------------------------------------------
\subsubsection{User functions generation} \label{user_fun_gen}
The user generation function needs to be tightly integrated with the rest of the code generation. To do that, we need to establish a relation between the Scala function and its C counterpart. This is done by modifying the Scala function such that it embeds its C code and related types (input, output and possibly internal structures). To do that, LMS is used to generate both Scala and C code (as the user would want to write only once his function, using the corresponding LMS {\tt Rep} types). The two implementations are then mixed to provide the augmented Scala function that can then be used at appropriate places by either the Scala parsers or the code generator.

Actually, the idea of mixing the two implementations into a single function emerged from experiments with the Scala macros \cite{scala_macros}, where it is possible to modify the AST of the Scala program before actually compiling it. Macros could also be an alternative to LMS in the sense that they have the same power in this particular case (because the code is just converted from Scala to C and does not benefit of additional run-time information); however, relying only on macros would imply rewriting significant portions of code conversion, which might end up being a duplicated effort with LMS. The most interesting use of the macros would actually be to stage plain Scala to its LMS representation in the <<context>> of user functions, hereby unleashing the power of LMS without forcing the DSL user to explicitly specify {\tt Rep} types\footnote{Since this is an ongoing project at LAMP with different schedule as this project, we do not want to duplicate effort currently but might integrate it at a later stage.}.

Another advantage of using LMS only for user-specific function, is that it does not impose any restriction on the types manipulated by Scala, hereby providing the opportunity to solve the DP problem (possibly on CUDA using restricted types) and apply the solution (in Scala) on complex types that would have no representation in LMS.

% ------------------------------------------------------------------------------------------------
\subsection{Runtime execution engine}
The runtime execution engine is made of two instrumented compilers:\ul
\item A wrapper for {\tt g++} and {\tt nvcc} that can combine different file types ({\tt .h}, {\tt .c}, {\tt .cu}) into a JNI library which is then loaded into the current JVM instance. If necessary, paths can be customized to fit the user environment.
\item A wrapper for the Scala compiler, which allow the creation of Scala interface to the freshly compiled JNI libraries. It should be noted there that using {\tt VirtualDirectory} as compilation target prevents the interaction with JNI, hence physical path has to be used.
\ule

These two compilers interfaces are then mixed in another class that transform the previously (see \ref{codegen}) generated code, fixing input sizes and splits (number of kernels to launch to respect the time limit) constants, and execute it.

% ------------------------------------------------------------------------------------------------
\subsection{LibRNA}
Since the energy computation for RNA folding involve complex coefficient and computations (that are also standardized through libraries and coefficient files), we might want to provide the user with it. To do that, we based our library on \cite{gapc_thesis} which itself is based on \href{http://www.tbi.univie.ac.at/~ivo/RNA/}{ViennaRNA}.\cite{vienna_rna}. Since the library is provided in C, we rely on JNI to reuse the code without modifying it; this allows Scala to immediately benefit from it, but also makes possible to write a GPU version, provided that the related functions are simple enough to be expressible in CUDA.

{\color{red}
XXX: attempt to strip it to make CUDA version
}

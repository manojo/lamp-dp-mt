\newpage
\section{Implementation}
% ------------------------------------------------------------------------------------------------
\subsection{Ad-hoc CUDA}
In the project planning, an ad-hoc implementation phase immediately followed the problem analysis (we also present the parallelogram matrix case). The goal of this phase is threefold:\ol
\item Better understand the challenges in CUDA implementation of dynamic programming problems and get on par with state-of-art implementations.
\item Have a baseline implementation that is independent of the hardware and that could be benchmarked. We also tried to contact the authors of \cite{swat_mega} and \cite{gpu_atlp} to obtain their implementation. The former provided us with their implementation, which turned out to address large serial problems whereas our focus was on smaller non-serial problems, the latter did not respond to our solicitations.
\item Have an optimal implementation that can serve as target to be imitated and generalized by the code generation.
\ole

Leveraging the insights provided by \cite{gpu_atlp} and \cite{gpu_barrier}, we started with a basic implementation (where each CUDA thread processes one matrix line) with three additional optimizations:\ul
\item Memory accesses must be coalesced (memory accesses account for a significant part of the total running time, according to both manufacturer documentation and experiments \cite{perfeval})
\item Synchronization between threads can be done according to \cite{gpu_barrier}, additionally, we can slightly loosen the synchronization restrictions, as the paper describes a thread barrier whereas we only require a condition on previous thread progress (except for the parallelogram case, where we still require a barrier).
\item Computation progresses element-wise along the diagonal (maximizes the parallelism level)
\item Thread block size = warp size (32) to benefit from implicit synchronization within warps
\ule

% ----------------------------------------------
\subsubsection{Related work}
Since \cite{swat_mega} focuses on a different class of problem, we compare our implementation against \cite{gpu_atlp}, which provides an efficient matrix multiplication implementation. However, since we have neither the source code (or binary) nor the same evaluation hardware, we need to normalize the results. To do that, we present hardware differences and their result:

\def\unt#1{& \footnotesize #1}
\begin{table}[H]\begin{center}\begin{tabular}{lrrr} \toprule
\bf Graphic card &				& \bf Our 		& \bf  ATLP\cite{gpu_atlp} \\ \midrule
Model &						& GeForce GT 650M	& Tesla C1060 \\
Architecture, capability  &			& Kepler (3.0)	& GT200 (1.3) \\
Memory \unt{Mb}				& 1024		& 4096 \\
CUDA cores &					& 384		& 240 \\
Clock (core, memory) \unt{MHz}	& 756, 1953	& 1300, 1600 \\
Memory bus \unt{bit}				& 128		& 512 \\
Memory bandwidth \unt{GB/s}		& 28.8		& 102.4 \\
Processing power \unt{GFLOPS}	&564.5		& 622.08 \\ \midrule
\bf Processing speedup & 		& 1			& 1.07 \\
\bf Memory speedup & 			& 1			& 3.55 \\ \bottomrule
\end{tabular}\end{center}\caption{Graphic cards technical specifications (source: \href{http://en.wikipedia.org/wiki/Comparison_of_Nvidia_graphics_processing_units}{Wikipedia})}\end{table}

\begin{table}[H]\begin{center}\begin{tabular}{lrrrrrrrrrr} \toprule
\bf Matrix size & 128 & 256 & 512 & 1024 & 1536 & 2048 & 2560 & 3072 & 3584 & 4096 \\ \midrule
\bf No split & 0.07 & 0.09 & 0.19 & 0.59 & 1.27 & 2.25 & 3.51 & 5.07 & 6.92 & 9.06 \\
\bf Split at 1 & 0.06 & 0.07 & 0.08 & 0.14 & 0.26 & 0.47 & 0.77 & 1.21 & 1.80 & 2.57 \\ \bottomrule
\end{tabular}\end{center}\caption{ATLP\cite{gpu_atlp} results: matrix chain multiplication, execution time (in seconds)}\end{table}

% ----------------------------------------------
\subsubsection{Results}
We present here the timings of our implementation. For correctness, we first implemented a CPU version that we used to compare CUDA results against. Input data is made of random numbers. The implemented dynamic programming problems are:\ul
\item Rectangle: Smith-Waterman with arbitrary cost
\item Triangle: matrix chain multiplication
\item Parallelogram: polygon triangulation using a matrix larger than necessary. Note that this implementation uses at most 32 blocks to prevent dead locks on our hardware (restriction due to the number of concurrent threads on the device).
\ule

\begin{table}[H]
\begin{center}\begin{tabular}{rlrrr} \toprule
\bf Matrix size & \bf Comment & \bf R & \bf T & \bf P \\ \midrule
1024 & CPU					& 1.965		& 1.191		& 6.069 \\
2048 & CPU					& 27.229		& 15.296		& 57.323 \\
4096 & CPU					& 			& 177.608	&  \\
1024 & GPU baseline			& 0.838		& 0.500		& 0.516 \\
1024 & GPU sync improved		& 0.642		& 0.316		& 0.343 \\
2048 & GPU P $\le32$ blocks		& 2.864		& 1.427		& 2.096 \\
4096 & GPU 8 splits				& 21.902		& 8.841		& 16.767 \\
8192 & GPU 64 splits			& 159.058	& 62.064		& 135.793 \\
12288 & GPU 256 splits			& 419.030	& 196.971	& 460.912 \\
\bottomrule \end{tabular}\end{center}
\caption{Execution time (in seconds) for R=rectangle, T=triangle, P=parallelogram}
\end{table}

% ----------------------------------------------
\subsubsection{Results discussion}\ul
\item \textbf{User interface:} It has been put in evidence in \cite{perfeval} that using the GPU exclusively for CUDA or in combination with UI display (Mac OS) affects the performance (GeForce 330M). With the newer architecture, this difference has been reduced to less than 3.5\%, decoupled UI and CUDA   performing best. So we can safely ignore this issue.
\item \textbf{Blocks synchronization:}\ul
	\item Removing {\tt \_\_threadfence()} before the synchronization is not syntactically correct but results still remains valid, this confirms the observation made by \cite{gpu_barrier}. Speedup for matrix size of 1024 are 67ms (parallelogram) 100ms (triangle) 180ms (rectangle).
	\item In the parallelogram case, using all threads to monitor other blocks status instead of the first one only results in a 6.4x speedup (22.72$\to$3.52ms) for the parallelogram.
	\ule
\item \textbf{Multiple threads per matrix cell:} in the case of a triangular matrix, at each step, the number of cells to be computed (on the diagonal) decrease while the computation complexity increase (there is one more dependency). According to \cite{gpu_atlp}, the solution lies in adaptive thread mapping, using more than one thread to compute one matrix cell, depending on the complexity. However, in our setup (memory layout+algorithm+hardware), we did not found any improvement by doing so. We want to explore the reason for that: we pose as hypothesis that the bandwidth is the bottleneck of our setup and test it.\ul
\item

First we need to prove that we use almost all the available memory bandwidth: for matrix multiplication, in a triangular matrix, we have
\[\text{Total transfer}=\frac{n(n+1)}{2} \text{ writes} + \sum_{i=0}^{n-1} 2 i \cdot (n-i) \text{ reads}\]
where each write is 10 bytes (long+short), and each read is 8 bytes (long). For $n=4096$ we transfer
% n*(n + 1)/2*12 + Sum[2*i*(n - i), {i, 0, n - 1}]*8
183'352'614'912 bytes which corresponds to 183.35GB. In 8.841 seconds, we can transfer theoretically at most $8.841\cdot 28.8 = 254 \rm GB$. Hence  72\% of the algorithm running time is spent into memory accesses.

\item On a 4096 matrix, if we assume that ATLP card would have the same bandwidth as our card, their running time would be
\[2.57 \cdot (1-.72) + 2.57 \cdot 0.72 \cdot \tfrac{102.4_{GB/s}}{28.8_{GB/s}} = 9.43\rm s_{\text{ ATLP}} > 8.84\rm s_{\text{ our}}\]
Which shows that our algorithm is comparable to theirs. However, we must avoid a close comparison because the fundamental hardware differences would make a tight computation almost intractable (additionally, we do not have ATLP source code).
\ule
As a conclusion, (1) we must remain away to invalidate their result as previous hardware generations might be subject to more constraint to our hardware and (2) we are on par, if not better with one of the best current implementation.

\item \textbf{Threads number:} reducing the number of threads launched at different splits of the algorithm (especially in latest splits in rectangular and triangular shapes) does not bring any speedup. Even worse, it slows down slightly the computation. We might attribute this to a better constant transformation by the compiler. Hence, having many idle threads does not impede performance.

\item \textbf{Unrolling:} unrolling the inner loops (non-serial dependencies) a small number of time provide some speedup, for a 2048-matrix respectively 10.9\% (rectangle, $2.765\to 2.464$), 14.1\% (triangle, $1.427\to 1.225$) and 9.7\% (parallelogram $1.539\to 1.389$). The best experimental number of unrolling is 5.
\ule

% ------------------------------------------------------------------------------------------------
\subsection{Scala parsers}
The Scala parsers consist in 4 traits that are used to construct a DSL program:\ul
\item \textbf{Signature:} abstraction to define input ({\tt Alphabet}) and output ({\tt Answer}) types, and the aggregation function. The signature is implemented by all other traits (in particular algebras and grammars).
\item \textbf{BaseParsers:} serves as basis for the two other traits and defines common features. It implements the {\tt Parser} abstraction and all its inheriting classes: {\tt Tabulate}, (abstract) {\tt Terminal}, {\tt Aggregate}, {\tt Filter}, {\tt Map}, {\tt Or}, {\tt Concat}. Terminals are further specialized in the two other traits (ADPParsers and TTParsers). The parser abstraction specifies 3 methods:\ul
	\item {\tt apply(subword)} computes the parser result; it is used to obtain the corresponding results.
	\item {\tt unapply(subword,backtrack)} computes the previous step of the backtrack by returning subsequences at the origin of the result; it is invoked recursively to obtain the full backtrack trace.
	\item {\tt reapply(subword,backtrack)} is very similar to apply, except that it  computes only the results matching the backtrack. It is used to construct the result corresponding to a backtrack trace (possibly in a different domain, pretty printing, ...).
	\ule
	To support analysis, the parsers carry additional values:\ul
	\item Minimum and maximum yield size: functions evaluated recursively except for tabulations where value is attributed in the yield analysis phase.
	\item Number of inner alternatives: helps counting alternatives, hereby guaranteeing an unique number for each (provided that parsers obtain non-overlapping ranges).
	\item Number of inner moving concatenations: helps determining required storage for the backtrack as well as retrieving the appropriate index in the backtrack phase
	\ule
	Additionally, the BaseParser implements the analysis that are shared by both the Scala and the CUDA version: dead rules elimination, yield analysis and dependencies ordering. Finally, it provides some implicit functions to flatten nested tuples (that are constructed by multiple concatenations).
\item \textbf{ADPParsers:} used as basis for a single track DP grammar (using one input sequence). It defines the concatenation operator $\sim$ ({\tt Concat} wrapper), and the terminals (empty, element and sequence). Additionally, it defines the interface functions {\tt parse(input)}, {\tt backtrack(input)} and {\tt build(in,backtrack)} that respectively compute the result, the backtrack and the result corresponding to a trace.
\item \textbf{TTParsers:} used to define two-track DP grammar (using a pair of sequences as input). Similarly, this class defines concatenations $-\!\!\sim$ and $\sim\!\!-$, terminals (for each track) and the {\tt parse(in1,in2)}, {\tt backtrack(in1,in2)} and {\tt build(in1,in2,backtrack)} functions.
\ule

\begin{figure}[H]\begin{center}\setlength{\unitlength}{.6cm}\begin{picture}(14,11)
\put(3,8){\tbox{8}{2.5}{{\bf Signature} \footnotesize\\ Types: Alphabet, Answer \\ $h$ (aggregation function)}}
\put(0,4){\tbox{14}{2.5}{{\bf BaseParsers} \footnotesize\\ Tabulate, Terminal, Aggregate, Filter, Map, Or, Concat \\ Analysis: dead rules, yield analysis, dependencies}}
\put(0,0){\tbox{6}{2.5}{{\bf ADPParsers} \footnotesize\\ $\sim$, $\sim(a,b,c,d)\sim$ \\ Single track terminals}}
\put(8,0){\tbox{6}{2.5}{{\bf TTParsers} \footnotesize\\ $-\!\!\sim$, $\sim\!\!-$ \\ Two-tracks terminals}}
{\linethickness{1.5pt}\put(3,2.5){\vector(1,1){1.5}}\put(11,2.5){\vector(-1,1){1.5}}\put(7,6.5){\vector(0,1){1.5}}}
\end{picture}\end{center}\caption{Class diagram (simplified)}\end{figure}

% ------------------------------------------------------------------------------------------------
\newpage
\subsection{Code generation}
The code generation step produces multiple outputs that are tightly bound to each other. Besides the Scala wrapper (a simple JNI interface), in the C/CUDA code generated we distinguish:\ol
\item JNI input and output conversion functions
\item Host memory management and scheduling of CUDA kernels
\item CUDA matrix computation, which can be further decomposed into matrix scheduling (loops) and (matrix cell) computation. 
\item CUDA backtrack collection kernel
\ole

\subsubsection{Scala structures conversion (JNI)}
Since Scala general types can be extremely complex and might heavily depend of the JVM, we want to restrict the supported types; additionally types should be of fixed size for more efficient processing and easier memory allocation. We support the following types:\ul
\item \textbf{Primitive types:} natively supported in both Java and C. Since there is some little semantics difference between these two languages types, we used C (signed) types as reference. Supported types are: boolean, byte (unsigned char), char, short, int (32bit), long (64bit), float and double.
\item \textbf{Empty case classes:} user-defined types might be more complex, so we allow users to define case classes that serve as data container and would be translated into C {\tt struct}s.
\item \textbf{Tuples:} if the user-defined type is fairly simple, a named case class might be cumbersome. Tuples are a syntactical lightweight alternative to case classes, although they translate very similarly. Since Tuple classes are generic and can carry different member types; need to name tuple types uniquely, according to their arity and inner types.
\ule

{\color{red} Currently we use {\tt Manifest}s and reflection to extract types, and convert their string representation into our restricted subset. Manifests expands tuple inner types and reflection can be used to find class member's types. This imposes the additional restriction that we can not nest tuples into case classes, because generic types are then erased. However, the same effect could be achieved with Scala 2.10 {\tt TypeTag}s although we then need to rely on macros expansion to convert them properly into concrete classes. [see hint from Eugene, \url{https://gist.github.com/4407488}].}

The JNI functions are involved at input to decode sequences arrays and at output, to encode the result and possibly its corresponding trace. Input method is constructed in two steps:\ul
\item Recursively obtain the classes and accessor methods of the composite input type. A subtle variation is that case classes primitive types are immediately converted into native types whereas tuple members are boxed in their respective class (i.e. {\tt java.lang.Integer}, ...).
\item For each element of the input array, retrieve the objects recursively and write their primitive values in the corresponding {\tt struct} array.
\ule
The output method consist of two different steps:\ul
\item Converting the result into its JVM counterpart by using the opposite rule as for decoding input (but with JNI types specified in the constructor lookup instead of accessors).
\item Optionally encoding the backtrack: this is pretty straightforward as the structure is more regular (and make uses of Lists); additional care should be taken to avoid bloating concatenation indices lists with unnecessary elements (as C uses fixed memory whereas Scala lists length might vary).
\ule

{\center\color{red} XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\\ CONTINUE HERE\\ XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX}

\begin{verbatim}
jobject Java_{className}_parse(JNIEnv* env, jobject obj, jobjectArray input1) {
  input_t *in1=NULL; jni_read(env,input1,&in1);
  g_init(in1,NULL); free(in1); g_solve();
  T3iii score=g_backtrack(NULL,NULL);
  jobject result = jni_write(env, score, NULL, 0);
  g_free(); return result;
}

jobject Java_{className}_backtrack(JNIEnv* env, jobject obj, jobjectArray input1) {
  input_t *in1=NULL; jni_read(env,input1,&in1);
  g_init(in1,NULL); free(in1); g_solve();
  trace_t *trace=NULL; unsigned size=0;
  T3iii score=g_backtrack(&trace,&size);
  jobject result = jni_write(env, score, trace, size); free(trace);
  g_free(); return result;
}
\end{verbatim}

\subsubsection{Host wrappers}
XXX: Explain the complexity analysis and split into multiple kernels.

XXX: interface JNI and CUDA kernel launch, allocate/free host/CUDA memory, sets the number of slices for the computation (overcome the kernel timeout issue).

\begin{verbatim}
void g_init(input_t* in1, input_t* in2) {
  cuMalloc(g_in1,sizeof(input_t)*(M_H-1));
  cuPut(in1,g_in1,sizeof(input_t)*(M_H-1),NULL);
  g_in2=NULL;
  cuMalloc(g_cost,sizeof(cost_t)*MEM_MATRIX);
  cuMalloc(g_back,sizeof(back_t)*MEM_MATRIX);
}

void g_free() { cuFree(g_in1); cuFree(g_cost); cuFree(g_back); cudaDeviceReset(); }

void g_solve() {
  #define WARP_SIZE 32 // constant over CUDA devices
  unsigned blk_size = WARP_SIZE;
  unsigned blk_num = (M_H+blk_size-1)/blk_size;
  unsigned* lock; cuMalloc(lock,sizeof(unsigned)*blk_num);
  cuErr(cudaMemset(lock,0,sizeof(unsigned)*blk_num));
  gpu_solve<<<blk_num, blk_size, 0, NULL>>>(g_in1, g_in2, g_cost, g_back, lock, 0, (M_W));
  cuFree(lock);
}

T3iii g_backtrack(trace_t** trace, unsigned* size) {
  T3iii res; unsigned i0=0, j0=M_W-1;
  cuGet(&res,&g_cost[idx(i0,j0)].M,sizeof(T3iii),NULL);
  if (trace && size) {
    unsigned mem=(M_W+M_H)*sizeof(trace_t);
    trace_t *g_trace=NULL; cuMalloc(g_trace,mem);
    unsigned *g_size=NULL; cuMalloc(g_size,sizeof(unsigned));
// XXX: new size adaptation
    gpu_backtrack<<<1,1,0,NULL>>>(g_trace, g_size, g_back, i0, j0);
    cuGet(size,g_size,sizeof(unsigned),NULL); cuFree(g_size); mem=(*size)*sizeof(trace_t);
    *trace=(trace_t*)malloc(mem); cuGet(*trace,g_trace,mem,NULL); cuFree(g_trace);
  }
  return res;
}
\end{verbatim}

\subsubsection{Matrix computation scheduling}
Similarly as in the ad-hoc implementation, progress is made along the diagonal (see \ref{mem_layout}) and each thread is responsible of one line. That is, the matrix is swept horizontally by a <<diagonal of threads>>, that are enabled only if they are within a valid matrix cell.

\begin{figure}[H]\begin{center}\setlength{\unitlength}{.6cm}\begin{picture}(6,6)
	\def\Cfl2#1{#1{0,4}#1{0,3}#1{1,3}#1{0,2}#1{1,2}#1{2,2}#1{0,1}#1{1,1}#1{2,1}#1{3,1}#1{0,0}#1{1,0}#1{2,0}#1{3,0}#1{4,0}}
	\Cfl2{\Cg}
	{\color{cyan}\Cd[0,1]{4,1}{2.8}\Cd[0,1]{4,2}{1.8}\Cd[1,0]{1,4}{2.8}\Cd[1,0]{2,4}{1.8}\Cd[1,1]{3,3}{0.8}\Cd[2,1]{2,3}{1.8}\Cd[1,2]{3,2}{.9}}
	\Cd[0,1]{4,3}{0.8}\Cd[1,0]{3,4}{0.8}
	\multiput(3.5,5.5)(1,-1){6}{\circle{.4}}
	\multiput(0,0)(1,0){7}{\line(0,1){6}}\multiput(0,0)(0,1){7}{\line(1,0){6}} % matrix
	\put(3.5,5.5){\color{lightgray}\line(1,-1){5}}
	\multiput(3.7,5.5)(1,-1){6}{\color{red}\linethickness{1.5pt}\vector(1,0){2}}
	\put(8.6,0.05){\tiny thread 0}
	\put(3.6,5.75){\tiny thread 5}
\end{picture}\end{center}\caption{<<Diagonal of threads>> and maximal dependencies}\label{fig:diag_deps}\end{figure}

Special care must be taken to handle computation dependencies: within a warp, all threads are executed at the same time, hence no synchronization is necessary. To benefit from this implicit synchronization, we set block size being equal to wrap size. It remains to provide inter-block synchronization: dependencies are along line, column and possibly intermediate elements. By induction on rows and columns, it suffice to have the last column and row element valid. Since line is computed by the current thread (hereby valid), it only remains to guarantee that the column element of the previous line is valid (in figure \ref{fig:diag_deps}, previous refers to the line immediately below). To do that, each block writes last valid diagonal in a <<lock>> array, and next block need only to wait (polling) until desired element is marked valid.


\begin{lstlisting}[language=C,caption=Synchronization with previous thread block (active waiting)]
__global__ void gpu_solve(/*...*/ volatile unsigned* lock, unsigned d_start, unsigned d_stop) {
	const unsigned tB = blockIdx.x;
	unsigned tP=d_start; // block progress

	for (unsigned diag=d_start; diag<d_stop; ++diag) {
		/* ... compute diagonal values ... */

		// __threadfence();
		if (threadIdx.x == 0) {
			lock[tB] = ++tP;
			if (tB > 0) while(lock[tB-1]<tP) {}
		}
		__syncthreads();
	}
}
\end{lstlisting}


\subsubsection{Parsers code generation}
XXX: structures generation(?)
XXX: refer to architecture, explain concrete transformations for all parsers

\begin{verbatim}
__device__ inline T3iii fun0(T2ii i) { return (T3iii){i._1,0,i._2}; }
__device__ inline int fun1(T3iii a) { return a._2; }
__device__ inline T3iii fun2(T3iii l, T3iii r) { return (T3iii){l._1, l._2 + r._2 + l._1 * l._3 * r._3, r._3}; }

__global__ void gpu_solve(const input_t* in1, const input_t* in2, cost_t* cost, back_t* back, /*...*/) {
...
        #define VALID(I,J,RULE) (back[idx(I,J)].RULE.rule!=-1)
        /* --- M[i,j] --- */
        if (i+1==j) {
          T3iii _c=fun0(in1[i]); if (fun1(_c)<fun1(_cost.M) || _back.M.rule==-1) { _cost.M=_c; _back.M=(bt1){0}; }
        }
        _unroll for(int k=i+1; k<j; ++k) {
          if (VALID(i,k,M) && VALID(k,j,M)) {
            T3iii _c=fun2(cost[idx(i,k)].M,cost[idx(k,j)].M); if (fun1(_c)<fun1(_cost.M) || _back.M.rule==-1) { _cost.M=_c; _back.M=(bt1){1,{k}}; }
          }
        }
        cost[idx(i,j)] = _cost;
        back[idx(i,j)] = _back;
...
}
\end{verbatim}

\subsubsection{Backtracking on the GPU}
XXX: explain how it is done, how to get list reversal for free, explain lattice of elements with construction order.

\begin{verbatim}
__global__ void gpu_backtrack(trace_t* trace, unsigned* size, back_t* back, int i0, int j0) {
  const unsigned trace_len[2] = {1,1};
  trace_t *rd=trace, *wr=trace; *size=0;
  #define PUSH_BACK(I,J,RULE) { wr->i=I; wr->j=J; wr->rule=RULE; ++wr; ++(*size); }
  PUSH_BACK(i0,j0,0);
  for(;rd<wr;++rd) {
    bt1* bt;
    switch (rd->rule) {
      case 0: bt=(bt1*)&back[idx(rd->i,rd->j)].M; break;
      case 1: bt=(bt1*)&back[idx(rd->i,rd->j)].M; break;
    }
    rd->rule=bt->rule;
    for (int i=0,l=trace_len[rd->rule]; i<l; ++i) rd->pos[i]=bt->pos[i];
    switch (rd->rule) {
      case 0: break;
      case 1: PUSH_BACK(rd->i,rd->pos[0],0); PUSH_BACK(rd->pos[0],rd->j,0); break;
    }
  }
}
\end{verbatim}

\subsection{Runtime execution engine}

\begin{verbatim}
 * Parser construction:
 * 1. Assign a "OR_id" (sub_rule_id) and "CONCAT_id" to all parsers so that we know which rule applies
 *    How to skip some indices ?
 *
 * Scala running:
 * 2. Provide meaningful backtrack: rule_id and list of concat indices
 *
 * Code generation:
 * 1. Normalize rules (at hash-map insertion (?))
 * 2. Compute dependency analysis between rules => order them into a list/queue
 *    - If rule R contains another rule S unconcatenated (or concatenated with empty)
 *      then we have S -> T (S before T)
 * 3. Compute the maximal number of concatenation among each rule (field in Treeable(?))
 * 4. Break rules into subrules (at each Or, which must be at top of the rule)
\end{verbatim}


\subsection{LibRNA (?)}
The project development was scheduled in \ul
\item Problems analysis
\item Ad-hoc implementation (CUDA)
\item Porting ADP parsers
\item Improving parsers / adding functionalities
\item Integrating CUDA generation into parsers
\ule

\begin{verbatim}
1 week on hash maps
2 weeks to define and analyze the problems (also with FPGA in mind)
3 weeks restriction to non-serial and ad-hoc implementation on GPU (rectangle, triangle, parallelogram); translation of ADP parsers in Scala (Manohar)
1 week run-time engine for Scala/JNI/CUDA
05.11 - rework of the ADP parser to aim at generating C-like code
12.11 - rework of ADP parsers to extend usages (cyclic, two-track) and aim at automatic backtracking
19.11 - explorations in LMS / macros
26.11 - full backtracking: apply/unapply/reapply, rework of the classes
03.12 - Zuker/JNI
10.12 - Yield analysis, code generation
17.12 - code generation: detupling, generic backtrack (vs. ad-hoc), nested aggregates, empty results support
24.12 - code generation, sick
31.12 - report
\end{verbatim}

\subsection{LMS integration}
Why cannot use LMS for everything
Where and how is LMS useful


\documentclass[11pt]{article}
\input{inc/style.sty}
\begin{document}

\section{Benchmarks} \label{benchmarks}
In an attempt to provide realistic benchmarks, we tried to gather related implementations. The authors of \cite{gpu_atlp} did not respond to our multiple solicitations. The authors of \cite{swat_mega} were very friendly and provided us their source code. Unfortunately, since they address a different category of problem (they focus on huge serial problems whereas we focus on smaller non-serial problems) their implementation might be biased towards large sequences that our implementation cannot address. Finally, we asked lately the authors of \cite{gpu_rnafold} who did not respond either to our solicitations.
	
We organize the benchmarks as follow: if we have at our disposal a working implementation that could be run on our evaluation platform, we use it, otherwise, we refer to the related paper and rescale the result according to the memory throughput and computation throughput of the related device so that we can have a good approximation of how they could compare.

\subsection{Metrics} \label{metrics}
The main metrics of interest is the running time. In an attempt to reduce the variance, we would like to run multiple consecutive test and take the median running time, since the median is less sensitive to outlier than the average\cite{perfeval}. Unfortunately, several factors hampers these ideal conditions. First the variance in the running time of CUDA kernels might be significant, in particular for short running time. This is due to the fact that the GPU needs to be 'warmed-up' before actual computation can happen. Similarly, the JVM is also subject to running time variance that is mainly due to the garbage collection\footnote{\url{http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html\#cms.overhead}} and JIT optimizations \cite{java_jit}.

Also the input and problem might introduce variance. As example, we can consider two extreme cases: matrix chain multiplication and Zuker RNA folding, with a test environment of 100 random inputs (of length respectively 512 and 80) and a GPU warmup of 10 computations. In this settings, matrix chain multiplication computations are executed in a perfectly constant time\footnote{With respect to truncation and measurment accurcy, has less than 1\% of variation (not observable).} (0.127 seconds), which mean that we sufficiently reduced the noise. Oppositely, the Zuker RNA folding running times appear much more scattered as presented below:

\begin{figure}[H]\begin{center}\includegraphics[width=16cm]{inc/var_zuker.pdf}\end{center}
\caption{Zuker folding running time (seconds). Quartiles: 0.152, 0.183 (median), 0.214}\label{fig:var_zuker}\end{figure}

Using the QQplot\footnote{Quantile-to-quantile plot, used to compare two distributions against each other.}, the distribution is heavily tailed (has more results towards the ends of the range) than a Gaussian distribution (fig.~\ref{fig:var_zuker} center) but fits better an uniform distribution (fig.~\ref{fig:var_zuker} right). If we run multiple time the program over the same input, we obtain the same behavior as with the matrix multiplication (strictly identical time); hence we can conclude that Zuker is an input sensitive problem whereas matrix chain multiplication is not. It follows that we need to be careful to test with exactly identical input set different implementations.

As the device memory is quite limited, it seems interesting to also take into account the space usage. The space requirement limits the maximal size of addressable problems  on a particular hardware. This might be a concern for large problems, because they would require special adaptation to handle such cases both correctly and efficiently. However, this metric heavily depends on the problem and simple solutions like using a device with larger memory or using main memory (if a $5\times$ slowdown is still acceptable) could solve this issue, hence we do not consider this metric hereafter (except as an upper bound on the dimension of the input).

\subsection{Benchmarking platform}
Our benchmarking platform is an Apple notebook with a Core i7-3720QM with 16Gb of main memory and an NVIDIA GeForce GT 650M running under MacOS X 10.8 and Oracle JDK 1.7.0-10. A workaround (see listing~\ref{cpu_workaround}) allows us to use the CPU to render the user interface while leaving the graphic card available to execute CUDA kernels. Unfortunately, due to impossibility to disable the watchdog timer in MacOS, CUDA kernels are limited to few seconds of running time before they are automatically aborted.

% ----------------------------------------------
% MatrixMult-512, Mac+JDK7\ul
% Original: 24.937 sec / 24.658 sec
% Optimized concatenation: 19.329 sec / 18.924 sec
% Tabulation as arrays+inline: 14.976 sec / 14.849 sec
\newpage
% ------------------------------------------------------------------------------------------------
{\center\color{red} \noindent\rule{16cm}{0.4pt} \\ XXX: CONTINUE HERE :XXX \\}
% ------------------------------------------------------------------------------------------------
\subsection{Matrix chain multiplication}
Since we have seen that this problem is not input sensitive in (\S\ref{metrics}), we can safely use different random number generators among different implementations without compromising the validity of the results.  Also note that the hand-optimized results are slightly worse than those presented in (\S\ref{baseline_impl}), this is because 64-bit mode is enabled. Because external libraries linked with the Java virtual machine must be in 64 bit, we also enabled this mode in hand-optimized version to maintain a fair comparison, thereby reducing the performance of CUDA operations.
% CPU: DynaProg(Scala), hand optimized C, GAPC and ADPFusion
% GPU: GPU: DynaProg(CUDA), hand optimized CUDA, ATLP and GAPC-OpenCL?

\def\hdr#1#2{\begin{minipage}{3.5cm} {\bf #1} \\[-2pt] \footnotesize #2 \vspace{6pt} \end{minipage}}
\begin{table}[H]\begin{center}{\small\begin{tabular}{llrrrrrrrr}\toprule
& \normalsize\bf Matrix dimension & \normalsize\bf 128 & \normalsize\bf 256 & \normalsize\bf 512 & \normalsize\bf 1024 & \normalsize\bf 2048 & \normalsize\bf 4096 & \normalsize\bf 8192 \\
\midrule \multirow{4}{*}{\rotatebox{90}{\normalsize\bf CPU $\qquad$}}
& \hdr{DynaProg}{Scala version}
	& 0.247		& 2.048		& 16.657		& 134.207	& 1101.134	&  			& 		 \\
& \hdr{Optimized}{C, single thread}
	& 0.001		& 0.008		& 0.078		& 1.179		& 19.811		& 206.556	& 2010.485 \\
& \hdr{GAPC}{\cite{gapc_thesis}, C, single thread}
	& 			& 			& 			& 			& 			& 			& 		 \\
& \hdr{ADP Fusion}{\cite{adp_fusion}}
	& 			& 			& 			& 			& 			& 			& 		 \\[-2pt]
\midrule \multirow{4}{*}{\rotatebox{90}{\normalsize\bf GPU $\qquad\quad$}}
& \hdr{DynaProg}{CUDA version}
	& 0.028		& 0.046		& 0.129		& 0.348		& 1.685		& 10.308		& 71.215 \\
& \hdr{Optimized}{CUDA, 64-bit}
	& 0.011		& 0.020		& 0.074		& 0.315		& 1.651		& 10.354		& 72.376 \\
& \hdr{ATLP}{\cite{gpu_atlp}, rescaled results}
	& 			& 			& 			& 			& 			& 			& 		 \\
& \hdr{GAPC}{\cite{gapc_thesis}, OpenCL}
	& 			& 			& 			& 			& 			& 			& 		 \\
\\[-10pt] \bottomrule\end{tabular}}\end{center}
\caption{Running time of matrix chain multiplication (in seconds)}\end{table}

The running time of DynaProg/CUDA includes the overhead of back and forth JNI conversion (scales linearly between 0.018 and 0.057 seconds) but does not include the overhead due to the code generation which decomposes in 0.068 seconds for analysis and code synthesis (once per algebra/grammar pair) and 0.086 + 1.753 seconds for respectively Scala and CUDA compilation (constant time, once per problem dimension).

For DynaProg/Scala we use a variant of the problem description: the original version only stores the matrix multiplication score whereas the modified version also stores the matrix dimension. This allows a speedup of $2.9\times$ probably due to additional function calls to retrieve the values. Also with the default JVM parameters, the program cannot address sequences longer than $\sim 420$ elements due to a stack overflow, for these benchmarks, we increased this limit.

% 1024=0.316, 2048=1.427, 4096=8.841, 8192=62.064




{\color{red}\ol
\item Write some conclusion ideas
\item Run benchmark and add results in report (automate, generate raw Matlab/TeX output)
\item Stabilize times, statistics, use CUDA profiler(?)
\ole}

\subsection{Smith-Watermann}
% CPU: DynaProg(Scala), hand optimized C, GAPC and ADPFusion
% GPU: DynaProg(CUDA), hand optimized CUDA, CUDAlign
XXX: to compare appropriately either use 3 matrices or constant gap penalty
XXX: move toward use of 3 matrices

\def\hdr#1#2{\begin{minipage}{3.5cm} {\bf #1} \\[-2pt] \footnotesize #2 \vspace{6pt} \end{minipage}}
\begin{table}[H]\begin{center}{\small\begin{tabular}{llrrrrrrrr}\toprule
& \normalsize\bf Matrix dimension & \normalsize\bf 128 & \normalsize\bf 256 & \normalsize\bf 512 & \normalsize\bf 1024 & \normalsize\bf 2048 & \normalsize\bf 4096 & \normalsize\bf 8192 \\
\midrule \multirow{4}{*}{\rotatebox{90}{\normalsize\bf CPU $\qquad$}}
& \hdr{DynaProg}{Scala version}
	& 			& 			& 			& 			& 			& 			& 		 \\
& \hdr{Optimized}{C, single thread}
	& <0.001		& <0.001		& 0.002		& 0.009		& 0.042		& 0.171		& 0.706 \\
& \hdr{GAPC}{\cite{gapc_thesis}, C, single thread}
	& 			& 			& 			& 			& 			& 			& 		 \\
& \hdr{ADP Fusion}{\cite{adp_fusion}}
	& 			& 			& 			& 			& 			& 			& 		 \\[-2pt]
\midrule \multirow{4}{*}{\rotatebox{90}{\normalsize\bf GPU $\qquad$}}
& \hdr{DynaProg}{CUDA version}
	& 			& 			& 			& 			& 			& 			& 		 \\
& \hdr{Optimized}{CUDA, 64-bit}
	& 0.002		& 0.004		& 0.004		& 0.007		& 0.024		& 0.070		& 0.270 \\
& \hdr{CUDAlign}{\cite{swat_linear}, version 2.0}
	& 			& 			& 			& 			& 			& 			& 		 \\
& \hdr{GAPC}{\cite{gapc_thesis}, OpenCL}
	& 			& 			& 			& 			& 			& 			& 		 \\
\\[-10pt] \bottomrule\end{tabular}}\end{center}
\caption{Running time of Smith-Waterman (in seconds)}\end{table}





\subsection{Zuker RNA folding}
% CPU: DynaProg(Scala), GAPC, ADPFusion and ViennaRNA
% GPU: DynaProg(CUDA), RNAFold?/Lavenier VS ViennaRNA-OpenCL?, GAPC-OpenCL?
XXX

\subsection{Synthetical results}
XXX

{\color{red} We want to see the benefits of moving to CUDA, also compare to how far Scala is from C.}

RUN 5-10x until running time stabilizes on Scala

 {\color{red} Size analysis to know what storage size we require: ex: Zucker requires $O(n^2)+O(n)$ storage...}

XXX: future work: proper support of two-track grammars

XXX: future work, resize tables appropriately as in GAPL.

XXX: future work: infer automatically "always non-empty" property

\subsection{Scala parsers}

\subsection{CUDA parsers}
{\color{red} XXX: explain why slower: approx 3x more memory used per score (maintain matrix dimensions)

XXX: continue benchmarking here
}
\begin{table}[H]\begin{center}\begin{tabular}{lrrrrrr} \toprule
Matrix size			& 1024	& 2048	& 4096	& 8192 \\
Splits				& 1		& 8		& 64		& 512 \\ \midrule
Analysis				& 0.007	& 0.007	& 0.007	& 0.007 \\
Code generation		& 0.071	& 0.072	& 0.073	& 0.071 \\
C/CUDA compilation	& 3.025	& 1.836	& 1.832	& 1.841 \\
Scala compilation		& 1.837	& 1.853	& 1.830	& 1.767 \\ \midrule
- JNI read				& 0.027	& 0.028	& 0.029	& 0.058 \\
- CUDA compute		& 0.868	& 3.378	& 19.782	& 139.697 \\
- CUDA backtrack		& 0.009	& 0.012	& 0.023	& 0.045 \\
- JNI output			& 0.005	& 0.009	& 0.018	& 0.032 \\
Total execution			& 0.915	& 3.432	& 19.860	& 139.846 \\ \bottomrule
\end{tabular}\end{center}\caption{Preliminary results for MatrixMultGen}\end{table}

{\color{red} XXX: Compare current implementation versus ad-hoc implementation. Compare CUDA vs Scala (we might need to ad-hoc fix stack overflows in Scala).If Zuker coefficients can be fixed, compare performance with \cite{adp_gpu} by rescaling numbers wrt to bandwidth and computation performance.}


\end{document}

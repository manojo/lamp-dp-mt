\section{Benchmarks} \label{benchmarks}
In an attempt to provide realistic benchmarks, we tried to gather related implementations. The authors of \cite{gpu_atlp} did not respond to our multiple solicitations. The authors of \cite{swat_mega} were very friendly and provided us their source code. Unfortunately, since they address a different category of problem (they focus on huge serial problems whereas we focus on smaller non-serial problems) their implementation might be biased towards large sequences that our implementation cannot address. Finally, we asked lately the authors of \cite{gpu_rnafold} who did not respond either to our sollicitations.

We organize the benchmarks as follow: if we have at our disposal a working implementation that could be run on our evaluation platform, we use it, otherwise, we refer to the related paper and rescale the result according to the memory throughput and computation throughput of the related device so that we can have a good approximation of how they could compare.

\subsection{Metrics}
The main metrics of interest is the running time. In an attempt to reduce the variance, we would like to run multiple consecutive test and take the median running time (since median is less sensitive to outlier than the average). Unfortunately, several factors hampers these ideal conditions. First the variance in the running time of CUDA kernels might be quite significant, in particular for short running time. This is due to the fact that the GPU needs to be 'warmed-up' during few seconds before actual computation can happen

  In case the variance is significant, we wou

talk about input sensivity, backup with results on rnafold and matrix mult

As the device memory is quite limited, it seems interesting to also take into account the space usage. The space requirement limits the maximal size of addressable problems  on a particular hardware. This might be a concern for large problems, because they would require special adaptation to handle such cases both correctly and efficiently. However, simple solutions like using a device with larger memory or using main memory (if a $5\times$ slowdown is still acceptable) could solve this issue.

\subsection{Benchmarking platform}
Our benchmarking platform is a notebook with a Core i7-3720QM, 16Gb or main memory and an NVIDIA GeForce GT 650M running under MacOS X 10.8. Tests are made when the discrete GPU is not involved in rendering the user interface\footnote{This involves a workaround to keep the discrete GPU powered, while the CPU is involved in rendering graphics; source code available on demand.}. Due to impossibility to disable the watchdog timer in MacOS, CUDA kernels are limited to few seconds of running time before they are automatically aborted.

% ------------------------------------------------------------------------------------------------
{\center\color{red} \noindent\rule{16cm}{0.4pt} \\ XXX: CONTINUE HERE :XXX \\}
% ------------------------------------------------------------------------------------------------

{\color{red}\ol
\item Make LibRNA work with CUDA generated parsers
\item Finish the LibRNA section, write some conclusion ideas
\item Run benchmark and add results in report (automate, generate raw Matlab/TeX output)
\item Stabilize times, statistics, use CUDA profiler(?)
\ole}

\subsection{Matrix chain multiplication}
\subsubsection{CPU: DynaProg(Scala), hand optimized C, GAPC and ADPFusion}
XXX

\subsubsection{GPU: DynaProg(CUDA), hand optimized CUDA, ATLP and GAPC-OpenCL?}
XXX

\subsection{Smith-Watermann}
\subsubsection{CPU: DynaProg(Scala), hand optimized C, GAPC and ADPFusion}
XXX

\subsubsection{GPU: DynaProg(CUDA), hand optimized CUDA, CUDAlign}
XXX

\subsection{Zuker RNA folding}
\subsubsection{CPU: DynaProg(Scala), GAPC, ADPFusion and ViennaRNA}
XXX

\subsubsection{GPU: DynaProg(CUDA), RNAFold?/Lavenier VS ViennaRNA-OpenCL?, GAPC-OpenCL?}
XXX

\subsection{Synthetical results}
XXX

{\color{red} We want to see the benefits of moving to CUDA, also compare to how far Scala is from C.}

RUN 5-10x until running time stabilizes on Scala

 {\color{red} Size analysis to know what storage size we require: ex: Zucker requires $O(n^2)+O(n)$ storage...}

XXX: future work: proper support of two-track grammars

XXX: future work, resize tables appropriately as in GAPL.

XXX: future work: infer automatically "always non-emptiness"

\subsection{Scala parsers}
{\color{red}
MatrixMult-512, Mac+JDK7\ul
\item Original: 24.937 sec / 24.658 sec
\item Optimized concatenation: 19.329 sec / 18.924 sec
\item Tabulation as arrays+inline: 14.976 sec / 14.849 sec
\ule}

\subsection{CUDA parsers}
{\color{red} XXX: explain why slower: approx 3x more memory used per score (maintain matrix dimensions)

XXX: continue benchmarking here
}
\begin{table}[H]\begin{center}\begin{tabular}{lrrrrrr} \toprule
Matrix size			& 1024	& 2048	& 4096	& 8192 \\
Splits				& 1		& 8		& 64		& 512 \\ \midrule
Analysis				& 0.007	& 0.007	& 0.007	& 0.007 \\
Code generation		& 0.071	& 0.072	& 0.073	& 0.071 \\
C/CUDA compilation	& 3.025	& 1.836	& 1.832	& 1.841 \\
Scala compilation		& 1.837	& 1.853	& 1.830	& 1.767 \\ \midrule
- JNI read				& 0.027	& 0.028	& 0.029	& 0.058 \\
- CUDA compute		& 0.868	& 3.378	& 19.782	& 139.697 \\
- CUDA backtrack		& 0.009	& 0.012	& 0.023	& 0.045 \\
- JNI output			& 0.005	& 0.009	& 0.018	& 0.032 \\
Total execution			& 0.915	& 3.432	& 19.860	& 139.846 \\ \bottomrule
\end{tabular}\end{center}\caption{Preliminary results for MatrixMultGen}\end{table}

{\color{red} XXX: Compare current implementation versus ad-hoc implementation. Compare CUDA vs Scala (we might need to ad-hoc fix stack overflows in Scala).If Zuker coefficients can be fixed, compare performance with \cite{adp_gpu} by rescaling numbers wrt to bandwidth and computation performance.}


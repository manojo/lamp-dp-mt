% ------------------------------------------------------------------------------------------------
\section{Future work}
We consider several directions and possible extensions for our work. We briefly describe each of them and give an idea of how they could be implemented:\ol

\item \textbf{Fusion and C with LMS:} Although we gained some speedup by optimizing manually the Scala parsers, we cannot benefit from grammar-specific optimizations. Passing the whole grammar to LMS could possibly lead to more efficient code, by folding multiple parser functions into a single one and providing loop fusion\footnote{For example if the aggregation function is simple enough as maximum or minimum, aggregation could be merged with element processing, simliarly as described in \S\ref{normalization}.}. The added cost of function lookup, even if minimal, might still account for a non-negligible part of the total running time as parser processing is very simple but run repeatedly a large number of time. Also constructing large lists of candidates that are later reduced to single results increases the work of the garbage collector. Generating the grammar through LMS has two major benefits:\ol
\item Improve the performance of the parsers within the JVM. Since we previously argued in favor of a decoupling of the algebra and the grammar, and since we want to reach optimal performance, we would need to provide both a plain version for result processing (independent of LMS restrictions so that we can use arbitrary functions within the algebra) and an optimized version for dynamic programming computations (possibly with restricted types). The latter could be achieved similarly as the current CUDA code generation: the Scala parsers could be converted in an abstract syntax tree (AST) that could then be merged with the algebra nodes and passed to LMS for code generation.
\item Since experimental results have shown that in some situations, the CPU outperforms the GPU, it might be interesting to target single thread C implementation to benefit from these situations (thereby removing the overhead of the JVM). Using LMS would provide such code generation mechanism. Since the program can be single threaded, no complex synchronization mechanism needs to be involved, hence this is a good candidate for LMS multi-architecture code generation. 
\ole
\item \textbf{Macros:} macros provide an interesting meta-programming opportunity as they are run after the typing phase of the Scala compiler and can leverage all the compile-time typing information. We could use them to either simplify the user-functions description (by converting their types into LMS) or even provide ad-hoc conversion from the Scala AST to C code.
\item \textbf{Non-serial scheduling for problems larger than the device memory:} as described in \S\ref{ns_mem_transfer}, it could be possible to handle problems that are larger than the device memory in an efficient way, thereby dramatically reducing the memory transfer penalties introduced by the main memory usage. However, this comes at the price of a more involved kernel scheduling and a complex element indexing strategy (because the enclosing matrix block needs to be looked-up before addressing one of its element).
\item \textbf{Serial problems larger than memory:} As discussed in \S\ref{serial_memory}, this class of problems require a completely different implementation. Since the authors of \cite{swat_mega} are planning to publish an extension for their implementation, duplicating the effort might not be worth the price. Would their future implementation be sufficiently modular, we could integrate it in our framework and redirect compatible grammars to this state of art implementation.
\item \textbf{CUDA $k$-best parsers:} since a $k$-best algorithm has constant memory requirements, an efficient algorithm for CUDA could be devised: instead of comparing with only one value to find the best value, it suffice to compare with $k$ values instead. Hence cost and backtrack matrix would contain $k$ elements per cell. Validity flag for one value can easily be extended to multiple values; cells with fewer results would simply have fewer elements marked valid.
\item \textbf{Multi-dimensional matrices and independent computations:} In the current implementation, all the matrices are encoded such that they are of the same size. Leveraging the yield analysis, we could reduce the dimension of smaller matrices (for tabulations with bounded maximal size). Matrix of different dimensions must be stored in their own array (versus being in a single array of matrices of similar dimensions). Also matrices might possibly be of different storage complexity: looking back at the Zuker problem description (\S\ref{zuker}), there are two $O(n^2)$ matrices and one $O(n)$ matrix. This discrepancy in the sizes also leads to multiple indexing strategies (depending on the complexity) and a more complex scheduling where matrix must be computed sequentially whereas in the current computation, one position is computed in all matrices at once.
\item \textbf{Data granularity:} Since the major bottleneck of CUDA architecture is the memory, we might focus on data representation: in the current project, data is stored in primary types but we could store them more efficiently. For example, RNA is represented with only 4 letters (g,a,t,c), thus 4 symbols could be encoded in a single byte. Unfortunately, this optimizations seems to only apply for the input data. Another solution in this direction is to operate on multiple cells with one thread, the argument being that they could share a row or a column, thereby dividing the number of memory accesses for non-serial dependencies on that axis.
\item \textbf{Adding FPGA as target platform:} Initially envisioned a second target platform, the underlying complexity of transforming DP recurrences into VHDL code made us leave this platform aside for the scope of this project. The reconfigurability possibilities of FPGA make them attractive whenever it comes to very simple and massively parallel computations where the data can be pipelined; this makes serial dynamic programming problems good candidates for such implementation.
\item \textbf{Algorithmic analysis:} so far, we considered that the DSL user would write an optimal program. Another direction in which we could improve the parsers is the recurrence analysis, either by removing serial dependencies when possible (\S\ref{calc_simplifications}) or reducing the algorithmic complexity by creating intermediate tabulations (\S\ref{user_lang}). These analysis would certainly involve a strong mathematical analysis and the ratio benefit over implementation complexity would be quite small under the initial assumption that the DSL users are experts in their field (thereby knowing how to optimize manually the grammar).
\item \textbf{Non-emptiness analysis:} At code generation level, when a tabulation is known to be \textit{non-empty} (every cell contain a valid result), it is possible to remove the corresponding verifications, thereby reducing the parser complexity\footnote{The validity of a result is determined if its corresponding backtrack has a valid rule number. Skipping this test saves a memory load, and a condition testing.}. Such analysis needs to guarantee that all paths would produce at least on candidate. Such analysis might be quite complex as it requires induction (for example in matrix multiplication, we need to prove inductively that every cell element is non-empty). Currently, this information needs to be explicitly provided by the programmer (by setting a flag on the non-empty tabulation).
\item \textbf{Pruning:} described in \cite{swat_mega}, this optimization could lead to a reduction of the computation, provided that the algorithm final score can be bounded. Such optimization would only be relevant with a non-uniform computation strategy where the matrix is tiled, thereby making it possible to prune entire computation tiles.
\ole

% ------------------------------------------------------------------------------------------------
\newpage
\section{Conclusion}
This Master project report focuses on how to solve efficiently dynamic programming. By restricting to the class of problem involving sequences, we were able to extract generic patterns and expose them to parallel architectures like CUDA. To do that, we first depicted the dynamic programming landscape and defined a set of problems we would like to solve (\S\ref{problems}). Then we discussed the required functionalities to provide the user with a convenient embedded DSL based on the ADP formalization (\S\ref{architecture}). These architectural decisions lead us to consider two different implementations: one in Scala that allows the same expressivity as ADP (in term of multiple solutions search), and one in CUDA that focuses on efficiently obtaining a single optimal result on GPU. We then provide some technical details explaining how these ideas are put in practice in DynaProg (\S\ref{implementation}). Finally, we see in the benchmarks (\S\ref{benchmarks}) that generated code performances are comparable with existing implementations.

From a larger perspective, we can describe our approach as a generic pattern to address domain-specific problems:\ol
\item Understand the gist of the problem and generalize its specific characteristics to similar problems (this can be called domain-specific knowledge)
\item Based on this information, it is possible to devise an efficient implementation, possibly parallel\footnote{Because the single core (processor) model has reached its limit and the industry is moving towards multi-core and many-core.} that encompasses as many problems as possible, balancing between generality and specific information required to maintain high performance.
\item Once an efficient solver is implemented, it should be offered to other people who are trying to solve similar problems. To do that, a <<standard>> should be agreed on: instead of creating a new dialect, it would be better that everybody speaks the same language. This is the very purpose of embedded DSL: people who know the host language easily understand how to encode their problem.
\ole
With the recent compiler technology, expressing such solution with code generation has become easier. The code generation approach is efficient because it allows to optimally address one specific problem for a particular hardware architecture, while preserving the generality in the code generator. In this perspective, Scala and LMS are an interesting host language. Our work provide one additional construction brick to address the space of the problems\footnote{For example \url{http://www.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.pdf}}.

We have demonstrated that our implementation, DynaProg, is able to deal with simple (Smith-Waterman) and complex (Zuker and RNAfold) dynamic programming problems on sequences that are expressed by a grammar and an algebra. With this formalism, their expression is simplified and exploiting the dynamic programming results becomes possible thanks to systematic backtrack encoding that allows traces to be exchanged between algebrae. This implementation is publicly available\footnote{\url{https://github.com/manojo/lamp-dp-mt/}} and can immediately be used in Scala projects.

% ------------------------------------------------------------------------------------------------
%{\center\color{red} \noindent\rule{16cm}{0.4pt} \\ XXX: CONTINUE HERE :XXX \\}
% ------------------------------------------------------------------------------------------------
%{\color{red} XXX: fix the introduction}
%{\color{red} Use CUDA profiler(?)}
% http://www.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.html
%
%\begin{verbatim}
%Big picture (Larger picture how to go further):
%- Code generation and actual compiler technology
%- Embedded DSL vs external DSL
%- Only a subset of the dynamic programming problems, easy to parallelize (similar memory accesses and scheduling pattern/progress)
%  - others are iterative or sparse, different scheduling => multiple scheduling patterns to be discovered and implemented efficiently, possibly in LMS (if time permits)
%\end{verbatim}

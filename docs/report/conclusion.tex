% ------------------------------------------------------------------------------------------------
\section{Future work}
We consider several directions and possible extensions for our work. We briefly describe each of them and give an idea of how they could be implemented:\ol

\item \textbf{Fusion and C with LMS:} Although we gained some speedup by optimizing manually the Scala parsers, we cannot benefit from grammar-specific optimization. Passing the whole grammar to LMS could possibly lead to more efficient code, by folding multiple parser functions into a single one and providing loops fusion\footnote{For example if the aggregation function is simple enough as maximum or minimum, aggregation could be merged with element processing, simliarly as described in \S\ref{normalization}.}. The added cost of function lookup, even if minimal, might still account for a non-negligible part of the total running time as parser processing is very simple but run repeatedly a large number of time. Also constructing large lists of candidates that are later reduced to a single one  increase the work of the garbage collector. Generating the grammar through LMS has two major benefits:\ol
\item Improve the performance of the parsers within the JVM. Since we previously argued in favor of a decoupling of the algebra and the grammar, and since we want to reach optimal performance, we would need to keep available both a generic version for result processing (independent of LMS restrictions so that we can use arbitrary functions within the algebra) and an optimized version for computations. The latter could be achieved similarly as the current CUDA code generation: the Scala parsers might produce an abstract syntax tree (AST) that could then be merged with the algebra nodes and passed to LMS for code generation.
\item Since experimental results have shown that in some situations, the CPU outperforms the GPU, it might also be interesting to also target single thread C implementation to benefit from these situations (thereby removing the overhead of the JVM). Using LMS would provide us with the support for such code generation. Since the program can be single threaded, no complex synchronization mechanism is involved, hence it is an ideal candidate for LMS multi-architecture code generation. 
\ole
\item \textbf{Macros:} macros provide an interesting meta-programming opportunity as they are being run after the typing phase of the Scala compiler and can leverage all the compile-time typing information. We could use them to either simplify the user-functions description (by converting types to bootstrap LMS code generation) or even provide ad-hoc conversion from the Scala AST to C code.
\item \textbf{Non-serial scheduling for problems larger than the device memory:} as described in \S\ref{ns_mem_transfer}, it could be possible to handle problems that are larger than the device memory in an efficient way, thereby dramatically reducing the memory transfer penalties introduced by the main memory usage. However, this comes at the price of a more involved kernel scheduling and a complex element indexing strategy (since we first need to find the enclosing matrix block before addressing the element relatively to it).
\item \textbf{Serial problems larger than memory:} As discussed in \S\ref{serial_memory}, this class of problems require a completely different implementation. Since the authors of \cite{swat_mega} are planning to write extensions to their implementation, duplicating the effort might not be worth the price; however, would their future implementation be sufficiently modular, we could integrate it in our framework and redirect compatible grammars to this state of art implementation.
\item \textbf{CUDA $k$-best parsers:} since a $k$-best algorithm has constant memory requirements, an efficient algorithm for CUDA could be devised: instead of comparing with only one value to find the best value, it suffice to compare with $k$ values instead. Hence cost and backtrack matrix would contain $k$ elements per cell. Since the problem of result validity is already addressed, cells with fewer results would simply have fewer cells marked valid.
\item \textbf{Multi-dimensional matrices and independent computations:} In the current implementation, all the matrices are encoded such that they are of the same size. Leveraging the yield analysis, we could reduce the dimension of matrices that are of smaller dimension (for tabulations with bounded maximal size). In the problems we have analyzed, no such special case appeared, this is why we do not support this optimization at present.

Matrix of different dimensions must be stored in their own array (versus being in a single array of struct enclosing corresponding element of all matrices). Also matrices might possibly be of different storage complexity: looking back at the Zuker problem description, there are two $O(n^2)$ matrices and one $O(n)$ matrix. This discrepancy in the sizes also leads to multiple indexing strategies (depending on the complexity) and a more complex scheduling where matrix must be computed one after another whereas in the current computation, the same cell in all matrices is computed at once.
\item \textbf{Data granularity:} Since the major bottleneck of CUDA architecture is the memory, we focus on data representation; in the current project, data is stored in primary types but we could store them more efficiently. For example, RNA is represented with only 4 letters (g,a,t,c), thus 4 symbols could be encoded in a single byte. Unfortunately, this optimizations seems to only apply for the input data. Another solution in this direction is to operate on multiple cells with one thread, the argument being that they could share a row or a column, thereby dividing the number of memory accesses for non-serial dependencies on the shared axis.
\item \textbf{Add FPGA as target platform:} Initially envisioned a second target platform, the underlying complexity of transforming DP recurrences into VHDL code made us leave this platform aside for the scope of this project. The reconfigurability possibilities of FPGA make them attractive whenever it comes to very simple and massively parallel computations where the data can be pipelined; this makes serial dynamic programming problems good candidates for such implementation.
\item \textbf{Algorithmic analysis:} so far, we considered that the DSL user would write an optimal program. Another direction in which we could improve the parsers is the recurrence analysis, either by removing serial dependencies when possible (\S\ref{calc_simplifications}) or reducing the algorithmic complexity by creating intermediate tabulations (\S\ref{user_lang}). These analysis would certainly involve a strong mathematical analysis and the ratio benefit over implementation complexity would be quite small under the initial assumption that the DSL users are experts in their field (thereby knowing how to optimize manually the grammar).
\item \textbf{Non-emptiness analysis:} At code generation level, when a tabulation is known to be \textit{non-empty} (every cell contain a valid result), it is possible to remove the validity checks, thereby reducing the parser complexity\footnote{The validity of a result is determined if its corresponding backtrack has a valid rule number. Skipping this test saves a memory load, and a condition testing.}. Such analysis needs to make sure that among all possible candidates, there exist at least one valid. Such analysis might be quite complex as it requires induction (for example in matrix multiplication, we inductively need to prove that every cell element is non-empty. Currently, this information needs to be explicitly provided by the programmer (by setting a flag on the non-empty tabulation).
\item \textbf{Pruning:} described in \cite{swat_mega}, this optimization could lead to a reduction of the computation, provided that the algorithm final score can be bounded. Such optimization would only be relevant with a non-uniform computation strategy where the matrix is tiled, thereby making it possible to prune entire computation tiles.
\ole

% ------------------------------------------------------------------------------------------------
\section{Conclusion}
This Master project is focused on how to solve efficiently dynamic programming. By restricting to the class of problem involving sequences as input, we were able to extract generic patterns and expose them to parallel architectures like CUDA. To do that, we first depicted the dynamic programming landscape and defined a set of problems we would like to solve (\S\ref{problems}). Then we discussed the requested functionalities to provide the user with a convenient embedded DSL that is based on the ADP formalization (\S\ref{architecture}). These architectural decisions lead us to consider two different implementations: one in Scala that allows the same expressivity as ADP (in term of multiple solutions search), and one in CUDA that focuses on efficiently obtain a single optimal result. We then provide some technical details explaining how these ideas are put in practice in DynaProg (\S\ref{implementation}). Finally, we compare our implementation with existing implementations (\S\ref{benchmarks}).

From the benchmarks, we see that our code generation is able to deal with problems from simple (Smith-Waterman) to complex (Zuker and RNAfold) and provide performance that is comparable to existing work. Since these problem are expressed with a grammar and algebra, their expression is simplified, and the possibility to exploit the dynamic programming results is leveraged through the use of multiple grammars.

% ------------------------------------------------------------------------------------------------
{\center\color{red} \noindent\rule{16cm}{0.4pt} \\ XXX: CONTINUE HERE :XXX \\}
% ------------------------------------------------------------------------------------------------
{\color{red} XXX: fix the introduction}

{\color{red} Benchmarks: we want to see the benefits of moving to CUDA, also compare to how far Scala is from C.}

{\color{red} XXX: say that Haskell package did not install and also time limited to provide largest result (but that are also less relevant as users do not want to wait)}

{\color{red} What's the conclusion on experimental results?}


{\color{red} Use CUDA profiler(?)}

% http://www.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.html

\begin{verbatim}
Recap of the report:
- Goal: generalize solving dynamic programming on sequences for GPUs
- classification of the problems; in terms of recurrences = complicated, grammar = easier
- general technique to describe backtracking information
- implemented in Scala+CUDA
- additional analysis to generate efficient code (1 result, memory alignment, scheduling)

Recap of the results:
- XXX

explain better approach: LMS write intermediate nodes, we embed nodes in actual objects

Big picture (Larger picture how to go further):
- Code generation and actual compiler technology
- Embedded DSL vs external DSL
- Only a subset of the dynamic programming problems, easy to parallelize (similar memory accesses and scheduling pattern/progress)
  - others are iterative or sparse, different scheduling => multiple scheduling patterns to be discovered and implemented efficiently, possibly in LMS (if time permits)
\end{verbatim}

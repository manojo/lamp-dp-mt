\documentclass[11pt]{article}
\input{inc/style.sty}

\title{DynaProg for Scala}
\subtitle{A Scala DSL for Dynamic Programming on CPU and GPU}
\begin{document}
\maketitle
%\shorttitle

% Paper introduction:
% Problem to solve, what exists (related work) and how do we compare to other
% Contributions (3): 3 tensed sentences
% Benchmarks => evaluation metrics, prove introduction statements by evaluation
\subsection*{Abstract}
Dynamic programming is a common pattern of Computer Science used in various domains. Yet underlying matrix recurrences might be difficult to express and error prone. Additionally, domain experts might not have the skills to make an efficient parallel implementation. In this project, we present \textit{DynaProg}, a Scala DSL for dynamic programming on heterogeneous platforms which allow to write concise programs and execute them efficiently on GPUs.

Existing work is a DSL embedded in Haskell \cite{adp} with possible conversion to CUDA code \cite{adp_gpu}, a compiler for a dynamic programming external DSL into C code \cite{gapc} or ad-hoc CUDA implementations for specific problem classes \cite{swat_mega}, \cite{gpu_atlp}.
% XXX: How we compare to other, how to evaluate
% Benchmark => prove by evaluation intro statements

Our contributions are: \ul
%\item A classification of DP problems characteristics (matrix shape, dependency graph, ...)
\item A systematic approach to process data (top-down/bottom-up) and backtracking information (focus on running time and memory efficiency)
\item A language embedded in Scala (DSL) to express DP problems concisely (based on ADP)
\item Two implementations: Scala for CPU (features) and an CUDA for GPU (efficiency)
%\item Reuse of existing compiler technology (fusion) for a specific purpose
%\item State of the art parallel implementation of these classes on GPUs
%\item Normalization of the grammar into efficient productions
%\item Code generator to transform a grammar into efficient code for CPU, GPU (and FPGA)
\ule

\vfill
This project has been achieved in collaboration with Manohar Jonnalagedda. I also would like to thank the LAMP team, including Eugene Burmako, Andro Stucki, Vojin Jovanovic and Tiark Rompf who provided insightful advices and suggestions. I hope you will enjoy your reading. \vspace{.3cm}\\
\textit{Thierry Coppey}

\newpage
\setcounter{tocdepth}{2} \tableofcontents

\input{introduction}
\input{problems}
\input{design}
\input{implementation}
\section{Benchmarks}
{\color{red} XXX: explain why slower: approx 3x more memory used per score (maintain matrix dimensions)

XXX: continue benchmarking here
}
\begin{table}[H]\begin{center}\begin{tabular}{lrrrrrr} \toprule
Matrix size			& 1024	& 2048	& 4096	& 8192 \\
Splits				& 1		& 8		& 64		& 512 \\ \midrule
Analysis				& 0.007	& 0.007	& 0.007	& 0.007 \\
Code generation		& 0.071	& 0.072	& 0.073	& 0.071 \\
C/CUDA compilation	& 3.025	& 1.836	& 1.832	& 1.841 \\
Scala compilation		& 1.837	& 1.853	& 1.830	& 1.767 \\ \midrule
- JNI read				& 0.027	& 0.028	& 0.029	& 0.058 \\
- CUDA compute		& 0.868	& 3.378	& 19.782	& 139.697 \\
- CUDA backtrack		& 0.009	& 0.012	& 0.023	& 0.045 \\
- JNI output			& 0.005	& 0.009	& 0.018	& 0.032 \\
Total execution			& 0.915	& 3.432	& 19.860	& 139.846 \\ \bottomrule
\end{tabular}\end{center}\caption{Preliminary results for MatrixMultGen}\end{table}

{\color{red} XXX: Compare current implementation versus ad-hoc implementation. Compare CUDA vs Scala (we might need to ad-hoc fix stack overflows in Scala).If Zuker coefficients can be fixed, compare performance with \cite{adp_gpu} by rescaling numbers wrt to bandwidth and computation performance.}

\section{Future work}
We consider several directions and possible extensions for our work. We briefly describe each of them and give an idea of how they could be implemented:\ol
\item \textbf{Non-serial scheduling for problems larger than the device memory:} as described in \ref{ns_mem_transfer}, it could be possible to handle problems that are larger than the device memory in an efficient way, hereby dramatically reducing the memory transfer penalties introduced by the main memory usage. However, this comes at the price of a more involved kernel scheduling and a complex element indexing strategy (since we first need to find the enclosing matrix block before addressing the element relatively to it).
\item \textbf{Algorithmic analysis:} so far, we considered that the DSL user would write an optimal program. Another direction in which we could improve the parsers is the recurrence analysis, either by removing serial dependencies when possible (see \ref{calc_simplifications}) or reducing the algorithmic complexity by creating intermediate tabulations (see \ref{user_lang}). These analysis would certainly involve a strong mathematical analysis and the ratio benefit over implementation complexity would be quite small under the initial assumption that the DSL users are experts in their field (hereby knowing how to optimize manually the grammar). 
\item \textbf{Serial problems larger than memory:} As discussed in \ref{serial_memory}, this class of problems require a completely different implementation. Since the authors of \cite{swat_mega} are planning to write extensions to their implementation, duplicating the effort might not be worth the price; however, would their future implementation be sufficiently modular, we could integrate it in our framework and redirect compatible grammars to this state of art implementation.
\item \textbf{Add FPGA as target platform:} Initially envisioned a second target platform by George Nithin, a PhD student of the LAP (Laboratory of processors architecture), the underlying complexity of transforming DP recurrences into VHDL code made us leave this platform aside for the scope of this project. The reconfigurability possibilities of FPGA make them attractive whenever it comes to very simple and massively parallel computations where the memory can be pipelined; this makes serial dynamic programming problems good candidates for such implementation.
\item \textbf{Multi-dimensional matrices:} in the current implementation, all the matrices are encoded such that they are of the same size. Leveraging the yield analysis, we could reduce the dimension of matrices that are of smaller dimension (for tabulations with bounded maximal size). In the problems we have analyzed, no such special case appeared, this is why we do not support this optimization at present.

{\color{red}
\begin{verbatim}
How to Encode multi-dimensional matrices efficiently
1. assume they have the same type put one after another => different dimensions ok
2. assume of same size => put into a struct
=> but using different pointers seems more reliable => completely different matrices => fixed list of matrices by dimensionality (O(1), O(n), O(n^2), ...) of structs (determined by number of indices to access object)
\end{verbatim}
}

\item \textbf{Independent computations:} {\color{red}The attentive reader might have noticed a discrepancy between the problem analysis and the recurrence rules of the Zuker problem implementation. This is due to the usage of multiple source for the problem description. 
}

\item {\color{red} Pack the data => less memory transfer (i.e. GATC=>4 letters in 1 char), but only applies to input; score is hardly compressible if appropriate type is initially chosen}
\item {\color{red} Operate on larger words (ex 64 bits) to increase thread locality and reduce memory accesses}
\ole

\section{Conclusion}
{\color{red} XXX}

% ------------------------------------------------------------------------------------------------
\newpage
\section*{Planning}
\subsubsection*{Todo @TCK}\ul
\item Test/proof parsers are correct -- make sure implementation is correct
\item Automate test to compare against implementation
\item Benchmarks -- use CUDA profiler(?)
\item Write report
\item Port LibRNA for CUDA?
\ule

\subsubsection*{Todo @Manohar}\ul
\item Integrate LMS code generation into v4.
\item Fix Zuker coefficients
\ule

Legacy roadmap deadlines: \\
\begin{tabular}{ll}
Nov 16 & Rules normalization and automatic backtracking \\
	& GenScala on LMS + GenCuda + LMS CudaCompiler \\
Nov 23 & Problem generalization: "cyclic keyword", Zucker problem / CudaLoop optimization \\
Nov 30 &--- gap due to LMS missing knowledge --- \\
Dec 7 & Benchmarking, grammar analysis \\
Dec 14 & First thoughts for larger than device memory \\
Dec 21 & Writing report \\
Jan 4 & --- holiday --- \\
Jan 18 & Writing report: implementation description and plan for future work
\end{tabular}

\subsubsection*{Journal}
\begin{tabular}{ll}
Sep. 17	& Getting started with the project, reading related work on hash maps. \\
Sep. 24	& Parallel hashing paper solve the problem for 32bit key/value pair. Stripped CudPP. \\
		& Devised (but not implemented) extension beyond 64bit using memory areas locks. \\
---		& Change of project suggested by Vojin (supervisor): joint work with Manohar on DP \\
Oct. 01	& Problems specifications: serial/non-serial, started CUDA implementation with blocks. \\
Oct. 08	& CudAlign solves serial monadic, might adapt it (but complicated / ad-hoc / in progress). \\
		& Focus on non-serial problems that fit in GPU memory. \\
Oct. 15	& First working implementation for non-serial problems (rectangle, triangle, parallelogram) \\
Oct. 22	& No workaround for timeout, fixed by multiple kernels. Implemented backtrack on GPU. \\
Oct. 29	& Scala/C compiler engine, to use Scala/CUDA, C code must be provided. \\
Nov. 05	& Multiple fixes and rework of ADP parsers to aim at generating C-like code. \\
Nov. 12	& Rework ADP parsers: cyclic, two-track grammars and automatic backtracking discussion. \\
Nov. 19	& Explorations LMS and Macros for C code, implementation is quite different, hence ad-hoc. \\
Nov. 26	& Full backtracking stack: apply / unapply / reapply, refactoring of the classes. \\
Dec. 03	& JNI for LibRNA to get coefficients for Zuker, errors in algebra (quite hairy code). \\
Dec. 10	& Rework concatenation operators, yield analysis, code generation. \\
Dec. 17	& Detupling, generic backtrack (vs. ad-hoc), nested aggregates, empty results support. \\
Dec. 24	& (sick 4 days), code generation: full JNI conversion support, rework CodeCompiler. \\
Dec. 31	& Writing report (design, implementation), fixing various issues, complexity analysis. \\
Jan. 7	& {\color{red} XXX} \\
Jan. 14	& {\color{red} XXX} \\
\end{tabular}

%Some GPU algorithms: http://hgpu.org/?cat=11
%Translation into C++: http://bibiserv.cebitec.uni-bielefeld.de/macports/resources/download/
%CUDPP libraries (but awfully big resulting binary): http://code.google.com/p/cudpp/
%13 dwarfs: http://developer.amd.com/afds/assets/presentations/2155_final.pdf
%http://tutorials.jenkov.com/java-reflection/fields.html
%http://lampwww.epfl.ch/~michelou/scala/scala-reflection.html
%
%Hint: use TypeClass to put a predicate on types
%  def fun[T: CanTranslateToC](...)
%  def fun[T](implicit ev:CanTranslateToC[T])
%  class CanTranslateToC[T] { def translate:String }
%  implicit def canTranslateInt = new CanTranslateToC[Int] = { def translate = "Int" }

% ------------------------------------------------------------------------------------------------
\newpage
%\usepackage{multicol}
%\usepackage{etoolbox}
%\patchcmd{\thebibliography}{\section*{\refname}}{\begin{multicols}{2}[\section*{\refname}]}{}{}
%\patchcmd{\endthebibliography}{\endlist}{\endlist\end{multicols}}{}{}
%\bibliographystyle{acm}
\bibliographystyle{plain}
\bibliography{bibliography.bib}
\end{document}

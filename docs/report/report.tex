\documentclass[11pt]{article}
\input{inc/style.sty}

\title{DynaProg for Scala}
\subtitle{A Scala DSL for Dynamic Programming on CPU and GPU}
\begin{document}
\maketitle
%\shorttitle

% Paper introduction:
% Problem to solve, what exists (related work) and how do we compare to other
% Contributions (3): 3 tensed sentences
% Benchmarks => evaluation metrics, prove introduction statements by evaluation
\subsection*{Abstract}
Dynamic programming is a common pattern of Computer Science used in various domains. Yet underlying matrix recurrences might be difficult to express and error prone. Additionally, domain experts might not have the skills to make an efficient parallel implementation. In this project, we present \textit{DynaProg}, a Scala DSL for dynamic programming on heterogeneous platforms which allow to write concise programs and execute them efficiently on GPUs.

Related work is a DSL embedded in Haskell \cite{adp} with possible conversion to CUDA code \cite{adp_gpu}, a compiler for a dynamic programming-specific language written in C \cite{gapc_thesis} or ad-hoc CUDA implementations for specific problem classes \cite{swat_mega}, \cite{gpu_atlp}.
% XXX: How we compare to other, how to evaluate
% Benchmark => prove by evaluation intro statements

Our contributions are: \ul
%\item A classification of DP problems characteristics (matrix shape, dependency graph, ...)
\item A systematic approach to process data (top-down/bottom-up) and backtracking information (focus on running time and memory efficiency)
\item A language embedded in Scala (DSL) to express DP problems concisely (based on ADP)
\item Two implementations: Scala for CPU (features) and an CUDA for GPU (efficiency)
%\item Reuse of existing compiler technology (fusion) for a specific purpose
%\item State of the art parallel implementation of these classes on GPUs
%\item Normalization of the grammar into efficient productions
%\item Code generator to transform a grammar into efficient code for CPU, GPU (and FPGA)
\ule

\vfill
This project has been achieved in collaboration with Manohar Jonnalagedda. I also would like to thank the LAMP team, including Eugene Burmako, Sandro Stucki, Vojin Jovanovic and Tiark Rompf who provided insightful advices and suggestions. I hope you will enjoy your reading. \vspace{.3cm}\\
\textit{Thierry Coppey}

\newpage
\setcounter{tocdepth}{2} \tableofcontents

\input{introduction}
\input{problems}
\input{design}
\input{implementation}
\section{Benchmarks}
{\center\color{red} XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\\ CONTINUE HERE\\ 
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX}

\subsection{Java parsers}
{\color{red}
MatrixMult-512, Mac+JDK7\ul
\item Original: 24.937 sec / 24.658 sec
\item Optimized concatenation: 19.329 sec / 18.924 sec
\item Tabulation as arrays+inline: 14.976 sec / 14.849 sec
\ule}

\subsection{CUDA parsers}
{\color{red} XXX: explain why slower: approx 3x more memory used per score (maintain matrix dimensions)

XXX: continue benchmarking here

XXX: relate to staging operation in LMS

XXX: include "RNAfold" grammar description into problems

XXX: talk about memory management and hash tables
XXX: relate to staging operation in LMS

XXX: fix layout in codegen (newline missing)

}
\begin{table}[H]\begin{center}\begin{tabular}{lrrrrrr} \toprule
Matrix size			& 1024	& 2048	& 4096	& 8192 \\
Splits				& 1		& 8		& 64		& 512 \\ \midrule
Analysis				& 0.007	& 0.007	& 0.007	& 0.007 \\
Code generation		& 0.071	& 0.072	& 0.073	& 0.071 \\
C/CUDA compilation	& 3.025	& 1.836	& 1.832	& 1.841 \\
Scala compilation		& 1.837	& 1.853	& 1.830	& 1.767 \\ \midrule
- JNI read				& 0.027	& 0.028	& 0.029	& 0.058 \\
- CUDA compute		& 0.868	& 3.378	& 19.782	& 139.697 \\
- CUDA backtrack		& 0.009	& 0.012	& 0.023	& 0.045 \\
- JNI output			& 0.005	& 0.009	& 0.018	& 0.032 \\
Total execution			& 0.915	& 3.432	& 19.860	& 139.846 \\ \bottomrule
\end{tabular}\end{center}\caption{Preliminary results for MatrixMultGen}\end{table}

{\color{red} XXX: Compare current implementation versus ad-hoc implementation. Compare CUDA vs Scala (we might need to ad-hoc fix stack overflows in Scala).If Zuker coefficients can be fixed, compare performance with \cite{adp_gpu} by rescaling numbers wrt to bandwidth and computation performance.}

\section{Future work}
We consider several directions and possible extensions for our work. We briefly describe each of them and give an idea of how they could be implemented:\ol
\item \textbf{Non-serial scheduling for problems larger than the device memory:} as described in \ref{ns_mem_transfer}, it could be possible to handle problems that are larger than the device memory in an efficient way, hereby dramatically reducing the memory transfer penalties introduced by the main memory usage. However, this comes at the price of a more involved kernel scheduling and a complex element indexing strategy (since we first need to find the enclosing matrix block before addressing the element relatively to it).
\item \textbf{Algorithmic analysis:} so far, we considered that the DSL user would write an optimal program. Another direction in which we could improve the parsers is the recurrence analysis, either by removing serial dependencies when possible (see \ref{calc_simplifications}) or reducing the algorithmic complexity by creating intermediate tabulations (see \ref{user_lang}). These analysis would certainly involve a strong mathematical analysis and the ratio benefit over implementation complexity would be quite small under the initial assumption that the DSL users are experts in their field (hereby knowing how to optimize manually the grammar). 
\item \textbf{Serial problems larger than memory:} As discussed in \ref{serial_memory}, this class of problems require a completely different implementation. Since the authors of \cite{swat_mega} are planning to write extensions to their implementation, duplicating the effort might not be worth the price; however, would their future implementation be sufficiently modular, we could integrate it in our framework and redirect compatible grammars to this state of art implementation.
\item \textbf{Add FPGA as target platform:} Initially envisioned a second target platform by George Nithin, a PhD student of the LAP (Laboratory of processors architecture), the underlying complexity of transforming DP recurrences into VHDL code made us leave this platform aside for the scope of this project. The reconfigurability possibilities of FPGA make them attractive whenever it comes to very simple and massively parallel computations where the memory can be pipelined; this makes serial dynamic programming problems good candidates for such implementation.
\item \textbf{Multi-dimensional matrices and independent computations:} In the current implementation, all the matrices are encoded such that they are of the same size. Leveraging the yield analysis, we could reduce the dimension of matrices that are of smaller dimension (for tabulations with bounded maximal size). In the problems we have analyzed, no such special case appeared, this is why we do not support this optimization at present.

Matrix of different dimensions must be stored in their own array (versus being in a single array of struct enclosing corresponding element of all matrices). Also matrices might possibly be of different storage complexity: looking back at the Zuker problem description, there are two $O(n^2)$ matrices and one $O(n)$ matrix. This discrepancy in the sizes also leads to multiple indexing strategies (depending on the complexity) and a more complex scheduling where matrix must be computed one after another whereas in the current computation, the same cell in all matrices is computed at once. 
\item \textbf{Data granularity:} Since the major bottleneck of CUDA architecture is the memory, we focus on data representation; in the current project, data is stored in primary types but we could store them more efficiently. For example, RNA is represented with only 4 letters (g,a,t,c), thus 4 symbols could be encoded in a single byte. Unfortunately, this optimizations seems to only apply for the input data. Another solution in this direction is to operate on multiple cells with one thread, the argument being that they could share a row or a column, hereby dividing the number of memory accesses for non-serial dependencies on the shared axis.
\item \textbf{Pruning:} described in \cite{swat_mega}, this optimization could lead to a reduction of the computation, provided that the algorithm final score can be bounded. Such optimization would only be relevant with a non-uniform computation strategy where the matrix is tiled, hereby making it possible to prune entire computation tiles.
\item \textbf{Fusion with LMS:} Although we gained some speedup by optimizing manually the Scala parsers, we cannot benefit from grammar-specific optimization. Passing the whole grammar to LMS could possibly lead to more efficient code, by folding multiple parser function calls into a single function making equivalent calls. The added cost of function lookup, even if minimal, might still account for a non-negligible part of the total running time as parser processing is very simple but run repeatedly a large number of time.
\item \textbf{Using macros:} macros provide an interesting meta-programming opportunity as they are being run after the typing phase of the Scala compiler and can leverage all the compile-time typing information. We could use them to either simplify the user-functions description (by implicitly bootstrapping LMS code translation) or even completely replace LMS types by either type conversion or by providing ad-hoc conversion from the Scala AST to C code.
\ole

\section{Conclusion}
{\color{red} XXX}

% ------------------------------------------------------------------------------------------------
\newpage
\section*{Planning}
\subsubsection*{Todo @TCK}\ul
\item Test/proof parsers are correct -- make sure implementation is correct
\item Automate test to compare against implementation
\item Benchmarks -- use CUDA profiler(?)
\item Write report
\item Port LibRNA for CUDA?
\ule

\subsubsection*{Todo @Manohar}\ul
\item Integrate LMS code generation into v4.
\item Fix Zuker coefficients
\ule

Legacy roadmap deadlines: \\
\begin{tabular}{ll}
Nov 16 & Rules normalization and automatic backtracking \\
	& GenScala on LMS + GenCuda + LMS CudaCompiler \\
Nov 23 & Problem generalization: "cyclic keyword", Zucker problem / CudaLoop optimization \\
Nov 30 &--- gap due to LMS missing knowledge --- \\
Dec 7 & Benchmarking, grammar analysis \\
Dec 14 & First thoughts for larger than device memory \\
Dec 21 & Writing report \\
Jan 4 & --- holiday --- \\
Jan 18 & Writing report: implementation description and plan for future work
\end{tabular}

\subsubsection*{Journal}
\begin{tabular}{ll}
Sep. 17	& Getting started with the project, reading related work on hash maps. \\
Sep. 24	& Parallel hashing paper solve the problem for 32bit key/value pair. Stripped CudPP. \\
		& Devised (but not implemented) extension beyond 64bit using memory areas locks. \\
---		& Change of project suggested by Vojin (supervisor): joint work with Manohar on DP \\
Oct. 01	& Problems specifications: serial/non-serial, started CUDA implementation with blocks. \\
Oct. 08	& CudAlign solves serial monadic, might adapt it (but complicated / ad-hoc / in progress). \\
		& Focus on non-serial problems that fit in GPU memory. \\
Oct. 15	& First working implementation for non-serial problems (rectangle, triangle, parallelogram) \\
Oct. 22	& No workaround for timeout, fixed by multiple kernels. Implemented backtrack on GPU. \\
Oct. 29	& Scala/C compiler engine, to use Scala/CUDA, C code must be provided. \\
Nov. 05	& Multiple fixes and rework of ADP parsers to aim at generating C-like code. \\
Nov. 12	& Rework ADP parsers: cyclic, two-track grammars and automatic backtracking discussion. \\
Nov. 19	& Explorations LMS and Macros for C code, implementation is quite different, hence ad-hoc. \\
Nov. 26	& Full backtracking stack: apply / unapply / reapply, refactoring of the classes. \\
Dec. 03	& JNI for LibRNA to get coefficients for Zuker, errors in algebra (quite hairy code). \\
Dec. 10	& Rework concatenation operators, yield analysis, code generation. \\
Dec. 17	& Detupling, generic backtrack (vs. ad-hoc), nested aggregates, empty results support. \\
Dec. 24	& (sick 4 days), code generation: full JNI conversion support, rework CodeCompiler. \\
Dec. 31	& Writing report (design, implementation), fixing various issues, complexity analysis. \\
Jan. 7	& Writing report, attempt to fix Zuker, fix SBT, strip LibRNA {\color{red} XXX} \\
Jan. 14	& {\color{red} XXX} \\
\end{tabular}

%Some GPU algorithms: http://hgpu.org/?cat=11
%Translation into C++: http://bibiserv.cebitec.uni-bielefeld.de/macports/resources/download/
%CUDPP libraries (but awfully big resulting binary): http://code.google.com/p/cudpp/
%13 dwarfs: http://developer.amd.com/afds/assets/presentations/2155_final.pdf
%http://tutorials.jenkov.com/java-reflection/fields.html
%http://lampwww.epfl.ch/~michelou/scala/scala-reflection.html
%
%Hint: use TypeClass to put a predicate on types
%  def fun[T: CanTranslateToC](...)
%  def fun[T](implicit ev:CanTranslateToC[T])
%  class CanTranslateToC[T] { def translate:String }
%  implicit def canTranslateInt = new CanTranslateToC[Int] = { def translate = "Int" }

% ------------------------------------------------------------------------------------------------
\newpage
%\usepackage{multicol}
%\usepackage{etoolbox}
%\patchcmd{\thebibliography}{\section*{\refname}}{\begin{multicols}{2}[\section*{\refname}]}{}{}
%\patchcmd{\endthebibliography}{\endlist}{\endlist\end{multicols}}{}{}
%\bibliographystyle{acm}
\bibliographystyle{plain}
\bibliography{bibliography.bib}
\end{document}

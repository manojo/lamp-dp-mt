Presentation plan
Duration: 30 minutes

--- COVER (DP+CUDA+Scala+EPFL)

--- INTRO
- What is Dynamic Programming
  + Algorithmic technique to solve optimization problems
  + Bellman's optimality principle (optimal sub-solutions)
  + Memoization reduces exponential to polynomial complexity
  + Goal: construct an optimal solution. How? Backtracking.
  + Example: matrix chain multiplication (cut in two) *** graph
    + Recurrence formula
- Problem examples (applied computer science, quick)
  + Biosequence analysis
    + Smith-Waterman sequence alignment
    + Zuker folding
  + Databases, natural language processing, operational research: expression parenthesisation
    + It's quite common
    + Recurrences expression is complicated

- Motivation
  + Provide user with simple language to express DP => ADP
  + Execute DP efficiently => GPU
  + At our disposal: Scala & LMS => embedded DSL
    + Encode domain specific problem with Scala syntax (concise)
    + Benefit from Scala and LMS power (typing, code conversion)
    + Allows to use all resources of Scala (+Java) world

--- THEORY
- User facing language: ADP
  - Abstract
    + Grammar (CFG): produces candidates (generate search space)
    + Algebra: (candidate -> score) functions
    + Optimization function: decides which scores are 'best' candidates
    + Tabulation: memoize best candidates
    -> Clean separation allows multiple algebra for one grammar
  - Concrete (parsers)
    + Signature: interface for algebra
    + Algebra/optimization function: arbitrary functions
    + Parsers
      + Terminal (grammar terminal)
      + Concatenation (combine two sequential elements)
      + Map (convert parsed elements into a candidate)
      + Filter (applied before computing subproblems)
      + Alternative (or, union of results)
      + Aggregation (candidate selection)
      + Tabulation (grammar rule)
  - Example: matrix chain multiplication (+variant?) *** graph

- Recurrences analysis
  + We need bottom-up approach to exploit parallelism: construct elements of all matrices at the same position
  + Dead rules elimination: save useless computation (bottom-up) and space (sparse resource, because we have matrices)
  + Yield analysis: automatically find the minimum/maximum size of tabulations.
    + Avoid infinite loops (empty ~ empty)
    + Matrix dimension => save space (not currently implemented)
  + Dependency analysis (dependencies at same position => bottom-up correctness)

- Backtracking
  - Encoding
    + Explicit = bad (re-introduce indices/ad-hoc for the user)
    + Grammar rules -> uniquely identify (separate alternatives)
    + Concatenation indices -> some are fixed (min/max yield), some are moving
      + Deduce fixed indices with yield size of elements
      + Memorize moving indices
    + Tabulate (candidate, (rule_id, List[ moving_indices ]))
    + Backtrack trace = List[ (rule_id, List[ moving_indices ]) ]
  - Multiple results (Scala)
    + We can reconstruct alternative (rule_id) and concatenation indices from tabulation
    + But multiple traces might share common path and diverge at some point *** graph
    + Needs to take into account the multiplicity of each paths
      + unique direction => continue with same multiplicity
      + r directions, multiplicity = k
        + if (k>=r) explore all paths with k-r+1 multiplicity (each branch might produce only 1 result)
        + if (k<r) explore the first k paths with multiplicity=1 (more solutions than needed)
        -> aggregate to retain only 'best' solutions, then choose k of them
  - For CUDA
    + Focus on speed => return only 1 optimal solution
    + Minimize memory consumption => aggregate as soon as element is produced
    + Fusion of candidate generation, mapping to score and aggregation
    + Special care needed for:
    + Nested aggregation: aggregate inner in fresh temporary (hoisted), then aggregate outer
    + Failure encoding
      + "Zero"/"infinite" element ? cumbersome, needs to be provided for each aggregations by the user
      + Better: use a special rule number in the backtrack to indicate "empty result"

- Multiple algebra
  + Remember algebra and aggregation function are abstracted in Signature => we can easily change algebra
  + Why so much effort describing backtracking ? We have normalized it ! (wrt. grammar)
  + We can exchange grammar between algebra sharing the same grammar
  + Exploit the generated trace with a computing algebra
  + Example: solve matrix multiplication DP with algebra 1, do actual multiplication with algebra 2 (load matrices from disk, compute on CUDA)

--- IMPLEMENTATION
- Device constraints
  - Threads: threads within a warp (32) share the same scheduler => penalty if branch divergence *** graph
  - Memories: global (large, off-chip), shared (64K on-chip for L1/scratchpad), constant (global read-only) *** graph
  - Memory layout: coalesced accesses (contiguous wrt. to threads to increase throughput) *** graph for triangle and square matrices, memory chip?
- Problem constraints
  + Complex dependencies (line progress depends on previous line)
    + Complex scheduling primitive not (yet) available in LMS
    + LMS => closed world hypothesis, but we want to support arbitrary algebrae
      => trade-off: algebra(LMS)/grammar(hand-optimized)
  - Runtime engine
    + Partial specialization: input and matrix size, # of splits hardcoded
    + Similar to Delite: CUDA kernel + JNI interface => compile+load in JVM
  - LibRNA
    + Energies function for RNA folding, based on coefficients (physical measurements)
    + Heavily uses input sequence and coefficients (~200K)
    + Load sequence in shared memory, some coefficients in constant memory

--- BENCHMARKS RESULTS
- Matrix multiplication: on par with hand-written implementation (slightly worse than ATLP) *** figures + graph
- Smith-Waterman
- Zuker/RNAfold

--- DEMO
- Syntax (ADP)
- CodeGen

--- CONCLUSION
- Recap results
- Future work

--- END: QUESTIONS


XXX: instead of maintaining all elements in memory, only maintain 1 table
XXX: future work: explicit initial value for aggregations 6%
